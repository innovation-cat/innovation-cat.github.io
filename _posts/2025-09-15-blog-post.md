---
title: 'Fast Generation with Flow Matching'
date: 2025-09-15
excerpt: "Fast sampling has become a central goal in generative modeling, enabling the transition from high-fidelity but computationally intensive diffusion models to real-time generation systems. While diffusion models rely on tailored numerical solvers to mitigate the stiffness of their probability flow ODEs, flow matching defines dynamics through smooth interpolation paths, fundamentally altering the challenges of acceleration. This article provides a comprehensive overview of fast sampling in flow matching, with emphasis on path linearization strategies (e.g., Rectified Flow, ReFlow, SlimFlow, InstaFlow), the integration of consistency models, and emerging approaches such as flow generators."
permalink: /posts/2025/09/flow-matching-sampling/
tags:
  - Flow Matching
  - Acceleration
  - Consistency Models
---

<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>üìö Table of Contents</strong></span>
  </summary>
  <ul>
	<li><a href="#section1">1. From Diffusion to Flow Matching in Fast Sampling</a>
		<ul>
			<li><a href="#section1.1">1.1 The challenges of Diffusion Fast Sampling</a></li>
			<li><a href="#section1.2">1.2 Flow Matching Fast Sampling: Structural Differences and New Opportunities</a></li>
		</ul>
	</li>
	<li><a href="#section2">2. Straighten trajectories for Efficient Flow Matching</a>
		<ul>
			<li><a href="#section2.1">2.1 Motivation</a>
				<ul>
					<li><a href="#section2.1.1">2.1.1 LTE and the Role of Curvature</a></li>
					<li><a href="#section2.1.2">2.1.2 Why Path Linearization Helps</a></li>
				</ul>
			</li>
			<li><a href="#section2.2">2.2 ReFlow ‚Äî Iterative Rectification of Flow Matching</a>
				<ul>
					<li><a href="#section2.2.1">2.2.1 Implementation: How ReFlow Works</a></li>
					<li><a href="#section2.2.2">2.2.2 Why ReFlow is Correct</a></li>
				</ul>
			</li>
		</ul>
	</li>
  </ul>
</details>



While Flow Matching (FM) offers a geometrically elegant alternative to diffusion models by learning continuous probability flows rather than stochastic denoising trajectories, its inference remains computationally expensive. Despite the apparent ‚Äústraightness‚Äù of the learned flow fields, generation still requires solving an ordinary differential equation (ODE) over many discrete steps. This blog investigates why **Flow Matching is not inherently fast**, tracing the inefficiencies that persist even in rectified or linearized trajectories. We then present and analyze recent advances designed to accelerate FM sampling‚Äî**Rectified Flow**, **InstaFlow**, **Shortcut**, and **MeanFlow**‚Äîwhich progressively move from geometrical straightening to functional reformulation and finally to one-step direct mappings. By contrasting their motivations, formulations, and numerical behaviors, we reveal a unifying view of *flow acceleration* as a shift from learning instantaneous velocity fields to learning integral or averaged mappings that approximate the entire transport process in minimal function evaluations. The discussion concludes by outlining open challenges toward achieving *true one-step generative flows* that bridge physical interpretability, numerical stability, and computational efficiency.


---

<h1 id="section1" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">1. Introduction: Why Flow Matching Is Not Fast Enough</h1>


Flow Matching (FM) [^FM] was originally proposed as a deterministic and conceptually elegant alternative to diffusion models. Instead of modeling noisy Markov chains or stochastic denoising processes, FM directly learns a **continuous velocity field** $ v_\theta(x, t)$ that defines a **probability flow ODE**. This ODE transports samples smoothly from a simple prior distribution (e.g., Gaussian noise) to the target data distribution. In principle, this framework eliminates stochasticity, reduces sampling variance, and offers a clean geometric interpretation of generation as a continuous mass transport problem.


---

<h1 id="section1.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">1.1 Why FM Can Be Faster Than Diffusion-Based PF-ODEs</h1>


Although both DM and FM can be expressed as deterministic ODEs during inference, they differ fundamentally in how these ODEs are defined and what they are trained to approximate. The key lies not in the mathematical form of the equations, but in the **origin of the training objective** and the **geometry of the learned transport paths**.



---

<h1 id="section1.1.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">1.1.1 Diffusion PF-ODEs: A By-Product of a Stochastic Process</h1>




In diffusion models, the forward dynamics follow a stochastic differential equation (SDE):

$$
dx=f(t,x)\,dt+g(t)\,dw_t
$$

where $w_t$ denotes Brownian noise.  The model is trained to predict auxiliary targets‚Äînoise $\epsilon_\theta$, score $\nabla_x\log p_t(x)$, or the denoised sample $x_{0,\theta}$‚Äîthat are defined with respect to this noisy diffusion process.
During inference, one can reformulate the reverse SDE into a deterministic **probability-flow ODE (PF-ODE)**:

$$
\frac{dx}{dt}=f(t,x)-\tfrac{1}{2}g(t)^2 s_\theta(x,t).
$$

Although this PF-ODE yields the same marginal distribution as the reverse SDE, it is **not the quantity being optimized during training**; it merely arises as an analytical consequence of the stochastic objective. Therefore, the learned flow inherits the geometric irregularities of the noisy trajectories.  The resulting velocity field remains highly curved in high-dimensional space, forcing numerical solvers to use small step sizes to maintain accuracy and stability.

---

<h1 id="section1.1.2" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">1.1.2 Flow Matching: Learning the ODE Itself</h1>


Flow Matching inverts this paradigm.  It **defines the deterministic ODE as the primary learning target** rather than as a by-product. Given a source sample (x_0) and a target noise (\epsilon), FM introduces an explicit interpolation

$$
x_t = (1-\alpha_t)x_0 + \alpha_t\epsilon,
$$

and supervises the network to approximate the true conditional velocity

$$
v_t = \frac{d}{dt}x_t = \dot\alpha_t(\epsilon - x_0).
$$

The model minimizes

$$
\mathcal{L}_{\text{FM}}= \mathbb{E}_{x_0,\epsilon,t}\left[\|v_\theta(x_t,t)-v_t\|^2\right],
$$

which directly teaches the network how to move data points through time. Unlike diffusion models‚Äîwhere the PF-ODE is recovered **after** training‚ÄîFM optimizes the ODE **itself**. As a result, the learned field $v_\theta(x,t)$ tends to be **geometrically straight and globally smooth**, leading to deterministic transport with lower curvature and improved numerical stability.

In geometric terms, diffusion‚Äôs PF-ODE performs a **passive simplification** of stochastic paths, whereas Flow Matching performs an **active planning** of deterministic, near-geodesic trajectories. This distinction explains why FM usually achieves high-fidelity synthesis with only **10‚Äì20 NFEs**, compared to the **50‚Äì100 NFEs** typically required by diffusion-based PF-ODEs.


---

<h1 id="section1.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">1.2 The Remaining Bottleneck: Straight but Not Instantaneous</h1>

Although Flow Matching yields smoother and more linear flows. However, the learned vector field is the **marginal velocity field**, i.e. the conditional expectation of all possible straight-line velocities passing through $x_t$. 

$$
v_t(x) = \mathbb{E}\big[\partial_t x_t \,\big|\, x_t = x\big].
$$

Therefore, what the model learns is **not** the deterministic velocity of a specific straight-line path, but the conditional expectation of velocities across **all** possible paths at that point. This means even though the training objective enforces straight lines, the learned ODE trajectories $\tfrac{dx}{dt} = v_\theta(t,x)$ are generally curved.

![straight-line path and  ODE trajectories](/images/posts/2025-07-02-blog-post/1.jpg)


The Hidden Curvature Problem makes  sampling procedure still relies on multi-step numerical integration.




---
<h1 id="section2" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">2. Rectified Flow: Straighten the Trajectory </h1>


The fundamental goal of path linearization in flow matching is to **reduce the geometric complexity** of the sample trajectories that connect the initial distribution $p_0$ to the data distribution $p_1$. When trajectories are highly curved, few-step ODE integration suffers from large global errors; conversely, nearly straight trajectories can be accurately captured with very few function evaluations. 


Rectified Flow constructs a family of straight trajectories connecting data and noise with a linear interpolation form.

$$
x_t = (1-t)x_0 + t x_1, \qquad t \in [0,1]
$$

where $x_0 \sim p_{\text{init}}$ and $x_1 \sim p_{\text{data}}$, such definition set to drive the flow to follow the direction $x_1 ‚àí x_0$ of the linear path pointing from $x_0$ to $x_1$ as much as possible.


---

<h1 id="section2.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.1 Reflow: iteratively training makes the path straighter</h1>



However, as we had mentioned above, even though we force the target to be a straight line, the hidden curve problem still makes the actually solved vector field nonlinear. This discrepancy motivates **ReFlow**: an iterative method to **straighten the learned trajectories** and accelerate sampling.



ReFlow is an iterative self-distillation procedure applied on top of Rectified Flow. In practice, the first rectified flow $v_\theta^{(1)}$ learned from random couplings $$(x_0, x_1) \sim p_0 \times p_1$$ may not achieve perfect linearity. Thus, the authors propose **ReFlow**, an **iterative self-distillation** procedure:

1. **Step 1**: Use the current flow $v_\theta^{(k)}$ to generate new data pairs:
   
   $$
   x_1' = x_0' + \int_{0}^{1} v_\theta^{(k)}(t, x_t) dt, \quad (x_0' \sim \pi_0).
   $$
   
2. **Step 2**: Treat $(x_0', x_1')$ as new couplings and re-train:
   
   $$
   v_\theta^{(k+1)} = \arg\min_v
   \int_0^1 \mathbb{E}\big[|(x_1' - x_0') - v((1-t)x_0' + t x_1', t)|^2\big] dt.
   \tag{6}
   $$
   
Repeat the above steps, each iteration yields straighter trajectories and smaller transport cost.This process can be viewed as a **geometry-level self-distillation**: each generation acts as a teacher defining a more coherent flow geometry, while the student learns to approximate it using newly induced couplings.


---

<h1 id="section2.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.2 Improving the Training: One Reflow is Sufficient</h1>

In practical settings, research [^improve_rf] indicated that the trajectory curvature of the optimal **2**-rectified flow is actually close to zero. the linear interpolation paths of pairs generated by 1-Rectified Flow almost never intersect.  Suppose there are two points, $x_1\sim p_{\text{init}}$ and $x_2\sim p_{\text{init}}$, both sampled from the initial distribution, and the vector field obtained by 1-rectified flow is $v_{\theta}^1$. Then, following this vector field $v_{\theta}^1$, the sampling results for these two points are:

$$
\begin{align}
z_1=x_1+\int_{0}^{1} v_{\theta}^1(t, x_t) dt \\[10pt]
z_2=x_2+\int_{0}^{1} v_{\theta}^1(t, x_t) dt
\end{align}
$$

These two trajectories ($$x_1 \to z_1$$ and $$x_2 \to z_2$$)  almost never intersect. Since if two linear interpolation trajectories intersect, that implys that $z_2-z_1$ is parallel to $x_2-x_1$, which is almost impossible under high-dimension space.

This pivotal observation shifts the focus from adding more Reflow stages to perfecting the training of the 2-Rectified Flow itself.

unlike the 1-Rectified Flow training, the 2-Rectified Flow can be maximized the effectiveness by the following techniques.

- **U-Shaped Timestep Distribution**: The training difficulty for 2-Rectified Flow is not uniform across time t. The tasks at the endpoints‚Äîprediction at $t=0$ (from noise to image) and inversion at $t=1$ (from image back to noise)‚Äîare far more challenging than the interpolation task in the middle. 

- **Incorporating Real Data**: Training 2-rectified flow does not require real data (i.e., it can be data-free), but we can use real data  if it is available.  mixing  endpoints strategy: with probability $1-p$ take a real endpoint, with probability $p$ take a synthetic endpoint.

- **Advanced Loss Function (LPIPS-Huber Loss)**: The original L2 loss is suboptimal for image generation as it often leads to blurry results and is sensitive to outliers. The new approach leverages the knowledge that the optimal flow is straight, allowing for more flexible loss functions.

---


# <a id="section3">3. Model Distillation: Bypassing the ODE for One-Step Generation</a>

While path linearization dramatically reduces the number of required steps by simplifying the ODE's geometry, it still operates within the paradigm of numerical integration. Even a single-step Euler method on a rectified flow is fundamentally an approximation of an integral. Model distillation takes a more radical approach: it seeks to **bypass the ODE solver entirely at inference time**.

The core idea is to treat a pre-trained, multi-step generative model (the "teacher") as a high-quality data generator. A new, compact model (the "student") is then trained to replicate the teacher's final output in a single forward pass. This reframes the generative task from a path integration problem into a direct function approximation problem, aiming to learn the ODE's solution map directly.

> **In Essence:** Instead of learning the *velocity field* $v(x, t)$ and integrating it, distillation learns the *flow map* $\Phi(x_0, 0, 1)$ that transports noise $x_0$ directly to the final sample $x_1$.


## <a id="section3.1">3.1 InstaFlow: A Case Study in Multi-Objective Distillation</a>

A naive distillation using only pixel-wise loss (e.g., L2) would result in blurry, overly smoothed images, as the student would learn the average of the teacher's potential outputs. **InstaFlow** provides a powerful blueprint for effective distillation by employing a multi-objective loss function that preserves detail and realism.

Given a noise vector $z$, we compute the student's output $x_{\text{student}} = G_{\theta}(z)$ and the teacher's reference output $x_{\text{teacher}} = M_{\text{teacher}}(z)$. The student is trained to minimize a weighted sum of three critical loss components:

1.  **Reconstruction Loss ($\mathcal{L}_{\text{rec}}$)**: A standard pixel-level loss, such as L1 or L2 distance, anchors the student's output to the teacher's global structure and color.
    $$
    \mathcal{L}_{\text{rec}} = \mathbb{E}_{z \sim p_0} \left[ \| G_{\theta}(z) - x_{\text{teacher}} \|_1 \right]
    $$
    While essential, this loss alone is insufficient for capturing high-frequency details.

2.  **Perceptual Loss ($\mathcal{L}_{\text{LPIPS}}$)**: To align the outputs with human perception, a perceptual loss like LPIPS is used. It measures the distance between the two images in a deep feature space (e.g., from a VGG network), penalizing textural and structural inconsistencies that pixel-wise losses ignore.
    $$
    \mathcal{L}_{\text{LPIPS}} = \mathbb{E}_{z \sim p_0} \left[ \text{LPIPS}(G_{\theta}(z), x_{\text{teacher}}) \right]
    $$

3.  **Adversarial Loss ($\mathcal{L}_{\text{adv}}$)**: This is the most critical component for achieving realism. A discriminator network $D$ is trained to distinguish between "real" samples from the teacher ($x_{\text{teacher}}$) and "fake" samples from the student ($G_{\theta}(z)$). The student, in turn, is trained to fool the discriminator. This forces the student's output distribution to match the teacher's, effectively combating mode collapse and encouraging the generation of sharp, plausible details that might be statistically insignificant but are perceptually vital.
    $$
    \mathcal{L}_{\text{adv}} = \mathbb{E}_{z \sim p_0} \left[ -\log D(G_{\theta}(z)) \right]
    $$

The final objective is a weighted combination:
$$
\mathcal{L}_{\text{total}} = \lambda_{\text{rec}}\mathcal{L}_{\text{rec}} + \lambda_{\text{LPIPS}}\mathcal{L}_{\text{LPIPS}} + \lambda_{\text{adv}}\mathcal{L}_{\text{adv}}
$$

This multi-objective approach ensures the student not only matches the content of the teacher's output but also its stylistic and distributional properties, leading to high-quality one-step generation.
	
---

# <a id="section8">8. References</a>

[^Reflow]: Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022.

[^nvp]: Dinh L, Sohl-Dickstein J, Bengio S. Density estimation using real nvp[J]. arXiv preprint arXiv:1605.08803, 2016.

[^Glow]: Kingma D P, Dhariwal P. Glow: Generative flow with invertible 1x1 convolutions[J]. Advances in neural information processing systems, 2018, 31.

[^Neural_ODE]: Chen R T Q, Rubanova Y, Bettencourt J, et al. Neural ordinary differential equations[J]. Advances in neural information processing systems, 2018, 31.

[^Ffjord]: Grathwohl W, Chen R T Q, Bettencourt J, et al. Ffjord: Free-form continuous dynamics for scalable reversible generative models[J]. arXiv preprint arXiv:1810.01367, 2018.

[^rectified_flow]: Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022.

[^FM]: Lipman Y, Chen R T Q, Ben-Hamu H, et al. Flow matching for generative modeling[J]. arXiv preprint arXiv:2210.02747, 2022.

[^SI]: Albergo M S, Boffi N M, Vanden-Eijnden E. Stochastic interpolants: A unifying framework for flows and diffusions[J]. arXiv preprint arXiv:2303.08797, 2023.

[^SI_1]: Albergo M S, Vanden-Eijnden E. Building normalizing flows with stochastic interpolants[J]. arXiv preprint arXiv:2209.15571, 2022.

[^SI_2]: Albergo M S, Goldstein M, Boffi N M, et al. Stochastic interpolants with data-dependent couplings[J]. arXiv preprint arXiv:2310.03725, 2023.

[^improve_rf]: Lee S, Lin Z, Fanti G. Improving the training of rectified flows[J]. Advances in neural information processing systems, 2024, 37: 63082-63109.