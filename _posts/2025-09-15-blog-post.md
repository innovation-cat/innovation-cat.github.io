---
title: 'Fast Generation with Flow Matching'
date: 2025-09-15
excerpt: "Fast sampling has become a central goal in generative modeling, enabling the transition from high-fidelity but computationally intensive diffusion models to real-time generation systems. While diffusion models rely on tailored numerical solvers to mitigate the stiffness of their probability flow ODEs, flow matching defines dynamics through smooth interpolation paths, fundamentally altering the challenges of acceleration. This article provides a comprehensive overview of fast sampling in flow matching, with emphasis on path linearization strategies (e.g., Rectified Flow, ReFlow, SlimFlow, InstaFlow), the integration of consistency models, and emerging approaches such as flow generators."
permalink: /posts/2025/09/flow-matching-sampling/
tags:
  - Flow Matching
  - Acceleration
  - Consistency Models
---

<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>üìö Table of Contents</strong></span>
  </summary>
  <ul>
	<li><a href="#section1">1. From Diffusion to Flow Matching in Fast Sampling</a>
		<ul>
			<li><a href="#section1.1">1.1 The challenges of Diffusion Fast Sampling</a></li>
			<li><a href="#section1.2">1.2 Flow Matching Fast Sampling: Structural Differences and New Opportunities</a></li>
		</ul>
	</li>
	<li><a href="#section2">2. Straighten trajectories for Efficient Flow Matching</a>
		<ul>
			<li><a href="#section2.1">2.1 Motivation</a>
				<ul>
					<li><a href="#section2.1.1">2.1.1 LTE and the Role of Curvature</a></li>
					<li><a href="#section2.1.2">2.1.2 Why Path Linearization Helps</a></li>
				</ul>
			</li>
			<li><a href="#section2.2">2.2 ReFlow ‚Äî Iterative Rectification of Flow Matching</a>
				<ul>
					<li><a href="#section2.2.1">2.2.1 Implementation: How ReFlow Works</a></li>
					<li><a href="#section2.2.2">2.2.2 Why ReFlow is Correct</a></li>
				</ul>
			</li>
		</ul>
	</li>
  </ul>
</details>



While Flow Matching (FM) offers a geometrically elegant alternative to diffusion models by learning continuous probability flows rather than stochastic denoising trajectories, its inference remains computationally expensive. Despite the apparent ‚Äústraightness‚Äù of the learned flow fields, generation still requires solving an ordinary differential equation (ODE) over many discrete steps. This blog investigates why **Flow Matching is not inherently fast**, tracing the inefficiencies that persist even in rectified or linearized trajectories. We then present and analyze recent advances designed to accelerate FM sampling‚Äî**Rectified Flow**, **InstaFlow**, **Shortcut**, and **MeanFlow**‚Äîwhich progressively move from geometrical straightening to functional reformulation and finally to one-step direct mappings. By contrasting their motivations, formulations, and numerical behaviors, we reveal a unifying view of *flow acceleration* as a shift from learning instantaneous velocity fields to learning integral or averaged mappings that approximate the entire transport process in minimal function evaluations. The discussion concludes by outlining open challenges toward achieving *true one-step generative flows* that bridge physical interpretability, numerical stability, and computational efficiency.


---

<h1 id="section1" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">1. Introduction: FM  Faster Than Diffusion-Based PF-ODEs</h1>


Flow Matching (FM) [^FM] was originally proposed as a deterministic and conceptually elegant alternative to diffusion models. Instead of modeling noisy Markov chains or stochastic denoising processes, FM directly learns a **continuous velocity field** $ v_\theta(x, t)$ that defines a **probability flow ODE**. This ODE transports samples smoothly from a simple prior distribution (e.g., Gaussian noise) to the target data distribution. In principle, this framework eliminates stochasticity, reduces sampling variance, and offers a clean geometric interpretation of generation as a continuous mass transport problem.



Although both DM and FM can be expressed as deterministic ODEs during inference, they differ fundamentally in how these ODEs are defined and what they are trained to approximate. The key lies not in the mathematical form of the equations, but in the **origin of the training objective** and the **geometry of the learned transport paths**.



---

<h1 id="section1.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">1.1 Diffusion PF-ODEs: A By-Product of a Stochastic Process</h1>




In diffusion models, the forward dynamics follow a stochastic differential equation (SDE):

$$
dx=f(t,x)\,dt+g(t)\,dw_t
$$

where $w_t$ denotes Brownian noise.  The model is trained to predict auxiliary targets‚Äînoise $\epsilon_\theta$, score $\nabla_x\log p_t(x)$, or the denoised sample $x_{0,\theta}$‚Äîthat are defined with respect to this noisy diffusion process.
During inference, one can reformulate the reverse SDE into a deterministic **probability-flow ODE (PF-ODE)**:

$$
\frac{dx}{dt}=f(t,x)-\tfrac{1}{2}g(t)^2 s_\theta(x,t).
$$

Although this PF-ODE yields the same marginal distribution as the reverse SDE, it is **not the quantity being optimized during training**; it merely arises as an analytical consequence of the stochastic objective. Therefore, the learned flow inherits the geometric irregularities of the noisy trajectories.  The resulting velocity field remains highly curved in high-dimensional space, forcing numerical solvers to use small step sizes to maintain accuracy and stability.

---

<h1 id="section1.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">1.2 Flow Matching: Learning the ODE Itself</h1>


Flow Matching inverts this paradigm.  It **defines the deterministic ODE as the primary learning target** rather than as a by-product. Given a source sample $x_0$ and a target data $x_1$, FM introduces an explicit interpolation

$$
x_t = \alpha_t x_0 + \beta_t x_1,
$$

and supervises the network to approximate the true conditional velocity

$$
v_t = \frac{d}{dt}x_t = \dot\alpha_t x_0 + \dot\beta_t x_1.
$$

The model minimizes

$$
\mathcal{L}_{\text{FM}}= \mathbb{E}_{x_0,\epsilon,t}\left[\|v_\theta(x_t,t)-v_t\|^2\right],
$$

which directly teaches the network how to move data points through time. Unlike diffusion models‚Äîwhere the PF-ODE is recovered **after** training‚ÄîFM optimizes the ODE **itself**. As a result, the learned field $v_\theta(x,t)$ tends to be **geometrically straight and globally smooth**, leading to deterministic transport with lower curvature and improved numerical stability.

In geometric terms, diffusion‚Äôs PF-ODE performs a **passive simplification** of stochastic paths, whereas Flow Matching performs an **active planning** of deterministic, near-geodesic trajectories. This distinction explains why FM usually achieves high-fidelity synthesis with only **10‚Äì20 NFEs**, compared to the **50‚Äì100 NFEs** typically required by diffusion-based PF-ODEs.


---
<h1 id="section2" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">2. RF Bottleneck: Straight but Not Instantaneous</h1>

We had discussed Rectified Flow, a specifical case of FM, who defines the probability path as a straight line from the start point ($x_0 \sim p_{\text{init}}$) to the end point ($x_1 \sim p_{\text{data}}$) through linear interpolation. Therefore, in theory, RF can achieve one-step generation.

$$
x_t = (1-t)x_0 + tx_1\, \Longrightarrow\, \frac{dx_t}{dt} = x_1-x_0 
$$
 
However, in practice, the trajectory of the ODE remains a curve. The core reason is thar the learned vector field is the **marginal velocity field**, i.e. the conditional expectation of all possible straight-line velocities passing through $x_t$. 

$$
v_t(x) = \mathbb{E}\big[\partial_t x_t \,\big|\, x_t = x\big] = \mathbb{E}\big[x_1 - x_0 \,\big|\, x_t = x\big].
$$

Therefore, what the model learns is **not** the deterministic velocity of a specific straight-line path, but the conditional expectation of velocities across **all** possible paths at that point. The figure shows the straight-line paths between different sample pairs, and the connecting line between any two paired points satisfies our predefined straight-line path.

![straight-line of any Two paired points](/images/posts/2025-09-15-blog-post/pair_line.jpg)


However, given this sample dataset, the obtained ODE trajectory is curved, as shown in the following figure.

![vector fields](/images/posts/2025-09-15-blog-post/average_vector_field.jpg)

This means even though the training objective enforces straight lines, the learned ODE trajectories $\tfrac{dx}{dt} = v_\theta(t,x)$ are generally curved.

![straight-line path and  ODE trajectories](/images/posts/2025-07-02-blog-post/1.jpg)


The Hidden Curvature Problem makes  sampling procedure still relies on multi-step numerical integration.




---
<h1 id="section2" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">2. Rectified Flow: Straighten the Trajectory </h1>


The fundamental goal of path linearization in flow matching is to **reduce the geometric complexity** of the sample trajectories that connect the initial distribution $p_0$ to the data distribution $p_1$. When trajectories are highly curved, few-step ODE integration suffers from large global errors; conversely, nearly straight trajectories can be accurately captured with very few function evaluations. 


Rectified Flow constructs a family of straight trajectories connecting data and noise with a linear interpolation form.

$$
x_t = (1-t)x_0 + t x_1, \qquad t \in [0,1]
$$

where $x_0 \sim p_{\text{init}}$ and $x_1 \sim p_{\text{data}}$, such definition set to drive the flow to follow the direction $x_1 ‚àí x_0$ of the linear path pointing from $x_0$ to $x_1$ as much as possible.


---

<h1 id="section2.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.1 Reflow: iteratively training makes the path straighter</h1>



However, as we had mentioned above, even though we force the target to be a straight line, the hidden curve problem still makes the actually solved vector field nonlinear. This discrepancy motivates **ReFlow**: an iterative method to **straighten the learned trajectories** and accelerate sampling.



ReFlow is an iterative self-distillation procedure applied on top of Rectified Flow. In practice, the first rectified flow $v_\theta^{(1)}$ learned from random couplings $$(x_0, x_1) \sim p_0 \times p_1$$ may not achieve perfect linearity. Thus, the authors propose **ReFlow**, an **iterative self-distillation** procedure:

1. **Step 1**: Use the current flow $v_\theta^{(k)}$ to generate new data pairs:
   
   $$
   x_1' = x_0' + \int_{0}^{1} v_\theta^{(k)}(t, x_t) dt, \quad (x_0' \sim \pi_0).
   $$
   
2. **Step 2**: Treat $(x_0', x_1')$ as new couplings and re-train:
   
   $$
   v_\theta^{(k+1)} = \arg\min_v
   \int_0^1 \mathbb{E}\big[|(x_1' - x_0') - v((1-t)x_0' + t x_1', t)|^2\big] dt.
   \tag{6}
   $$
   
Repeat the above steps, each iteration yields straighter trajectories and smaller transport cost.This process can be viewed as a **geometry-level self-distillation**: each generation acts as a teacher defining a more coherent flow geometry, while the student learns to approximate it using newly induced couplings.


---

<h1 id="section2.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.2 Improving the Training: One Reflow is Sufficient</h1>

In practical settings, research [^improve_rf] indicated that the trajectory curvature of the optimal **2**-rectified flow is actually close to zero. the linear interpolation paths of pairs generated by 1-Rectified Flow almost never intersect.  Suppose there are two points, $x_1\sim p_{\text{init}}$ and $x_2\sim p_{\text{init}}$, both sampled from the initial distribution, and the vector field obtained by 1-rectified flow is $v_{\theta}^1$. Then, following this vector field $v_{\theta}^1$, the sampling results for these two points are:

$$
\begin{align}
z_1=x_1+\int_{0}^{1} v_{\theta}^1(t, x_t) dt \\[10pt]
z_2=x_2+\int_{0}^{1} v_{\theta}^1(t, x_t) dt
\end{align}
$$

These two trajectories ($$x_1 \to z_1$$ and $$x_2 \to z_2$$)  almost never intersect. Since if two linear interpolation trajectories intersect, that implys that $z_2-z_1$ is parallel to $x_2-x_1$, which is almost impossible under high-dimension space.

This pivotal observation shifts the focus from adding more Reflow stages to perfecting the training of the 2-Rectified Flow itself.

unlike the 1-Rectified Flow training, the 2-Rectified Flow can be maximized the effectiveness by the following techniques.

- **U-Shaped Timestep Distribution**: The training difficulty for 2-Rectified Flow is not uniform across time t. The tasks at the endpoints‚Äîprediction at $t=0$ (from noise to image) and inversion at $t=1$ (from image back to noise)‚Äîare far more challenging than the interpolation task in the middle. 

- **Incorporating Real Data**: Training 2-rectified flow does not require real data (i.e., it can be data-free), but we can use real data  if it is available.  mixing  endpoints strategy: with probability $1-p$ take a real endpoint, with probability $p$ take a synthetic endpoint.

- **Advanced Loss Function (LPIPS-Huber Loss)**: The original L2 loss is suboptimal for image generation as it often leads to blurry results and is sensitive to outliers. The new approach leverages the knowledge that the optimal flow is straight, allowing for more flexible loss functions.

---

<h1 id="section3" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">3. InstaFlow: Text-Conditioned Rectified Flow with One-Step Distillation</h1>

In previous articles ([Accelerating Diffusion Sampling](https://innovation-cat.github.io/posts/2025/05/diffusion-sampling/)), we introduced how to use distillation to accelerate the sampling of diffusion models. However, as we discussed in Sec. [1.1.1](#section1.1.1), diffusion models are based on the path strategy of SDEs, while PF-ODE being merely a by-product, which leads to suboptimal results in distillation. **InstaFlow** [^instaflow] extends this line of work by combining **Rectified Flow** with **ReFlow** and a final **one-step distillation**, transforming a large diffusion model (e.g., Stable Diffusion) into a high-quality, single-step flow generator.



---

<h1 id="section3.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.1 Overview: From Stable Diffusion to One-Step Flow</h1>


InstaFlow can be viewed as a two-stage rectification and distillation pipeline:

1. **Teacher Initialization**: Start from a pretrained diffusion model (e.g., Stable Diffusion) and interpret its deterministic sampling process as a probability flow ODE:

   $$
   \frac{dx}{dt} = v_{SD}(x, t \mid T)
   $$

   where $T$ is the text condition. By integrating this ODE for 20‚Äì50 steps, the model generates pairs $(x_0, x_1)$ ‚Äî noise to image mappings under given text conditions.

2. **ReFlow Rectification**: Apply Rectified Flow training on the collected dataset $(x_0, x_1, T)$, learning a smoother and more linear velocity field $v_{\theta}(x,t\mid T)$. This is the **text-conditioned ReFlow stage**, which regularizes the geometry of the generative flow under each prompt $T$.

3. **Distillation to One-Step Flow**: A student network $\tilde{v}(x_0\mid T)$ is trained to approximate the final displacement:

   $$
   x_1 \approx x_0 + \tilde{v}(x_0 \mid T)
   $$

   using a perceptual distance loss (LPIPS). This collapses the full ODE integration into a single Euler step, enabling instantaneous generation.


---

<h1 id="section3.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.2 Text-Conditioned ReFlow</h1>   



Unlike the original ReFlow which is unconditional, InstaFlow introduces **text-conditioned Rectified Flow** by conditioning $v(x,t)$ on textual embeddings $T$ through cross-attention. The training objective becomes:

$$
\mathcal{L}_{\text{ReFlow-Cond}} = \mathbb{E}_{X_0,T}\left[\int_0^1 \|(x_1(T)-x_0) - v(x_t, t \mid T)\|^2 dt\right],
$$

where $x_1(T)$ is generated via the teacher Stable Diffusion model under the same text condition $T$. Each ReFlow iteration improves the local straightness of conditional trajectories without altering their marginal image distribution.

This step is essential because conditional generation introduces multiple sub-manifolds (different prompts induce different flow geometries). Text-conditioned ReFlow ensures each submanifold‚Äôs flow is straightened individually, preserving semantic alignment across conditions.

---

<h1 id="section3.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.3 One-Step Distillation</h1>   

After obtaining a 2-Rectified Flow teacher, InstaFlow performs a final distillation to a one-step generator. The student network learns a displacement field that directly maps noise to image:

$$
\tilde{v} = \arg\min_v \mathbb{E}_{X_0,T}[ D( ODE[v_T](X_0\mid T), X_0 + v(X_0\mid T) ) ],
$$

where $D(\cdot, \cdot)$ denotes a perceptual similarity loss (e.g., LPIPS + Huber). The resulting model, InstaFlow, generates a 512√ó512 image in **~0.09 seconds** with quality comparable to 20‚Äì50 step diffusion sampling.



---

<h1 id="section3.4" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.4 Practical Insights and Limitations</h1> 

In essence, InstaFlow extends ReFlow into the conditional domain and compresses the entire ODE integration into a single learned displacement. It demonstrates that rectifying and distilling a diffusion teacher can yield **true one-step generation** without sacrificing image quality.


- **Text Coverage Issue:** Since ReFlow training relies on data generated by Stable Diffusion, text conditions $T$ that are rarely sampled yield few training pairs $(x_0, x_1, T)$. As a result, InstaFlow inherits the coverage limitations of its teacher: performance degrades for rare or unseen prompts.

- **Improvement Strategies:** Expanding the prompt dataset, applying prompt resampling (e.g., CLIP-based diversity sampling), or self-distillation with prompt augmentation can mitigate this bias.

- **Relation to Diffusion Distillation:** The same limitation exists in all conditional distillation frameworks ‚Äî missing conditions lead to poor generalization. InstaFlow emphasizes path rectification before distillation, which reduces geometric variance and improves knowledge transfer.



---

<h1 id="section4" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">4. MeanFlow: Learning Mean Velocity Fields for One-Step Generation</h1>


Although Flow Matching (FM) and Rectified Flow (RF) have provided a unified ODE view of generative modeling, both still rely on **instantaneous velocity fields** $v(x_t, t)$ and therefore require **numerical integration** during sampling. When the integration step is coarse (e.g., 1‚Äì4 steps), numerical error accumulates rapidly because the **marginal flow trajectories** are not perfectly straight even if the **conditional flow** is linear. This limitation makes **true one-step generation** (1-NFE) difficult: the model is trained to predict a continuous-time velocity field but is asked to execute a large discrete jump during inference.



To overcome this structural mismatch, **MeanFlow** [^meanflow] proposes to replace the instantaneous velocity by its **temporal average**, thus directly learning a quantity that corresponds to a **finite displacement** rather than an infinitesimal one. By learning the **mean velocity field**

$$
u(x_t, t, t_{\text{end}}) = \frac{1}{t_{\text{end}}-t} \int_t^{t_{\text{end}}} v(x_\tau, \tau)\, d\tau,
$$


The relation between instantaneous velocity and mean velocity is shown as follows.

![Mean Flow](/images/posts/2025-09-15-blog-post/meanflow.jpg)

MeanFlow explicitly models the average motion between two time instants $t$ and $t_{\text{end}}$.

$$
x_{t_{\text{end}}} = x_t + (t_{\text{end}}-t)\,u(x_t, t, t_{\text{end}})
$$


This design removes the need for numerical ODE integration: a single evaluation of (u) gives a displacement estimate capable of jumping directly from noise $(t{=}1)$ to data $(r{=}0)$.

$$
x_1 = x_0 + u(x_0, 0, 1)
$$



---

<h1 id="section4.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">4.1 MeanFlow Identity</h1> 



Differentiating the definition of $u$ with respect to $t$ gives

$$
\frac{d}{dt}\!\left[(t_{\text{end}}-t)u(x_t, t, t_{\text{end}})\right] = \frac{d}{dt} \int_t^{t_{\text{end}}} v(x_\tau, \tau)\, d\tau = -v(x_t, t).
$$

Applying the product rule to the left side, leading to the **MeanFlow Identity**

$$
\boxed{\,
u(x_t, t, t_{\text{end}})
= v(x_t, t) + (t_{\text{end}}-t)\,\frac{d}{dt}u(x_t, t, t_{\text{end}})\label{eq:meanflow1}
\,}
$$

This equation forms the theoretical foundation of MeanFlow: it expresses the instantaneous velocity in terms of the mean velocity and its total derivative. Conversely, the identity uniquely determines $u$ given $v$ and vice versa.

The total derivative $\tfrac{d}{dt}u(x_t, t, t_{\text{end}})$ contains both explicit and implicit time dependence, using chain rule gives 

$$
\begin{align}
\frac{d}{dt}u(x_t, t, t_{\text{end}})
& = \underbrace{\frac{dx_t}{dt}}_{v(x_t, t)} \partial_{x_t} u(x_t, t, t_{\text{end}})
+ \underbrace{\frac{dt}{dt}}_{1} \partial_{t} u(x_t, t, t_{\text{end}}) + \underbrace{\frac{dt_{\text{end}}}{dt}}_{0} \partial_{t_{\text{end}}} u(x_t, t, t_{\text{end}}) \\[10pt]
& = v(x_t, t) \partial_{x_t} u(x_t, t, t_{\text{end}})+ \partial_{t} u(x_t, t, t_{\text{end}}) \\[10pt]
& = \underbrace{[\partial_{x_t} u, \partial_{t} u, \partial_{t_{\text{end}}} u]}_{\text{Jacobian Matrix}} \cdot [v, 1, 0]^T\label{eq:meanflow2}
\end{align}
$$

This is a Jacobian-vector product (JVP) that can be efficiently computed with modern autodiff frameworks (PyTorch, JAX) without incurring second-order gradient cost, as gradients are detached from the target branch.


---

<h1 id="section4.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">4.2 Training Objective</h1> 


Let $u_\theta(x_t, t, t_{\text{end}})$ denotes the neural network predicting the mean velocity. Substituting Eq. \ref{eq:meanflow2} into Eq. \ref{eq:meanflow2} yields a regression target:

$$
u_{\text{target}}
= v(x_t, t) + (t_{\text{end}}-t)(v(x_t, t)\partial_{x_t} u + \partial_{t} u)
$$

where $$v_t = a_t' x_0 + b_t' \epsilon$$ is the **conditional velocity**, an unbiased estimator of the marginal $v(z_t, t)$. The training loss is a simple mean-squared error:

$$
\mathcal L(\theta)
= \big\|
u_\theta(x_t, t, t_{\text{end}})
- \text{stopgrad}(u_{\text{target}})
\big\|_2^2.
$$

When $t_{\text{end}}{=}t$, the second term vanishes and the loss degenerates to Flow Matching, ensuring compatibility.

Sampling rule (1-NFE).

$$
x_1
= \epsilon
+ u_\theta(x_0, t{=}0, t_{\text{end}}{=}1).
$$

Thus one forward pass through $u_\theta$ suffices to map pure noise to data.

---

# <a id="section8">8. References</a>

[^Reflow]: Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022.

[^nvp]: Dinh L, Sohl-Dickstein J, Bengio S. Density estimation using real nvp[J]. arXiv preprint arXiv:1605.08803, 2016.

[^Glow]: Kingma D P, Dhariwal P. Glow: Generative flow with invertible 1x1 convolutions[J]. Advances in neural information processing systems, 2018, 31.

[^Neural_ODE]: Chen R T Q, Rubanova Y, Bettencourt J, et al. Neural ordinary differential equations[J]. Advances in neural information processing systems, 2018, 31.

[^Ffjord]: Grathwohl W, Chen R T Q, Bettencourt J, et al. Ffjord: Free-form continuous dynamics for scalable reversible generative models[J]. arXiv preprint arXiv:1810.01367, 2018.

[^rectified_flow]: Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022.

[^FM]: Lipman Y, Chen R T Q, Ben-Hamu H, et al. Flow matching for generative modeling[J]. arXiv preprint arXiv:2210.02747, 2022.

[^SI]: Albergo M S, Boffi N M, Vanden-Eijnden E. Stochastic interpolants: A unifying framework for flows and diffusions[J]. arXiv preprint arXiv:2303.08797, 2023.

[^SI_1]: Albergo M S, Vanden-Eijnden E. Building normalizing flows with stochastic interpolants[J]. arXiv preprint arXiv:2209.15571, 2022.

[^SI_2]: Albergo M S, Goldstein M, Boffi N M, et al. Stochastic interpolants with data-dependent couplings[J]. arXiv preprint arXiv:2310.03725, 2023.

[^improve_rf]: Lee S, Lin Z, Fanti G. Improving the training of rectified flows[J]. Advances in neural information processing systems, 2024, 37: 63082-63109.

[^instaflow]: Liu X, Zhang X, Ma J, et al. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation[C]//The Twelfth International Conference on Learning Representations. 2023.

[^meanflow]: Geng Z, Deng M, Bai X, et al. Mean flows for one-step generative modeling[J]. arXiv preprint arXiv:2505.13447, 2025.