---
title: 'Fast Generation with Flow Matching'
date: 2025-09-15
excerpt: "Fast sampling has become a central goal in generative modeling, enabling the transition from high-fidelity but computationally intensive diffusion models to real-time generation systems. While diffusion models rely on tailored numerical solvers to mitigate the stiffness of their probability flow ODEs, flow matching defines dynamics through smooth interpolation paths, fundamentally altering the challenges of acceleration. This article provides a comprehensive overview of fast sampling in flow matching, with emphasis on path linearization strategies (e.g., Rectified Flow, ReFlow, SlimFlow, InstaFlow), the integration of consistency models, and emerging approaches such as flow generators."
permalink: /posts/2025/09/flow-matching-sampling/
tags:
  - Flow Matching
  - Acceleration
  - Consistency Models
---

<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>üìö Table of Contents</strong></span>
  </summary>
  <ul>
	<li><a href="#section1">1. From Diffusion to Flow Matching in Fast Sampling</a>
		<ul>
			<li><a href="#section1.1">1.1 The challenges of Diffusion Fast Sampling</a></li>
			<li><a href="#section1.2">1.2 Flow Matching Fast Sampling: Structural Differences and New Opportunities</a></li>
		</ul>
	</li>
	<li><a href="#section2">2. Straighten trajectories for Efficient Flow Matching</a>
		<ul>
			<li><a href="#section2.1">2.1 Motivation</a>
				<ul>
					<li><a href="#section2.1.1">2.1.1 LTE and the Role of Curvature</a></li>
					<li><a href="#section2.1.2">2.1.2 Why Path Linearization Helps</a></li>
				</ul>
			</li>
			<li><a href="#section2.2">2.2 ReFlow ‚Äî Iterative Rectification of Flow Matching</a>
				<ul>
					<li><a href="#section2.2.1">2.2.1 Implementation: How ReFlow Works</a></li>
					<li><a href="#section2.2.2">2.2.2 Why ReFlow is Correct</a></li>
				</ul>
			</li>
		</ul>
	</li>
  </ul>
</details>



While Flow Matching (FM) offers a geometrically elegant alternative to diffusion models by learning continuous probability flows rather than stochastic denoising trajectories, its inference remains computationally expensive. Despite the apparent ‚Äústraightness‚Äù of the learned flow fields, generation still requires solving an ordinary differential equation (ODE) over many discrete steps. This blog investigates why **Flow Matching is not inherently fast**, tracing the inefficiencies that persist even in rectified or linearized trajectories. We then present and analyze recent advances designed to accelerate FM sampling‚Äî**Rectified Flow**, **InstaFlow**, **Shortcut**, and **MeanFlow**‚Äîwhich progressively move from geometrical straightening to functional reformulation and finally to one-step direct mappings. By contrasting their motivations, formulations, and numerical behaviors, we reveal a unifying view of *flow acceleration* as a shift from learning instantaneous velocity fields to learning integral or averaged mappings that approximate the entire transport process in minimal function evaluations. The discussion concludes by outlining open challenges toward achieving *true one-step generative flows* that bridge physical interpretability, numerical stability, and computational efficiency.


---

<h1 id="section1" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">1. Introduction: Why Flow Matching Is Not Fast Enough</h1>


Flow Matching (FM) [^FM] was originally proposed as a deterministic and conceptually elegant alternative to diffusion models. Instead of modeling noisy Markov chains or stochastic denoising processes, FM directly learns a **continuous velocity field** $ v_\theta(x, t)$ that defines a **probability flow ODE**. This ODE transports samples smoothly from a simple prior distribution (e.g., Gaussian noise) to the target data distribution. In principle, this framework eliminates stochasticity, reduces sampling variance, and offers a clean geometric interpretation of generation as a continuous mass transport problem.


---

<h1 id="section1.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">1.1 Why FM Can Be Faster Than Diffusion-Based PF-ODEs</h1>


Although both DM and FM can be expressed as deterministic ODEs during inference, they differ fundamentally in how these ODEs are defined and what they are trained to approximate. The key lies not in the mathematical form of the equations, but in the **origin of the training objective** and the **geometry of the learned transport paths**.



---

<h1 id="section1.1.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">1.1.1 Diffusion PF-ODEs: A By-Product of a Stochastic Process</h1>




In diffusion models, the forward dynamics follow a stochastic differential equation (SDE):

$$
dx=f(t,x)\,dt+g(t)\,dw_t
$$

where $w_t$ denotes Brownian noise.  The model is trained to predict auxiliary targets‚Äînoise $\epsilon_\theta$, score $\nabla_x\log p_t(x)$, or the denoised sample $x_{0,\theta}$‚Äîthat are defined with respect to this noisy diffusion process.
During inference, one can reformulate the reverse SDE into a deterministic **probability-flow ODE (PF-ODE)**:

$$
\frac{dx}{dt}=f(t,x)-\tfrac{1}{2}g(t)^2 s_\theta(x,t).
$$

Although this PF-ODE yields the same marginal distribution as the reverse SDE, it is **not the quantity being optimized during training**; it merely arises as an analytical consequence of the stochastic objective. Therefore, the learned flow inherits the geometric irregularities of the noisy trajectories.  The resulting velocity field remains highly curved in high-dimensional space, forcing numerical solvers to use small step sizes to maintain accuracy and stability.

---

<h1 id="section1.1.2" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">1.1.2 Diffusion PF-ODEs: A By-Product of a Stochastic Process</h1>


Flow Matching inverts this paradigm.  It **defines the deterministic ODE as the primary learning target** rather than as a by-product. Given a source sample (x_0) and a target noise (\epsilon), FM introduces an explicit interpolation

$$
x_t = (1-\alpha_t)x_0 + \alpha_t\epsilon,
$$

and supervises the network to approximate the true conditional velocity

$$
v_t = \frac{d}{dt}x_t = \dot\alpha_t(\epsilon - x_0).
$$

The model minimizes

$$
\mathcal{L}_{\text{FM}}= \mathbb{E}_{x_0,\epsilon,t}\left[\|v_\theta(x_t,t)-v_t\|^2\right],
$$

which directly teaches the network how to move data points through time. Unlike diffusion models‚Äîwhere the PF-ODE is recovered **after** training‚ÄîFM optimizes the ODE **itself**. As a result, the learned field $v_\theta(x,t)$ tends to be **geometrically straight and globally smooth**, leading to deterministic transport with lower curvature and improved numerical stability.

In geometric terms, diffusion‚Äôs PF-ODE performs a **passive simplification** of stochastic paths, whereas Flow Matching performs an **active planning** of deterministic, near-geodesic trajectories. This distinction explains why FM usually achieves high-fidelity synthesis with only **10‚Äì20 NFEs**, compared to the **50‚Äì100 NFEs** typically required by diffusion-based PF-ODEs.


---

<h1 id="section1.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">1.2 The Remaining Bottleneck: Straight but Not Instantaneous</h1>

Although Flow Matching yields smoother and more linear flows. However, the learned vector field is the **marginal velocity field**, i.e. the conditional expectation of all possible straight-line velocities passing through $x_t$. 

$$
v_t(x) = \mathbb{E}\big[\partial_t x_t \,\big|\, x_t = x\big].
$$

Therefore, what the model learns is **not** the deterministic velocity of a specific straight-line path, but the conditional expectation of velocities across **all** possible paths at that point. This means even though the training objective enforces straight lines, the learned ODE trajectories $\tfrac{dx}{dt} = v_\theta(t,x)$ are generally curved.

![straight-line path and  ODE trajectories](/images/posts/2025-07-02-blog-post/1.jpg)


The Hidden Curvature Problem makes  sampling procedure still relies on multi-step numerical integration.




---

# <a id="section2">2. Straighten trajectories for Efficient Flow Matching</a>


The fundamental goal of path linearization in flow matching is to **reduce the geometric complexity** of the sample trajectories that connect the initial distribution $p_0$ to the data distribution $p_1$. When trajectories are highly curved, few-step ODE integration suffers from large global errors; conversely, nearly straight trajectories can be accurately captured with very few function evaluations. This phenomenon can be understood by examining the role of higher-order derivatives in numerical integration error.

---

## <a id="section2.1">2.1 Motivation</a>

Before introducing the concrete algorithms, let‚Äôs first analyze why a straighter path can lead to faster sampling. Although this seems very intuitive, we still hope it can be explained mathematically.


### <a id="section2.1.1">2.1.1 LTE and the Role of Curvature</a>

Consider solving an ODE

$$
\frac{dx}{dt} = f(t, x)
$$

with step size $h$. The exact solution admits a Taylor expansion at $t$:

$$
x(t+h) = x(t) + h x'(t) + \tfrac{h^2}{2} x''(t) + \tfrac{h^3}{6} x^{(3)}(t) + \cdots .
$$

A first-order method such as Euler‚Äôs scheme only retains the first derivative term:

$$
x(t+h) \approx x(t) + h f(t, x(t)).
$$

The **local truncation error (LTE)** is therefore dominated by

$$
\text{LTE} \;\approx\; \tfrac{h^2}{2} x''(t) + \mathcal{O}(h^3).
$$

This shows explicitly that the second derivative $$x''(t)$$ ‚Äî i.e., the **curvature of the trajectory** ‚Äî acts as a multiplicative factor in the error constant. For higher-order integrators (Runge‚ÄìKutta of order $p$), the leading error terms similarly involve derivatives up to $x^{(p+1)}(t)$. Thus, trajectories with larger curvature inevitably incur larger numerical error for the same step size.

---


### <a id="section2.1.2">2.1.2 Why Path Linearization Helps</a>

Path linearization strategies aim to **design or refine the interpolation path** between $p_0$ and $p_1$ so that trajectories are as straight as possible, thereby reducing $$\|x''(t)\|$$ and higher-order derivatives. When curvature is small:

- The error constants in numerical integrators shrink,
- Few-step integration (even one-step) can approximate the full trajectory reliably,
- The sampling process is no longer limited by stiffness or oscillatory dynamics.

In other words, **straightening trajectories reduces the dependence of numerical error on higher-order derivatives**, making fast sampling feasible. This geometric perspective explains why methods such as Rectified Flow, ReFlow, and subsequent linearization techniques are effective in enabling one-step or few-step generation.



---
## <a id="section2.1">2.1 ReFlow: Iterative Rectification of Flow Matching</a>

Flow Matching (FM) defines a *straight path* between a base distribution $p_0$ (e.g., Gaussian noise) and the data distribution $p_1$. In training, the target velocity field corresponds to the tangent of this straight line:

$$
x_t = (1-t)x_0 + t x_1, \qquad \partial_t x_t = x_1 - x_0.
$$

However, the model is only conditioned on $(t, x_t)$ and does not observe the specific pair $(x_0, x_1)$. As a result, the learned vector field is the **marginal velocity field**, i.e. the conditional expectation of all possible straight-line velocities passing through $x_t$. 

$$
v_t(x) = \mathbb{E}\big[\partial_t x_t \,\big|\, x_t = x\big].
$$

Therefore, what the model learns is **not** the deterministic velocity of a specific straight-line path, but the conditional expectation of velocities across all possible paths at that point. This means even though the training objective enforces straight lines, the learned ODE trajectories $\tfrac{dx}{dt} = v_\theta(t,x)$ are generally curved.

![straight-line path and  ODE trajectories](/images/posts/2025-07-02-blog-post/1.jpg)

This discrepancy motivates **ReFlow (Rectified Flow)**: an iterative method to **straighten the learned trajectories** and accelerate sampling.

---

### <a id="section2.2.1">2.2.1 Implementation: How ReFlow Works</a>

ReFlow [^Reflow] is an iterative self-distillation procedure applied on top of FM:

1. **Warmup with real endpoints**

   * Train the model with real data samples $x_1 \sim p_{\text{data}}$.
   * This anchors the velocity field to the true data manifold.

2. **Synthetic endpoints generation**

   * Use the current (EMA) model as a teacher.
   * Sample from $p_0$ and integrate the learned ODE to obtain synthetic endpoints $\tilde{x}_1$.
   * These endpoints may be off-manifold or of imperfect quality.

3. **Mixing strategy**

   * Build a training buffer that contains both real endpoints and synthetic endpoints.
   * During training, each minibatch uses a mixture: with probability $1-p$ take a real endpoint, with probability $p$ take a synthetic endpoint.
   * The mixing ratio $p$ is gradually increased across rounds (e.g., from 0.4 ‚Üí 0.9).

4. **Rectification step**

   * Regardless of whether the endpoint is real or synthetic, the teacher velocity is again defined as the **straight-line velocity**:

     $$
     u^{\star}(x_t) = \frac{x_1 - x_0}{1-t} \quad \text{(or equivalently } x_1 - x_0 \text{ for straight paths)}.
     $$
   * Thus the model is always forced to interpret the path as a straight line between the chosen endpoints.

5. **Iteration**

   * Repeat: generate new synthetic endpoints with the updated model, refill the buffer, retrain with mixing.
   * After a few rounds, the flow field becomes increasingly straightened.

### <a id="section2.2.2">2.2.2 Why ReFlow is Correct</a>

A natural concern is: *if synthetic endpoints are poor quality or off-manifold, wouldn‚Äôt the model drift further away from real data?* ReFlow avoids this problem through two safeguards:

1. **Real endpoints remain in training**

   * Real data samples are always part of the mixture.
   * They serve as an anchor, preventing the flow field from collapsing onto low-quality synthetic samples.

2. **Straight-path supervision is independent of endpoint quality**

   * Even if a synthetic endpoint $\tilde{x}_1$ is imperfect, the loss still enforces a *straight-line path* between $x_0$ and $\tilde{x}_1$.
   * This ‚Äúrectification‚Äù objective simplifies the geometry of the velocity field, making trajectories straighter, regardless of endpoint realism.

3. **Self-distillation effect**

   * Similar to knowledge distillation, the model learns from both ground-truth endpoints and its own outputs.
   * Over iterations, the velocity field becomes smoother and more consistent, which leads to improved sampling efficiency.

4. **EMA teacher + gradual mixing**

   * Using an exponential moving average (EMA) teacher to generate synthetic samples improves stability.
   * Gradually increasing the mixing ratio $p$ ensures the model does not overfit to poor synthetic samples in early rounds.


---


# <a id="section3">3. Model Distillation: Bypassing the ODE for One-Step Generation</a>

While path linearization dramatically reduces the number of required steps by simplifying the ODE's geometry, it still operates within the paradigm of numerical integration. Even a single-step Euler method on a rectified flow is fundamentally an approximation of an integral. Model distillation takes a more radical approach: it seeks to **bypass the ODE solver entirely at inference time**.

The core idea is to treat a pre-trained, multi-step generative model (the "teacher") as a high-quality data generator. A new, compact model (the "student") is then trained to replicate the teacher's final output in a single forward pass. This reframes the generative task from a path integration problem into a direct function approximation problem, aiming to learn the ODE's solution map directly.

> **In Essence:** Instead of learning the *velocity field* $v(x, t)$ and integrating it, distillation learns the *flow map* $\Phi(x_0, 0, 1)$ that transports noise $x_0$ directly to the final sample $x_1$.


## <a id="section3.1">3.1 InstaFlow: A Case Study in Multi-Objective Distillation</a>

A naive distillation using only pixel-wise loss (e.g., L2) would result in blurry, overly smoothed images, as the student would learn the average of the teacher's potential outputs. **InstaFlow** provides a powerful blueprint for effective distillation by employing a multi-objective loss function that preserves detail and realism.

Given a noise vector $z$, we compute the student's output $x_{\text{student}} = G_{\theta}(z)$ and the teacher's reference output $x_{\text{teacher}} = M_{\text{teacher}}(z)$. The student is trained to minimize a weighted sum of three critical loss components:

1.  **Reconstruction Loss ($\mathcal{L}_{\text{rec}}$)**: A standard pixel-level loss, such as L1 or L2 distance, anchors the student's output to the teacher's global structure and color.
    $$
    \mathcal{L}_{\text{rec}} = \mathbb{E}_{z \sim p_0} \left[ \| G_{\theta}(z) - x_{\text{teacher}} \|_1 \right]
    $$
    While essential, this loss alone is insufficient for capturing high-frequency details.

2.  **Perceptual Loss ($\mathcal{L}_{\text{LPIPS}}$)**: To align the outputs with human perception, a perceptual loss like LPIPS is used. It measures the distance between the two images in a deep feature space (e.g., from a VGG network), penalizing textural and structural inconsistencies that pixel-wise losses ignore.
    $$
    \mathcal{L}_{\text{LPIPS}} = \mathbb{E}_{z \sim p_0} \left[ \text{LPIPS}(G_{\theta}(z), x_{\text{teacher}}) \right]
    $$

3.  **Adversarial Loss ($\mathcal{L}_{\text{adv}}$)**: This is the most critical component for achieving realism. A discriminator network $D$ is trained to distinguish between "real" samples from the teacher ($x_{\text{teacher}}$) and "fake" samples from the student ($G_{\theta}(z)$). The student, in turn, is trained to fool the discriminator. This forces the student's output distribution to match the teacher's, effectively combating mode collapse and encouraging the generation of sharp, plausible details that might be statistically insignificant but are perceptually vital.
    $$
    \mathcal{L}_{\text{adv}} = \mathbb{E}_{z \sim p_0} \left[ -\log D(G_{\theta}(z)) \right]
    $$

The final objective is a weighted combination:
$$
\mathcal{L}_{\text{total}} = \lambda_{\text{rec}}\mathcal{L}_{\text{rec}} + \lambda_{\text{LPIPS}}\mathcal{L}_{\text{LPIPS}} + \lambda_{\text{adv}}\mathcal{L}_{\text{adv}}
$$

This multi-objective approach ensures the student not only matches the content of the teacher's output but also its stylistic and distributional properties, leading to high-quality one-step generation.
	
---

# <a id="section8">8. References</a>

[^Reflow]: Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022.

[^nvp]: Dinh L, Sohl-Dickstein J, Bengio S. Density estimation using real nvp[J]. arXiv preprint arXiv:1605.08803, 2016.

[^Glow]: Kingma D P, Dhariwal P. Glow: Generative flow with invertible 1x1 convolutions[J]. Advances in neural information processing systems, 2018, 31.

[^Neural_ODE]: Chen R T Q, Rubanova Y, Bettencourt J, et al. Neural ordinary differential equations[J]. Advances in neural information processing systems, 2018, 31.

[^Ffjord]: Grathwohl W, Chen R T Q, Bettencourt J, et al. Ffjord: Free-form continuous dynamics for scalable reversible generative models[J]. arXiv preprint arXiv:1810.01367, 2018.

[^rectified_flow]: Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022.

[^FM]: Lipman Y, Chen R T Q, Ben-Hamu H, et al. Flow matching for generative modeling[J]. arXiv preprint arXiv:2210.02747, 2022.

[^SI]: Albergo M S, Boffi N M, Vanden-Eijnden E. Stochastic interpolants: A unifying framework for flows and diffusions[J]. arXiv preprint arXiv:2303.08797, 2023.

[^SI_1]: Albergo M S, Vanden-Eijnden E. Building normalizing flows with stochastic interpolants[J]. arXiv preprint arXiv:2209.15571, 2022.

[^SI_2]: Albergo M S, Goldstein M, Boffi N M, et al. Stochastic interpolants with data-dependent couplings[J]. arXiv preprint arXiv:2310.03725, 2023.