---
title: 'Diffusion Architectures, Part II: Efficiency-Oriented Designs'
excerpt: "Efficiency is a defining challenge for diffusion models, which often suffer from high computational cost and slow inference. This article surveys architectural strategies that enhance efficiency, from latent-space diffusion and multi-resolution cascades to lightweight convolutional blocks, efficient attention mechanisms, and parameter-efficient modules like LoRA. We also examine distillation and inference-time acceleration techniques that drastically reduce sampling steps. Together, these approaches demonstrate how architectural design can expand the reach of diffusion models — from research labs to real-time and mobile applications."
date: 2025-03-28
permalink: /posts/2025/03/diffusion-architectures-efficiency/
tags:
  - Diffusion Model
  - UNET
  - Transformer
  - DiT
  - Stability
---


<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>📚 Table of Contents</strong></span>
  </summary>
  <ul style="margin:0; padding-left:1.1rem;">
	<li><a href="#section1">1. Introduction</a></li>
	<li><a href="#section2">2. Computational Efficiency (Faster)</a>
		<ul>
			<li><a href="#section2.1">2.1 Latent and Multi-Resolution Strategies</a>
				<ul>
					<li><a href="#section2.1.1">2.1.1 Latent Diffusion</a></li>
					<li><a href="#section2.1.2">2.1.2 Multi-Stage Cascades</a></li>
				</ul>
			</li>
			<li><a href="#section2.2">2.2 Lightweight Building Blocks</a></li>
				<ul>
					<li><a href="#section2.2.1">2.2.1 Efficient Convolutions</a></li>
					<li><a href="#section2.2.2">2.2.2 Efficient Attention</a></li>
				</ul>
			<li><a href="#section2.3">2.3 Token Reduction in Transformers</a></li>
			<li><a href="#section2.4">2.4 Dynamic Computation</a></li>
			<li><a href="#section2.5">2.5 Summary</a></li>
		</ul>
	</li>	
	<li><a href="#section3">3. Model Size Efficiency (Smaller)</a>
      <ul>
        <li><a href="#section3.1">3.1 Parameter-Efficient Fine-Tuning</a></li>
        <li><a href="#section3.2">3.2 Weight Sharing and Recurrent Blocks</a></li>
        <li><a href="#section3.3">3.3 Distillation and Compact Model Design</a></li>
      </ul>
    </li>
    <li><a href="#section4">4. Memory Efficiency (Leaner)</a></li>
    <li><a href="#section5">5. References</a></li>
  </ul>
</details>


In the previous article, we focused on **architectural strategies for stability**, showing how U-Nets and DiTs differ in their design philosophies and failure modes. In this article, we shift our attention to **efficiency** — how diffusion architectures can be redesigned to reduce memory footprint, accelerate training, and enable faster inference. From latent-space modeling (as in Stable Diffusion) to lightweight convolutional blocks, token compression in Transformers, and parameter-efficient modules such as LoRA, we examine a broad spectrum of techniques that make diffusion models more practical and deployable. Efficiency-oriented designs are not merely about saving computation: they fundamentally reshape the accessibility of diffusion models, making them viable on mobile devices, in real-time applications, and at scale across industries.

---

<h1 id="section1" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">1. Introduction</h1>



The last few years have marked a golden age for generative AI, largely powered by the astonishing capabilities of diffusion models. We have witnessed the creation of vivid landscapes from simple phrases, photorealistic portraits indistinguishable from reality, and artistic styles that blend human creativity with algorithmic precision. Models like Stable Diffusion XL, Midjourney, and their contemporaries have redefined the boundaries of digital creation, demonstrating a profound ability to translate abstract concepts into high-fidelity pixels.

Yet, beneath this stunning surface of progress lies a silent, growing bottleneck. Diffusion architectures are notoriously resource-hungry: training often requires large clusters of GPUs for weeks, and inference can involve hundreds of iterative steps with billions of multiply–accumulate operations. These costs limit accessibility, slow down research iteration, and present major barriers to real-world deployment on edge devices or interactive applications.

The critical question for the next era of generative AI is no longer just **Can we generate it?** but **Can we generate it efficiently, accessibly, and sustainably?** This shift moves the spotlight from raw generative capability to **architectural efficiency**. While algorithmic advances in sampling or optimization provide incremental improvements, the most profound and lasting gains come from rethinking the blueprint itself—the design of the U-Net or Transformer backbone. Architecture is not only a determinant of quality; it is the primary lever for controlling computational cost, scalability, and usability.

Building on Part I of this series, where we explored architectural strategies for *stability*, this article turns to the equally pressing question of **efficiency**. We examine how diffusion architectures are being redesigned to be not only powerful, but also **faster, smaller, and leaner**, organized into three complementary dimensions:


1. **Computational Efficiency (Faster):** Reducing FLOPs and latency through latent-space modeling, lightweight convolutions, efficient attention mechanisms, and token reduction strategies.
2. **Model Size Efficiency (Smaller):** Shrinking the parameter footprint via weight sharing, low-rank factorization, and distillation, enabling diffusion models to fit into constrained storage and deployment environments.
3. **Memory Efficiency (Leaner):** Lowering training and inference memory usage with techniques such as gradient checkpointing, quantization, pruning, and KV caching, making larger batch sizes and longer sequences feasible.


This article serves as a comprehensive guide to the efficiency-oriented architectural strategies that are making diffusion models practical at scale. Understanding these strategies is vital for both researchers aiming to push the frontier of generative modeling and practitioners seeking to bring diffusion systems into practical, scalable applications.


---

<h1 id="section2" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">2. Computational Efficiency (Faster)</h1>

Diffusion models are inherently iterative: each sample requires dozens to hundreds of denoising steps, with every step involving a full forward pass through a deep architecture. This makes computational efficiency the most visible bottleneck in practice. Architectural choices—how features are represented, how attention is computed, how convolutional kernels are structured—directly determine the number of floating-point operations (FLOPs) per step and thus the latency of inference. In this section, we review architectural innovations that reduce computation without sacrificing generation quality.

---

<h1 id="section2.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.1 Latent and Multi-Resolution Strategies</h1>

A unifying principle behind efficiency-oriented architectures is to **avoid running the full diffusion process at full pixel resolution**. Since the cost of convolutions and attention grows rapidly with spatial size, denoising directly in pixel space becomes prohibitive at high resolutions. Two dominant strategies address this challenge: (i) **latent diffusion**, which compresses images into a smaller latent space and performs the entire diffusion process there; and (ii) **multi-stage cascades**, which decompose generation into multiple diffusion stages of increasing resolution. Both approaches drastically reduce computation, but they differ in how the process is structured.

---

<h1 id="section2.1.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">2.1.1 Latent Diffusion</h1>

A key leap in efficiency came with **Latent Diffusion Models (LDMs)** (Rombach et al., 2022). Instead of applying the diffusion process directly on pixel space $\mathbf{x} \in \mathbb{R}^{H \times W \times 3}$, which is computationally expensive, an autoencoder first maps images into a compressed latent space:

$$
\mathbf{z} = \mathcal{E}(\mathbf{x}), \quad \mathbf{x} \approx \mathcal{D}(\mathbf{z}),
$$

where $\mathcal{E}$ and $\mathcal{D}$ are encoder–decoder pairs trained with perceptual and adversarial losses. The diffusion process then operates on $\mathbf{z} \in \mathbb{R}^{h \times w \times c}$, with $h = H/f, \; w = W/f$, and $f \in \{4,8\}$ as the downsampling factor.

This reduces computation approximately by

$$
\text{FLOPs}_{\text{latent}} \approx \frac{1}{f^2} \cdot \text{FLOPs}_{\text{pixel}},
$$

while maintaining perceptual quality through a powerful decoder. In practice, Stable Diffusion achieves up to **16–64× savings** compared to pixel-space diffusion.

- **Strength:** The entire denoising process happens in a **single latent space**, greatly reducing FLOPs and memory, while preserving semantics.
- **Weakness:** Performance is tied to the **quality of the autoencoder**; poor reconstruction leads to artifacts.


---

<h1 id="section2.1.2" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">2.1.2 Multi-Stage Cascades</h1>

Another efficiency-oriented design is the use of **multi-stage cascaded models** (Ho et al., 2022; Saharia et al., 2022; Podell et al., 2023). instead of compressing the space, they **split generation into multiple independent diffusion stages**, each operating at progressively higher resolutions. Formally, for a target image $\mathbf{x}_T$:

$$
p(\mathbf{x}_T) \approx p_{\text{base}}(\mathbf{x}_{\text{low}}) \cdot p_{\text{refiner}}(\mathbf{x}_T \mid \mathbf{x}_{\text{low}}),
$$

where the **base model** generates a coarse low-resolution sample (e.g., 64×64), and **refiner models** upsample and add detail at higher resolutions.

During training, each stage is a **standard diffusion model trained at its target resolution**. At inference, all stages are executed in sequence:

- **Base stage:** many denoising steps (e.g., 50–100) at low resolution, where compute is cheap.

- **Refiner stage(s):** fewer steps (e.g., 10–20) at high resolution, where compute is expensive but limited.

The core idea of cascaded model is to break down this generative task into a series of simpler tasks, which are accomplished in succession by multiple independent diffusion models. Each model plays the role of an independent expert. For instance, the basic model focuses on ensuring the global composition of the image but does not care about the details, while the subsequent refined model conducts more refined and detailed processing based on the previous output.



- **Strength:** Efficient **compute allocation**—global structure from the base, fine details from refiners. Modular design allows refiners to be swapped or specialized.

- **Weakness:** Requires **training and storing multiple diffusion models**, increasing system complexity.

---

<h1 id="section2.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.2 Lightweight Building Blocks</h1>

Beyond operating at reduced resolutions, another path to efficiency is to redesign the **fundamental building blocks** of diffusion backbones. Convolution and attention are the two dominant computational modules: U-Net–based architectures rely heavily on convolutions, while Transformer-based DiTs are dominated by self-attention. Both can be optimized for efficiency without severely compromising quality.


---

<h1 id="section2.2.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">2.2.1 Efficient Convolutions</h1>

In U-Net style backbones, most computation comes from repeated $3\times3$ convolutions. A standard convolution of kernel size $K$ with input channels $C_{\text{in}}$ and output channels $C_{\text{out}}$ has cost:

$$
\text{FLOPs}_{\text{conv}} = H \cdot W \cdot K^2 \cdot C_{\text{in}} \cdot C_{\text{out}}.
$$

The number of parameters is:

$$
\text{Param}_{\text{conv}} = K^2 \cdot C_{\text{in}} \cdot C_{\text{out}}
$$

To reduce cost, several lightweight alternatives are widely adopted:

## <span style="color:#3498DB;">A: Parameter & Computation Reduction</span>

The goal is to reduce FLOPs and parameter count while maintaining accuracy.

- 🧠 **A1: Depthwise-Separable Convolution**: depthwise-separable convolution [^Xception] [^mobilenet] decomposes a standard convolution into two lightweight components:  

  **depthwise convolution**: that applies a single spatial filter per input channel. specifically, each input channel is filtered independently with a spatial kernel of size $ K \times K \times 1 $. The output remains $ C_{\text{in}} $ channels. The computational cost (in FLOPs) and the number of parameters are:
  
  $$
  \text{FLOPs}_{\text{dw}} =  H' \cdot W' \cdot K^2 \cdot C_{\text{in}},\qquad \text{Param}_{\text{dw}} = K^2 \cdot C_{\text{in}}
  $$


  **pointwise convolution**: A 1×1 convolution is applied to project the $ C_{\text{in}} $-channel feature map into $ C_{\text{out}} $ channels. The computational cost (in FLOPs) and the number of parameters are:
  
  $$
  \text{FLOPs}_{\text{pw}} =  H' \cdot W' \cdot C_{\text{in}} \cdot C_{\text{out}},\qquad \text{Param}_{\text{pw}} = C_{\text{in}} \cdot C_{\text{out}}
  $$
  
  The total computational cost (in FLOPs) and the total number of parameters are:

  $$
  \begin{align}
  & \text{FLOPs}_{\text{seq}} = \text{FLOPs}_{\text{dw}} + \text{FLOPs}_{\text{pw}} = H' \cdot W' \cdot \left( K^2 \cdot C_{\text{in}} + C_{\text{in}} \cdot C_{\text{out}} \right) \\[10pt]
  & \text{Param}_{\text{seq}} = \text{FLOPs}_{\text{dw}} + \text{FLOPs}_{\text{pw}} = K^2 \cdot C_{\text{in}} + C_{\text{in}} \cdot C_{\text{out}}
  \end{align}
  $$

  The relative computational reduction compared to standard convolution is:

  $$
  \frac{\text{FLOPs}_{\text{seq}}}{\text{FLOPs}_{\text{conv}}} \approx \frac{1}{C_{\text{out}}} + \frac{1}{K^2}
  $$

  For typical values (e.g., $ K=3, C_{\text{out}} \geq 64 $), this yields more than **85%** reduction in FLOPs.


- 🧠 **A2: Group Convolution**: divide the input channel and the output channel into $g$ groups, and convolution is only computed within each groups separately. The total computational cost (in FLOPs) and the total number of parameters are:

  $$
  \begin{align}
  & \text{FLOPs}_{\text{group}} = H' \cdot W' \cdot \left( K^2 \cdot \frac{C_{\text{in}} \cdot C_{\text{out}}}{g} \right)
  & \text{Param}_{\text{group}} =  K^2 \frac{C_{\text{in}} \cdot C_{\text{out}}}{g}
  \end{align}
  $$
  
- 🧠 **A3: Spatially Separable Convolution**:    Decomposes a $K \times K$ convolution into a sequence of a $K \times 1$ convolution followed by a $1 \times K$, particularly for factorizing **larger kernels** like $7 \times 7$ to reduce computational cost.

  $$
  \begin{align}
  & \text{FLOPs}_{\text{group}} = H' \cdot W' \cdot \left( (2K) \cdot \frac{C_{\text{in}} \cdot C_{\text{out}}}{g} \right)
  & \text{Param}_{\text{group}} =  (2K) \frac{C_{\text{in}} \cdot C_{\text{out}}}{g}
  \end{align}
  $$


## <span style="color:#3498DB;">B: Efficient Block Design</span>

The goal is to design modular blocks that improve efficiency beyond basic convolutions.

- 🧠 **B1: ResNet Bottleneck**: the bottleneck block was introduced in ResNet-50/101/152 [^resnet] to reduce the cost of stacking very deep networks. Instead of applying a full $3\times3$ convolution over $C$ channels (which costs $O(C^2k^2)$), the bottleneck design **compresses** the channel dimension first, performs spatial convolution in a reduced space, and then **expands** back. Bottleneck is act as a "wide -> narrow -> wide" channel structure, the workflow of bottleneck is shown as follows.

  ![ResNet Bottleneck](/images/posts/2025-03-28-blog-post/bottleneck.jpg)

  The total computational cost (in FLOPs) and the total number of parameters are:

  $$
  \begin{align}
  & \text{FLOPs}_{\text{bottleneck}} = H \cdot W \cdot \Big(\frac{2}{r}C^2 + \frac{k^2}{r^2}C^2\Big) \\[10pt]
  & \text{Params}_{\text{bottleneck}} = \frac{2}{r}C^2 + \frac{k^2}{r^2}C^2
  \end{align}
  $$
  
  Ratio to standard $k\times k$ convolution:

  $$
  \frac{\text{FLOPs}_{\text{bottleneck}}}{\text{FLOPs}_{\text{std}}}= \frac{2t}{k^2} + \frac{t}{C}
  $$



- 🧠 **B2: Mobile Inverted Bottleneck (MBConv)**: The Mobile Inverted Bottleneck was introduced in MobileNetV2 [^Mobilenetv2] and later extended in EfficientNet [^EfficientNet]. Unlike ResNet’s bottleneck, MBConv first expands the channels, applies an inexpensive depthwise convolution in this higher-dimensional space, then projects back to the original dimension. The final layer is a linear bottleneck (no activation) to avoid information loss. The term inverted bottleneck reflects the fact that the middle is wide rather than narrow.

  ![Mobile Inverted Bottlenec](/images/posts/2025-03-28-blog-post/mbconv.jpg)
  
  The total computational cost (in FLOPs) and the total number of parameters are:
  
  $$
  \begin{align}
  & \text{FLOPs}_{\text{MBConv}} = H \cdot W \cdot (2tC^2 + tCk^2) \\[10pt]
  & \text{Params}_{\text{MBConv}} = 2tC^2 + tCk^2
  \end{align}
  $$

  Ratio to standard $k\times k$ convolution:

  $$
  \frac{\text{FLOPs}_{\text{MBConv}}}{\text{FLOPs}_{\text{std}}}= \frac{2t}{k^2} + \frac{t}{C}
  $$

- 🧠 **B3: ShuffleNet and Channel Shuffle**: ShuffleNet (Zhang et al., CVPR 2018) is a lightweight CNN architecture designed for mobile/edge devices. It combines grouped 1×1 convolutions and depthwise convolutions to greatly reduce FLOPs and parameters. However, unlike pointwise convolution, grouped 1×1 convolutions create a **channel isolation problem**. The reason is that grouped 1×1 convolutions performs convolutions within each group, channels in different groups are isolated, and information cannot flow freely across groups.

  To solve the group isolation problem, ShuffleNet introduces **channel shuffle**, a simple permutation operation that mixes channels across groups with negligible cost. Assume the feature map has \$C\$ channels, divided into \$g\$ groups, each of size \$C/g\$:

  1. **Reshape** the tensor from \$(N, C, H, W)\$ to \$(N, g, C/g, H, W)\$. This makes group membership explicit.

  2. **Transpose** the group and channel dimensions → \$(N, C/g, g, H, W)\$. This rearranges channels so that channels from different groups are interleaved.

  3. **Flatten back** to \$(N, C, H, W)\$. Now, each group in the next convolution layer contains a mixture of channels from all previous groups.


---

<h1 id="section2.2.2" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">2.2.1 Efficient Attention</h1>

For Transformer-based DiTs, the bottleneck lies in the **quadratic cost of self-attention**. Standard self-attention on $N$ tokens of dimension $d$:

$$
\text{FLOPs}_{\text{attn}} = O(N^2 \cdot d).
$$

At high resolution ($N = H \cdot W$), this quickly dominates compute. Several architectural strategies address this:

## <span style="color:#3498DB;">A: Linear Attention</span>

## <span style="color:#3498DB;">B: Local Attention</span>

## <span style="color:#3498DB;">C: Sparse / Low-Rank Attention</span>

## <span style="color:#3498DB;">D: FlashAttention</span>


---

<h1 id="section3" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">3. Training-Time Acceleration of Attention Computation</h1>

Vanilla attention can be expressed as:

$$
\text{Attn}(Q,K,V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d}}\right)V,
$$

which requires quadratic time ($O(n^2\,d)$) and memory complexity ($O(n^2)$) to form and stroe the full similarity matrix $QK^\top \in \mathbb{R}^{n \times n}$, where $n$ is the sequence length and $d$ the feature dimension — makes it infeasible for very long sequences. To alleviate this, researchers have proposed three main families of techniques that accelerate computation during training.

---

## 3.1 Sparse / Local Attention

**Core Idea**: The core assumption is that the most critical information for a given token is often localized or found in a few specific global tokens, rendering a fully dense attention matrix redundant. Instead of computing attention over all $n x n$ pairs, these methods employ predefined sparsity patterns (e.g., sliding windows, dilated/strided windows, global-local attention) to compute only a subset of the attention scores.

**Impact**: This reduces the complexity from $O(n^2\,d)$ to a more manageable $O(nkd)$, where $k$ is a small, constant factor. This significantly reduces the computational burden during both the forward and backward passes.

Below we summarize the most prominent variants of this families.

- **Local Attention (Sliding Window) [^swin]**: Each query attends only to its neighbors within a fixed window of radius $k$:

  $$
  \mathcal{S}_{\text{local}}(i) = \{ j \;\mid\; |i-j| \leq k \}.
  $$

  The time complexity is: $O(nkd)$, and Memory: $O(nk)$.

- **Stride Attention [^sparse]**: Queries attend only to keys that share the same position modulo stride $s$:

  $$
  \mathcal{S}_{\text{stride}}(i) = \{ j \;\mid\; j \equiv i \pmod{s} \}.
  $$

  Each query sees roughly $n/s$ keys, time complexity is reduced to $O\!\left(n\cdot\frac{n}{s}\cdot d\right)$; Memory: $O(n^2/s)$.

- **Block-Sparse Attention [^sparse]**: The sequence is partitioned into blocks of size $b$. Queries in block $t$ attend only to keys in the same or neighboring blocks:

  $$
  \mathcal{S}_{\text{block}}(i \in \mathcal{B}_t) = \bigcup_{u=-w}^{w} \mathcal{B}_{t+u}.
  $$
  
  With neighborhood size $w$, each query attends to $(2w+1)b$ keys, Time: $O\!\left(n (2w+1) b d\right)$, Memory: $O(n (2w+1) b)$.

- **Longformer [^long]**: Longformer augments local attention with a small set of global tokens $\mathcal{G}$. Every query attends to its local window plus these global tokens, while global queries themselves attend to the entire sequence:

  $$
  \mathcal{S}_{\text{LF}}(i) = \mathcal{N}_k(i) \cup \mathcal{G}.
  $$

  Time complexity is equal to $O(n(k+g)d)$.

- **BigBird [^bigbird]**: BigBird extends Longformer by adding $r$ random connections:

  $$
  \mathcal{S}_{\text{BB}}(i) = \mathcal{N}_k(i) \cup \mathcal{G} \cup \mathcal{R}_r(i).
  $$

  Time complexity is equal to Time: $O(n(k+g+r)d)$.


- **Reformer [^reformer]**: Reformer, also known as LSH (Locality-Sensitive Hashing) Attention, replaces the quadratic similarity search with **locality-sensitive hashing (LSH)**. Queries and keys are bucketed via hash codes; attention is restricted to tokens within the same bucket.

  With average bucket size $B$, Time: $O(nBd)$. Memory: $O(nB)$.


- **Routing Transformer [^route]**: The Routing Transformer employs **online k-means clustering** of queries/keys. Tokens within the same cluster attend to each other, producing structured sparsity:

  $$
  \mathcal{S}_{\text{routing}}(i) = \{ j \;\mid\; \text{cluster}(j)=\text{cluster}(i)\}.
  $$

  With cluster size $C$, Time: $O(nCd)$. Memory: $O(nC)$.



We use the following figure to visualize different sparse attentions mechanism. Each heatmap shows the query–key connectivity pattern (**blue = attended**, gray = masked).

![ResNet Bottleneck](/images/posts/2025-03-28-blog-post/sparse_attention.jpg)








---

## 2. Linearized Attention

**Core Idea**: These methods avoid the explicit construction of the  matrix $QK^T \in \mathbb{R}^{n\times n}$ by reordering the computation $\text{softmax}(QK^{T}/\sqrt{d})V$. The central thesis of Linearized Attention is to replace the exponential kernel inherent in the softmax function with a general similarity function, $sim(q, k)$, that is decomposable via a **kernel feature map** $\phi : \mathbb{R}^{d} \to \mathbb{R}^{r}$. Specifically, the similarity is expressed as an inner product in a feature space:

$\text{sim}(q, k) \approx \phi(q) \phi(k)\top$

where 

Substituting this kernel into the attention formula, the output for a query $q_i$ becomes:

$$o_i = \sum_{j=1}^{N} \text{sim}(q_i, k_j) v_j = \sum_{j=1}^{N} (\phi(q_i) \phi(k_j)^T) v_j$$

By leveraging the associative property of matrix multiplication, the query-dependent term $\phi(q_i)$ can be factored out of the summation:

$$o_i = \phi(q_i)^T \left( \sum_{j=1}^{N} \phi(k_j)^T v_j \right)$$

This algebraic manipulation is the cornerstone of all Linearized Attention methods. It fundamentally alters the order of computation from $(Q, K) -> V$ to $(K, V) -> Q$:

- **Old Order:** First, compute the $N \times N$ similarity matrix from $Q$ and $K$, then multiply by $V$. Complexity: $O(N^2 d)$.

- **New Order:** First, compute a "context summary" matrix $\sum_{j=1}^{N} \phi(k_j) v_j^T$ (of size $d_r \times d_v$) from $K$ and $V$. This step has a complexity of $O(N d_r d_v)$. Then, each query $q_i$ attends to this summary. The total complexity becomes linear in $N$ ($O(N d_r d_v)$).

Below, we detail four representative implementations of this principle.

- **Efficient Attention [^efficient]**: 

- **Linear Transformer [^linear]**:

- **Performer [^performer]**:

- **Random Feature Attention (RFA) [^rfa]**:


---

<h1 id="section4" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">4. FlashAttention: Hardware-Aware Exact Attention</h1>


A central challenge of scaling transformers lies not only in the quadratic FLOPs of self-attention but also—more critically—in its **quadratic memory footprint and memory traffic**. In the conventional implementation, the intermediate attention scores $S=QK^\top/\sqrt{d}$ and probability matrix $P=\mathrm{softmax}(S)$ are both **materialized in high-bandwidth memory (HBM)**, leading to $O(N^2)$ space usage and repeated HBM↔SRAM data transfers. On modern GPUs, where compute throughput vastly outpaces memory bandwidth, this input/output (I/O) overhead becomes the true bottleneck: even though FLOPs scale quadratically, the effective utilization of Tensor Cores can remain as low as 10–20%.

**FlashAttention addresses this issue at its root**. It does *not* change the mathematical operator—outputs are bitwise identical to dense softmax attention—but instead reorganizes how computation interacts with hardware. By tiling queries, keys, and values into blocks that fit in on-chip SRAM, and by employing an **online (streaming) softmax algorithm** that incrementally merges block-level statistics, FlashAttention eliminates the need to store $N^2$ intermediate tensors. As a result, memory complexity drops from quadratic to linear in sequence length, while actual wall-clock time shrinks dramatically due to reduced I/O and higher FLOPs utilization. In essence, FlashAttention demonstrates that *exact attention can be made fast* by carefully designing the kernel around hardware constraints, rather than by approximating the operator.


## Notation and Abbreviations

Before delving into the detailed discussion of FlashAttention from V1 to V3, we first provide a consolidated list of the notations and abbreviations used throughout this chapter. This includes the mathematical symbols that appear in the derivations of tiled softmax attention, as well as hardware-related terminology (e.g., HBM, SRAM, Tensor Cores) that is essential for understanding the I/O-aware design principles behind FlashAttention. Having this reference at hand will make the subsequent explanations more accessible and self-contained.
<table class="flash-table">
  <thead>
    <tr>
      <th>Category</th>
      <th>Symbol / Abbreviation</th>
      <th>Definition & Meaning</th>
    </tr>
  </thead>
  <tbody>

    <!-- Core symbols -->
    <tr>
      <td class="category core" rowspan="9">⚡ Core Symbols</td>
      <td class="symbol math">$$N$$</td>
      <td class="definition">Sequence length (number of tokens)</td>
    </tr>
    <tr><td class="symbol math">$$d$$</td><td class="definition">Head dimension</td></tr>
    <tr><td class="symbol math">$$d_v$$</td><td class="definition">Value dimension (often $$d_v = d$$)</td></tr>
    <tr><td class="symbol math">$$Q \in \mathbb{R}^{N \times d}$$</td><td class="definition">Query matrix</td></tr>
    <tr><td class="symbol math">$$K \in \mathbb{R}^{N \times d}$$</td><td class="definition">Key matrix</td></tr>
    <tr><td class="symbol math">$$V \in \mathbb{R}^{N \times d_v}$$</td><td class="definition">Value matrix</td></tr>
    <tr><td class="symbol math">$$S = QK^\top/\sqrt d$$</td><td class="definition">Attention score matrix</td></tr>
    <tr><td class="symbol math">$$P = \mathrm{softmax}(S)$$</td><td class="definition">Attention probability matrix</td></tr>
    <tr><td class="symbol math">$$O = PV$$</td><td class="definition">Attention output</td></tr>

    <!-- Streaming softmax -->
    <tr>
      <td class="category streaming" rowspan="12">🌀 Streaming Softmax</td>
      <td class="symbol math">$$B_r$$</td>
      <td class="definition">Row tile size (blocking size for $$Q$$)</td>
    </tr>
    <tr><td class="symbol math">$$B_c$$</td><td class="definition">Column tile size (blocking size for $$K,V$$)</td></tr>
    <tr><td class="symbol math">$$i$$</td><td class="definition">Row index (query index or row in a row tile)</td></tr>
    <tr><td class="symbol math">$$j$$</td><td class="definition">Column tile index (key/value tile)</td></tr>
    <tr><td class="symbol math">$$T_c = \lceil N / B_c \rceil$$</td><td class="definition">Number of column tiles</td></tr>
    <tr><td class="symbol math">$$\tilde m_i^{(j)}$$</td><td class="definition">Row maximum inside tile $$j$$</td></tr>
    <tr><td class="symbol math">$$\tilde \ell_i^{(j)}$$</td><td class="definition">Row sum of exponentials inside tile $$j$$</td></tr>
    <tr><td class="symbol math">$$\tilde u_i^{(j)}$$</td><td class="definition">Tile-local unnormalized weighted sum of values</td></tr>
    <tr><td class="symbol math">$$m_i^{(j)}$$</td><td class="definition">Global row maximum after tile $$j$$</td></tr>
    <tr><td class="symbol math">$$\ell_i^{(j)}$$</td><td class="definition">Global row exp-sum after tile $$j$$</td></tr>
    <tr><td class="symbol math">$$\tilde O_i^{(j)}$$</td><td class="definition">Global unnormalized weighted sum after tile $$j$$</td></tr>
    <tr><td class="symbol math">$$O_i = \tilde O_i^{(T_c)}/\ell_i^{(T_c)}$$</td><td class="definition">Final normalized row output</td></tr>

    <!-- Backpropagation -->
    <tr>
      <td class="category backprop" rowspan="4">📉 Backpropagation</td>
      <td class="symbol math">$$dO$$</td>
      <td class="definition">Gradient of output $$O$$</td>
    </tr>
    <tr><td class="symbol math">$$dQ, dK, dV$$</td><td class="definition">Gradients of $$Q, K, V$$</td></tr>
    <tr><td class="symbol math">$$dS$$</td><td class="definition">Gradient of score matrix $$S$$</td></tr>
    <tr><td class="symbol">RNG mask</td><td class="definition">Dropout random mask (reconstructed deterministically)</td></tr>

    <!-- Hardware -->
    <tr>
      <td class="category hardware" rowspan="9">🖥️ Hardware</td>
      <td class="symbol"><b>HBM</b></td>
      <td class="definition"><i>High Bandwidth Memory</i> — off-chip GPU memory, large capacity (GBs), high bandwidth but higher latency</td>
    </tr>
    <tr><td class="symbol"><b>SRAM</b></td><td class="definition"><i>Static Random Access Memory</i> — on-chip memory, very fast, very small (KB–MB)</td></tr>
    <tr><td class="symbol"><b>SMEM</b></td><td class="definition"><i>Shared Memory</i> — CUDA’s on-chip shared memory (a form of SRAM)</td></tr>
    <tr><td class="symbol"><b>Registers</b></td><td class="definition">Fastest on-chip storage inside GPU cores, very limited capacity</td></tr>
    <tr><td class="symbol"><b>Occupancy</b></td><td class="definition">Ratio of active threads to maximum supported threads (GPU utilization)</td></tr>
    <tr><td class="symbol"><b>Warp</b></td><td class="definition">Basic execution unit on NVIDIA GPUs (32 threads in lockstep)</td></tr>
    <tr><td class="symbol"><b>Tensor Core</b></td><td class="definition">Specialized GPU unit for high-throughput matrix multiplications (FP16/BF16/FP8)</td></tr>
    <tr><td class="symbol"><b>TMA</b></td><td class="definition"><i>Tensor Memory Accelerator</i> — Hopper feature for efficient HBM↔SMEM transfers</td></tr>
    <tr><td class="symbol"><b>WGMMA</b></td><td class="definition"><i>Warp Group Matrix Multiply-Accumulate</i> — Hopper instruction for warp-group GEMMs</td></tr>

    <!-- Precision -->
    <tr>
      <td class="category precision" rowspan="5">🔢 Precision & Numerical</td>
      <td class="symbol"><b>FP32</b></td>
      <td class="definition">32-bit floating point precision</td>
    </tr>
    <tr><td class="symbol"><b>FP16</b></td><td class="definition">16-bit half precision floating point</td></tr>
    <tr><td class="symbol"><b>BF16</b></td><td class="definition">16-bit bfloat16 (FP32 exponent range, lower mantissa precision)</td></tr>
    <tr><td class="symbol"><b>FP8</b></td><td class="definition">8-bit floating point (Hopper), extreme throughput with higher quantization error</td></tr>
    <tr><td class="symbol">log-sum-exp trick</td><td class="definition">Numerical trick: subtract row max before exponentiation to avoid overflow</td></tr>

    <!-- Complexity -->
    <tr>
      <td class="category complexity" rowspan="3">📊 Complexity</td>
      <td class="symbol">Time complexity</td>
      <td class="definition">FLOPs order; both standard and FlashAttention are $$O(N^2 d)$$</td>
    </tr>
    <tr><td class="symbol">Space complexity</td><td class="definition">Memory usage; standard $$O(N^2)$$, FlashAttention $$O(N d)$$</td></tr>
    <tr><td class="symbol">I/O complexity</td><td class="definition">HBM↔SMEM transfer volume; FlashAttention-V1 is provably I/O-optimal</td></tr>

  </tbody>
</table>

<h1 id="section4.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">4.1 FlashAttention-V1: I/O-Aware Tiling and Online Softmax</h1>


<h1 id="section4.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">4.2 FlashAttention-V2: Reducing Non-GEMM Work and Improving Parallelism</h1>

<h1 id="section4.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">4.3 FlashAttention-V3: Asynchronous Pipelines and FP8 Tensor Cores</h1>

---





---

<h1 id="section9" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">9. References</h1>


[^mobilenet]: Howard A G, Zhu M, Chen B, et al. Mobilenets: Efficient convolutional neural networks for mobile vision applications[J]. arXiv preprint arXiv:1704.04861, 2017.

[^Xception]: Chollet F. Xception: Deep learning with depthwise separable convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 1251-1258.

[^resnet]: He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.

[^Mobilenetv2]: Sandler M, Howard A, Zhu M, et al. Mobilenetv2: Inverted residuals and linear bottlenecks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 4510-4520.

[^EfficientNet]: Tan M, Le Q. Efficientnet: Rethinking model scaling for convolutional neural networks[C]//International conference on machine learning. PMLR, 2019: 6105-6114.

[^swin]: Liu Z, Lin Y, Cao Y, et al. Swin transformer: Hierarchical vision transformer using shifted windows[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2021: 10012-10022.

[^sparse]: Child R, Gray S, Radford A, et al. Generating long sequences with sparse transformers[J]. arXiv preprint arXiv:1904.10509, 2019.

[^long]: Beltagy I, Peters M E, Cohan A. Longformer: The long-document transformer[J]. arXiv preprint arXiv:2004.05150, 2020.

[^bigbird]: Zaheer M, Guruganesh G, Dubey K A, et al. Big bird: Transformers for longer sequences[J]. Advances in neural information processing systems, 2020, 33: 17283-17297.

[^reformer]: Kitaev N, Kaiser Ł, Levskaya A. Reformer: The efficient transformer[J]. arXiv preprint arXiv:2001.04451, 2020.


[^route]: Roy A, Saffar M, Vaswani A, et al. Efficient content-based sparse attention with routing transformers[J]. Transactions of the Association for Computational Linguistics, 2021, 9: 53-68.


[^efficient]: Shen Z, Zhang M, Zhao H, et al. Efficient attention: Attention with linear complexities[C]//Proceedings of the IEEE/CVF winter conference on applications of computer vision. 2021: 3531-3539.

[^linear]: Katharopoulos A, Vyas A, Pappas N, et al. Transformers are rnns: Fast autoregressive transformers with linear attention[C]//International conference on machine learning. PMLR, 2020: 5156-5165.


[^performer]: Choromanski K, Likhosherstov V, Dohan D, et al. Rethinking attention with performers[J]. arXiv preprint arXiv:2009.14794, 2020.

[^rfa]: Peng H, Pappas N, Yogatama D, et al. Random feature attention[J]. arXiv preprint arXiv:2103.02143, 2021.