---
title: 'Diffusion Architectures Part II: Efficiency-Oriented Designs'
excerpt: "Efficiency is a defining challenge for diffusion models, which often suffer from high computational cost and slow inference. This article surveys architectural strategies that enhance efficiency, from latent-space diffusion and multi-resolution cascades to lightweight convolutional blocks, efficient attention mechanisms, and parameter-efficient modules like LoRA. We also examine distillation and inference-time acceleration techniques that drastically reduce sampling steps. Together, these approaches demonstrate how architectural design can expand the reach of diffusion models — from research labs to real-time and mobile applications."
date: 2025-03-28
permalink: /posts/2025/03/diffusion-architectures-efficiency/
tags:
  - Diffusion Model
  - UNET
  - Transformer
  - DiT
  - Stability
---


<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>📚 Table of Contents</strong></span>
  </summary>
  <ul style="margin:0; padding-left:1.1rem;">
	<li><a href="#section1">1. Introduction</a></li>
	<li><a href="#section2">2. Computational Efficiency (Faster)</a>
		<ul>
			<li><a href="#section2.1">2.1 Latent and Multi-Resolution Strategies</a>
				<ul>
					<li><a href="#section2.1.1">2.1.1 Latent Diffusion</a></li>
					<li><a href="#section2.1.2">2.1.2 Multi-Stage Cascades</a></li>
				</ul>
			</li>
			<li><a href="#section2.2">2.2 Lightweight Building Blocks</a></li>
				<ul>
					<li><a href="#section2.2.1">2.2.1 Efficient Convolutions</a></li>
					<li><a href="#section2.2.2">2.2.2 Efficient Attention</a></li>
				</ul>
			<li><a href="#section2.3">2.3 Token Reduction in Transformers</a></li>
			<li><a href="#section2.4">2.4 Dynamic Computation</a></li>
			<li><a href="#section2.5">2.5 Summary</a></li>
		</ul>
	</li>	
	<li><a href="#section3">3. Model Size Efficiency (Smaller)</a>
      <ul>
        <li><a href="#section3.1">3.1 Parameter-Efficient Fine-Tuning</a></li>
        <li><a href="#section3.2">3.2 Weight Sharing and Recurrent Blocks</a></li>
        <li><a href="#section3.3">3.3 Distillation and Compact Model Design</a></li>
      </ul>
    </li>
    <li><a href="#section4">4. Memory Efficiency (Leaner)</a></li>
    <li><a href="#section5">5. References</a></li>
  </ul>
</details>


In the previous article, we focused on **architectural strategies for stability**, showing how U-Nets and DiTs differ in their design philosophies and failure modes. In this article, we shift our attention to **efficiency** — how diffusion architectures can be redesigned to reduce memory footprint, accelerate training, and enable faster inference. From latent-space modeling (as in Stable Diffusion) to lightweight convolutional blocks, token compression in Transformers, and parameter-efficient modules such as LoRA, we examine a broad spectrum of techniques that make diffusion models more practical and deployable. Efficiency-oriented designs are not merely about saving computation: they fundamentally reshape the accessibility of diffusion models, making them viable on mobile devices, in real-time applications, and at scale across industries.

---

<h1 id="section1" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">1. Introduction</h1>



The last few years have marked a golden age for generative AI, largely powered by the astonishing capabilities of diffusion models. We have witnessed the creation of vivid landscapes from simple phrases, photorealistic portraits indistinguishable from reality, and artistic styles that blend human creativity with algorithmic precision. Models like Stable Diffusion XL, Midjourney, and their contemporaries have redefined the boundaries of digital creation, demonstrating a profound ability to translate abstract concepts into high-fidelity pixels.

Yet, beneath this stunning surface of progress lies a silent, growing bottleneck. Diffusion architectures are notoriously resource-hungry: training often requires large clusters of GPUs for weeks, and inference can involve hundreds of iterative steps with billions of multiply–accumulate operations. These costs limit accessibility, slow down research iteration, and present major barriers to real-world deployment on edge devices or interactive applications.

The critical question for the next era of generative AI is no longer just **Can we generate it?** but **Can we generate it efficiently, accessibly, and sustainably?** This shift moves the spotlight from raw generative capability to **architectural efficiency**. While algorithmic advances in sampling or optimization provide incremental improvements, the most profound and lasting gains come from rethinking the blueprint itself—the design of the U-Net or Transformer backbone. Architecture is not only a determinant of quality; it is the primary lever for controlling computational cost, scalability, and usability.

Building on Part I of this series, where we explored architectural strategies for *stability*, this article turns to the equally pressing question of **efficiency**. We examine how diffusion architectures are being redesigned to be not only powerful, but also **faster, smaller, and leaner**, organized into three complementary dimensions:


1. **Computational Efficiency (Faster):** Reducing FLOPs and latency through latent-space modeling, lightweight convolutions, efficient attention mechanisms, and token reduction strategies.
2. **Model Size Efficiency (Smaller):** Shrinking the parameter footprint via weight sharing, low-rank factorization, and distillation, enabling diffusion models to fit into constrained storage and deployment environments.
3. **Memory Efficiency (Leaner):** Lowering training and inference memory usage with techniques such as gradient checkpointing, quantization, pruning, and KV caching, making larger batch sizes and longer sequences feasible.


This article serves as a comprehensive guide to the efficiency-oriented architectural strategies that are making diffusion models practical at scale. Understanding these strategies is vital for both researchers aiming to push the frontier of generative modeling and practitioners seeking to bring diffusion systems into practical, scalable applications.


---

<h1 id="section2" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">2. Computational Efficiency (Faster)</h1>

Diffusion models are inherently iterative: each sample requires dozens to hundreds of denoising steps, with every step involving a full forward pass through a deep architecture. This makes computational efficiency the most visible bottleneck in practice. Architectural choices—how features are represented, how attention is computed, how convolutional kernels are structured—directly determine the number of floating-point operations (FLOPs) per step and thus the latency of inference. In this section, we review architectural innovations that reduce computation without sacrificing generation quality.

---

<h1 id="section2.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.1 Latent and Multi-Resolution Strategies</h1>

A unifying principle behind efficiency-oriented architectures is to **avoid running the full diffusion process at full pixel resolution**. Since the cost of convolutions and attention grows rapidly with spatial size, denoising directly in pixel space becomes prohibitive at high resolutions. Two dominant strategies address this challenge: (i) **latent diffusion**, which compresses images into a smaller latent space and performs the entire diffusion process there; and (ii) **multi-stage cascades**, which decompose generation into multiple diffusion stages of increasing resolution. Both approaches drastically reduce computation, but they differ in how the process is structured.

---

<h1 id="section2.1.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">2.1.1 Latent Diffusion</h1>

A key leap in efficiency came with **Latent Diffusion Models (LDMs)** (Rombach et al., 2022). Instead of applying the diffusion process directly on pixel space $\mathbf{x} \in \mathbb{R}^{H \times W \times 3}$, which is computationally expensive, an autoencoder first maps images into a compressed latent space:

$$
\mathbf{z} = \mathcal{E}(\mathbf{x}), \quad \mathbf{x} \approx \mathcal{D}(\mathbf{z}),
$$

where $\mathcal{E}$ and $\mathcal{D}$ are encoder–decoder pairs trained with perceptual and adversarial losses. The diffusion process then operates on $\mathbf{z} \in \mathbb{R}^{h \times w \times c}$, with $h = H/f, \; w = W/f$, and $f \in \{4,8\}$ as the downsampling factor.

This reduces computation approximately by

$$
\text{FLOPs}_{\text{latent}} \approx \frac{1}{f^2} \cdot \text{FLOPs}_{\text{pixel}},
$$

while maintaining perceptual quality through a powerful decoder. In practice, Stable Diffusion achieves up to **16–64× savings** compared to pixel-space diffusion.

- **Strength:** The entire denoising process happens in a **single latent space**, greatly reducing FLOPs and memory, while preserving semantics.
- **Weakness:** Performance is tied to the **quality of the autoencoder**; poor reconstruction leads to artifacts.


---

<h1 id="section2.1.2" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">2.1.2 Multi-Stage Cascades</h1>

Another efficiency-oriented design is the use of **multi-stage cascaded models** (Ho et al., 2022; Saharia et al., 2022; Podell et al., 2023). instead of compressing the space, they **split generation into multiple independent diffusion stages**, each operating at progressively higher resolutions. Formally, for a target image $\mathbf{x}_T$:

$$
p(\mathbf{x}_T) \approx p_{\text{base}}(\mathbf{x}_{\text{low}}) \cdot p_{\text{refiner}}(\mathbf{x}_T \mid \mathbf{x}_{\text{low}}),
$$

where the **base model** generates a coarse low-resolution sample (e.g., 64×64), and **refiner models** upsample and add detail at higher resolutions.

During training, each stage is a **standard diffusion model trained at its target resolution**. At inference, all stages are executed in sequence:

- **Base stage:** many denoising steps (e.g., 50–100) at low resolution, where compute is cheap.

- **Refiner stage(s):** fewer steps (e.g., 10–20) at high resolution, where compute is expensive but limited.

The core idea of cascaded model is to break down this generative task into a series of simpler tasks, which are accomplished in succession by multiple independent diffusion models. Each model plays the role of an independent expert. For instance, the basic model focuses on ensuring the global composition of the image but does not care about the details, while the subsequent refined model conducts more refined and detailed processing based on the previous output.



- **Strength:** Efficient **compute allocation**—global structure from the base, fine details from refiners. Modular design allows refiners to be swapped or specialized.

- **Weakness:** Requires **training and storing multiple diffusion models**, increasing system complexity.

---

<h1 id="section2.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.2 Lightweight Building Blocks</h1>

Beyond operating at reduced resolutions, another path to efficiency is to redesign the **fundamental building blocks** of diffusion backbones. Convolution and attention are the two dominant computational modules: U-Net–based architectures rely heavily on convolutions, while Transformer-based DiTs are dominated by self-attention. Both can be optimized for efficiency without severely compromising quality.


---

<h1 id="section2.2.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">2.2.1 Efficient Convolutions</h1>

In U-Net style backbones, most computation comes from repeated $3\times3$ convolutions. A standard convolution of kernel size $K$ with input channels $C_{\text{in}}$ and output channels $C_{\text{out}}$ has cost:

$$
\text{FLOPs}_{\text{conv}} = H \cdot W \cdot K^2 \cdot C_{\text{in}} \cdot C_{\text{out}}.
$$

The number of parameters is:

$$
\text{Param}_{\text{conv}} = K^2 \cdot C_{\text{in}} \cdot C_{\text{out}}
$$

To reduce cost, several lightweight alternatives are widely adopted:

## <span style="color:#3498DB;">A: Parameter & Computation Reduction</span>

The goal is to reduce FLOPs and parameter count while maintaining accuracy.

- 🧠 **A1: Depthwise-Separable Convolution**: depthwise-separable convolution [^Xception] [^mobilenet] decomposes a standard convolution into two lightweight components:  

  **depthwise convolution**: that applies a single spatial filter per input channel. specifically, each input channel is filtered independently with a spatial kernel of size $ K \times K \times 1 $. The output remains $ C_{\text{in}} $ channels. The computational cost (in FLOPs) and the number of parameters are:
  
  $$
  \text{FLOPs}_{\text{dw}} =  H' \cdot W' \cdot K^2 \cdot C_{\text{in}},\qquad \text{Param}_{\text{dw}} = K^2 \cdot C_{\text{in}}
  $$


  **pointwise convolution**: A 1×1 convolution is applied to project the $ C_{\text{in}} $-channel feature map into $ C_{\text{out}} $ channels. The computational cost (in FLOPs) and the number of parameters are:
  
  $$
  \text{FLOPs}_{\text{pw}} =  H' \cdot W' \cdot C_{\text{in}} \cdot C_{\text{out}},\qquad \text{Param}_{\text{pw}} = C_{\text{in}} \cdot C_{\text{out}}
  $$
  
  The total computational cost (in FLOPs) and the total number of parameters are:

  $$
  \begin{align}
  & \text{FLOPs}_{\text{seq}} = \text{FLOPs}_{\text{dw}} + \text{FLOPs}_{\text{pw}} = H' \cdot W' \cdot \left( K^2 \cdot C_{\text{in}} + C_{\text{in}} \cdot C_{\text{out}} \right) \\[10pt]
  & \text{Param}_{\text{seq}} = \text{FLOPs}_{\text{dw}} + \text{FLOPs}_{\text{pw}} = K^2 \cdot C_{\text{in}} + C_{\text{in}} \cdot C_{\text{out}}
  \end{align}
  $$

  The relative computational reduction compared to standard convolution is:

  $$
  \frac{\text{FLOPs}_{\text{seq}}}{\text{FLOPs}_{\text{conv}}} \approx \frac{1}{C_{\text{out}}} + \frac{1}{K^2}
  $$

  For typical values (e.g., $ K=3, C_{\text{out}} \geq 64 $), this yields more than **85%** reduction in FLOPs.


- 🧠 **A2: Group Convolution**: divide the input channel and the output channel into $g$ groups, and convolution is only computed within each groups separately. The total computational cost (in FLOPs) and the total number of parameters are:

  $$
  \begin{align}
  & \text{FLOPs}_{\text{group}} = H' \cdot W' \cdot \left( K^2 \cdot \frac{C_{\text{in}} \cdot C_{\text{out}}}{g} \right)
  & \text{Param}_{\text{group}} =  K^2 \frac{C_{\text{in}} \cdot C_{\text{out}}}{g}
  \end{align}
  $$
  
- 🧠 **A3: Spatially Separable Convolution**:    Decomposes a $K \times K$ convolution into a sequence of a $K \times 1$ convolution followed by a $1 \times K$, particularly for factorizing **larger kernels** like $7 \times 7$ to reduce computational cost.

  $$
  \begin{align}
  & \text{FLOPs}_{\text{group}} = H' \cdot W' \cdot \left( (2K) \cdot \frac{C_{\text{in}} \cdot C_{\text{out}}}{g} \right)
  & \text{Param}_{\text{group}} =  (2K) \frac{C_{\text{in}} \cdot C_{\text{out}}}{g}
  \end{align}
  $$


## <span style="color:#3498DB;">B: Efficient Block Design</span>

The goal is to design modular blocks that improve efficiency beyond basic convolutions.

- 🧠 **B1: ResNet Bottleneck**: the bottleneck block was introduced in ResNet-50/101/152 [^resnet] to reduce the cost of stacking very deep networks. Instead of applying a full $3\times3$ convolution over $C$ channels (which costs $O(C^2k^2)$), the bottleneck design **compresses** the channel dimension first, performs spatial convolution in a reduced space, and then **expands** back. Bottleneck is act as a "wide -> narrow -> wide" channel structure, the workflow of bottleneck is shown as follows.

  ![ResNet Bottleneck](/images/posts/2025-03-28-blog-post/bottleneck.jpg)

  The total computational cost (in FLOPs) and the total number of parameters are:

  $$
  \begin{align}
  & \text{FLOPs}_{\text{bottleneck}} = H \cdot W \cdot \Big(\frac{2}{r}C^2 + \frac{k^2}{r^2}C^2\Big) \\[10pt]
  & \text{Params}_{\text{bottleneck}} = \frac{2}{r}C^2 + \frac{k^2}{r^2}C^2
  \end{align}
  $$
  
  Ratio to standard $k\times k$ convolution:

  $$
  \frac{\text{FLOPs}_{\text{bottleneck}}}{\text{FLOPs}_{\text{std}}}= \frac{2t}{k^2} + \frac{t}{C}
  $$



- 🧠 **B2: Mobile Inverted Bottleneck (MBConv)**: The Mobile Inverted Bottleneck was introduced in MobileNetV2 [^Mobilenetv2] and later extended in EfficientNet [^EfficientNet]. Unlike ResNet’s bottleneck, MBConv first expands the channels, applies an inexpensive depthwise convolution in this higher-dimensional space, then projects back to the original dimension. The final layer is a linear bottleneck (no activation) to avoid information loss. The term inverted bottleneck reflects the fact that the middle is wide rather than narrow.

  ![Mobile Inverted Bottlenec](/images/posts/2025-03-28-blog-post/mbconv.jpg)
  
  The total computational cost (in FLOPs) and the total number of parameters are:
  
  $$
  \begin{align}
  & \text{FLOPs}_{\text{MBConv}} = H \cdot W \cdot (2tC^2 + tCk^2) \\[10pt]
  & \text{Params}_{\text{MBConv}} = 2tC^2 + tCk^2
  \end{align}
  $$

  Ratio to standard $k\times k$ convolution:

  $$
  \frac{\text{FLOPs}_{\text{MBConv}}}{\text{FLOPs}_{\text{std}}}= \frac{2t}{k^2} + \frac{t}{C}
  $$

- 🧠 **B3: ShuffleNet and Channel Shuffle**: ShuffleNet (Zhang et al., CVPR 2018) is a lightweight CNN architecture designed for mobile/edge devices. It combines grouped 1×1 convolutions and depthwise convolutions to greatly reduce FLOPs and parameters. However, unlike pointwise convolution, grouped 1×1 convolutions create a **channel isolation problem**. The reason is that grouped 1×1 convolutions performs convolutions within each group, channels in different groups are isolated, and information cannot flow freely across groups.

  To solve the group isolation problem, ShuffleNet introduces **channel shuffle**, a simple permutation operation that mixes channels across groups with negligible cost. Assume the feature map has \$C\$ channels, divided into \$g\$ groups, each of size \$C/g\$:

  1. **Reshape** the tensor from \$(N, C, H, W)\$ to \$(N, g, C/g, H, W)\$. This makes group membership explicit.

  2. **Transpose** the group and channel dimensions → \$(N, C/g, g, H, W)\$. This rearranges channels so that channels from different groups are interleaved.

  3. **Flatten back** to \$(N, C, H, W)\$. Now, each group in the next convolution layer contains a mixture of channels from all previous groups.


---

<h1 id="section2.2.2" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">2.2.1 Efficient Attention</h1>

For Transformer-based DiTs, the bottleneck lies in the **quadratic cost of self-attention**. Standard self-attention on $N$ tokens of dimension $d$:

$$
\text{FLOPs}_{\text{attn}} = O(N^2 \cdot d).
$$

At high resolution ($N = H \cdot W$), this quickly dominates compute. Several architectural strategies address this:

## <span style="color:#3498DB;">A: Linear Attention</span>

## <span style="color:#3498DB;">B: Local Attention</span>

## <span style="color:#3498DB;">C: Sparse / Low-Rank Attention</span>

## <span style="color:#3498DB;">D: FlashAttention</span>


---

<h1 id="section3" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">3. Acceleration of Attention Computation</h1>

Vanilla attention can be expressed as:

$$
\text{Attn}(Q,K,V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d}}\right)V,
$$

which requires quadratic time ($O(N^2\,d)$) and memory complexity ($O(N^2)$) to form and stroe the full similarity matrix $QK^\top \in \mathbb{R}^{N \times N}$, where $N$ is the sequence length and $d$ the feature dimension — makes it infeasible for very long sequences. To alleviate this, researchers have proposed three main families of techniques that accelerate computation.


- **Approximate Attention Computation**: The conventional time complexity is O(N^2d). The first type of method reduces time complexity through approximate rather than exact attention computation.

- **Hard-aware Acceleration**: The second type of method maintains the time complexity of exact computation but achieves acceleration by fully leveraging GPU hardware characteristics to reduce data I/O transfer.


---

<h1 id="section3.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.1 Notation and Abbreviations</h1>


Before delving into the detailed algorithms, we first provide a consolidated list of the notations and abbreviations used throughout this chapter. 



<table class="flash-table">
  <thead>
    <tr>
      <th>Category</th>
      <th>Symbol / Abbreviation</th>
      <th>Definition & Meaning</th>
    </tr>
  </thead>
  <tbody>

    <!-- Core symbols -->
    <tr>
      <td class="category core" rowspan="11">⚡ Core Symbols</td>
      <td class="symbol math">$N$</td>
      <td class="definition">Sequence length (number of tokens)</td>
    </tr>
    <tr><td class="symbol math">$d$</td><td class="definition">Head dimension</td></tr>
    <tr><td class="symbol math">$d_v$</td><td class="definition">Value dimension (often $d_v = d$)</td></tr>
    <tr><td class="symbol math">$Q \in \mathbb{R}^{N \times d}$</td><td class="definition">Query matrix</td></tr>
    <tr><td class="symbol math">$K \in \mathbb{R}^{N \times d}$</td><td class="definition">Key matrix</td></tr>
    <tr><td class="symbol math">$V \in \mathbb{R}^{N \times d_v}$</td><td class="definition">Value matrix</td></tr>
    <tr><td class="symbol math">$S = QK^\top/\sqrt d$</td><td class="definition">Attention score matrix</td></tr>
    <tr><td class="symbol math">$A = \mathrm{softmax}(S)$</td><td class="definition">Attention probability matrix</td></tr>
    <tr><td class="symbol math">$O = PV$</td><td class="definition">Attention output</td></tr>
	<tr><td class="symbol math">$\odot$</td><td class="definition">Element-wise product</td></tr>
	<tr><td class="symbol math">$\cdot$</td><td class="definition">Dot product</td></tr>
  </tbody>
</table>





---

<h1 id="section3.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.2 Sparse & Local Attention</h1>


**Core Idea**: The core assumption is that the most critical information for a given token is often localized or found in a few specific global tokens, rendering a fully dense attention matrix redundant. Instead of computing attention over all $n x n$ pairs, these methods employ predefined sparsity patterns (e.g., sliding windows, dilated/strided windows, global-local attention) to compute only a subset of the attention scores.

**Impact**: This reduces the complexity from $O(n^2\,d)$ to a more manageable $O(nkd)$, where $k$ is a small, constant factor. This significantly reduces the computational burden during both the forward and backward passes.

Below we summarize the most prominent variants of this families.

- **Local Attention (Sliding Window) [^swin]**: Each query attends only to its neighbors within a fixed window of radius $k$:

  $$
  \mathcal{S}_{\text{local}}(i) = \{ j \;\mid\; |i-j| \leq k \}.
  $$

  The time complexity is: $O(nkd)$, and Memory: $O(nk)$.

- **Stride Attention [^sparse]**: Queries attend only to keys that share the same position modulo stride $s$:

  $$
  \mathcal{S}_{\text{stride}}(i) = \{ j \;\mid\; j \equiv i \pmod{s} \}.
  $$

  Each query sees roughly $n/s$ keys, time complexity is reduced to $O\!\left(n\cdot\frac{n}{s}\cdot d\right)$; Memory: $O(n^2/s)$.

- **Block-Sparse Attention [^sparse]**: The sequence is partitioned into blocks of size $b$. Queries in block $t$ attend only to keys in the same or neighboring blocks:

  $$
  \mathcal{S}_{\text{block}}(i \in \mathcal{B}_t) = \bigcup_{u=-w}^{w} \mathcal{B}_{t+u}.
  $$
  
  With neighborhood size $w$, each query attends to $(2w+1)b$ keys, Time: $O\!\left(n (2w+1) b d\right)$, Memory: $O(n (2w+1) b)$.

- **Longformer [^long]**: Longformer augments local attention with a small set of global tokens $\mathcal{G}$. Every query attends to its local window plus these global tokens, while global queries themselves attend to the entire sequence:

  $$
  \mathcal{S}_{\text{LF}}(i) = \mathcal{N}_k(i) \cup \mathcal{G}.
  $$

  Time complexity is equal to $O(n(k+g)d)$.

- **BigBird [^bigbird]**: BigBird extends Longformer by adding $r$ random connections:

  $$
  \mathcal{S}_{\text{BB}}(i) = \mathcal{N}_k(i) \cup \mathcal{G} \cup \mathcal{R}_r(i).
  $$

  Time complexity is equal to Time: $O(n(k+g+r)d)$.


- **Reformer [^reformer]**: Reformer, also known as LSH (Locality-Sensitive Hashing) Attention, replaces the quadratic similarity search with **locality-sensitive hashing (LSH)**. Queries and keys are bucketed via hash codes; attention is restricted to tokens within the same bucket.

  With average bucket size $B$, Time: $O(nBd)$. Memory: $O(nB)$.


- **Routing Transformer [^route]**: The Routing Transformer employs **online k-means clustering** of queries/keys. Tokens within the same cluster attend to each other, producing structured sparsity:

  $$
  \mathcal{S}_{\text{routing}}(i) = \{ j \;\mid\; \text{cluster}(j)=\text{cluster}(i)\}.
  $$

  With cluster size $C$, Time: $O(nCd)$. Memory: $O(nC)$.



We use the following figure to visualize different sparse attentions mechanism. Each heatmap shows the query–key connectivity pattern (**blue = attended**, gray = masked).

![ResNet Bottleneck](/images/posts/2025-03-28-blog-post/sparse_attention.jpg)



---

<h1 id="section3.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.3 Linearized Attention</h1>


**Core Idea**: These methods avoid the explicit construction of the  matrix $QK^T \in \mathbb{R}^{n\times n}$ by reordering the computation $\text{softmax}(QK^{T}/\sqrt{d})V$. The central thesis of Linearized Attention is to replace the exponential kernel inherent in the softmax function with a general similarity function, $sim(q, k)$, that is decomposable via a **kernel feature map** $\phi : \mathbb{R}^{d} \to \mathbb{R}^{r}$. Specifically, the similarity is expressed as an inner product in a feature space:

$\text{sim}(q, k) \approx \phi(q) \phi(k)\top$

where 

Substituting this kernel into the attention formula, the output for a query $q_i$ becomes:

$$o_i = \sum_{j=1}^{N} \text{sim}(q_i, k_j) v_j = \sum_{j=1}^{N} (\phi(q_i) \phi(k_j)^T) v_j$$

By leveraging the associative property of matrix multiplication, the query-dependent term $\phi(q_i)$ can be factored out of the summation:

$$o_i = \phi(q_i)^T \left( \sum_{j=1}^{N} \phi(k_j)^T v_j \right)$$

This algebraic manipulation is the cornerstone of all Linearized Attention methods. It fundamentally alters the order of computation from $(Q, K) -> V$ to $(K, V) -> Q$:

- **Old Order:** First, compute the $N \times N$ similarity matrix from $Q$ and $K$, then multiply by $V$. Complexity: $O(N^2 d)$.

- **New Order:** First, compute a "context summary" matrix $\sum_{j=1}^{N} \phi(k_j) v_j^T$ (of size $d_r \times d_v$) from $K$ and $V$. This step has a complexity of $O(N d_r d_v)$. Then, each query $q_i$ attends to this summary. The total complexity becomes linear in $N$ ($O(N d_r d_v)$).

Below, we detail four representative implementations of this principle.

- **Efficient Attention [^efficient]**: 

- **Linear Transformer [^linear]**:

- **Performer [^performer]**:

- **Random Feature Attention (RFA) [^rfa]**:


---

<h1 id="section4" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">4. FlashAttention: Hardware-Aware Exact Attention</h1>


A central challenge of scaling transformers lies not only in the quadratic FLOPs of self-attention but also—more critically—in its **quadratic memory footprint and memory traffic**. In the conventional implementation, the intermediate attention scores $S=QK^\top/\sqrt{d}$ and probability matrix $P=\mathrm{softmax}(S)$ are both **materialized in high-bandwidth memory (HBM)**, leading to $O(N^2)$ space usage and repeated HBM↔SRAM data transfers. On modern GPUs, where compute throughput vastly outpaces memory bandwidth, this input/output (I/O) overhead becomes the true bottleneck: even though FLOPs scale quadratically, the effective utilization of Tensor Cores can remain as low as 10–20%.

**FlashAttention addresses this issue at its root**. It does *not* change the mathematical operator—outputs are bitwise identical to dense softmax attention—but instead reorganizes how computation interacts with hardware. By tiling queries, keys, and values into blocks that fit in on-chip SRAM, and by employing an **online (streaming) softmax algorithm** that incrementally merges block-level statistics, FlashAttention eliminates the need to store $N^2$ intermediate tensors. As a result, memory complexity drops from quadratic to linear in sequence length, while actual wall-clock time shrinks dramatically due to reduced I/O and higher FLOPs utilization. In essence, FlashAttention demonstrates that *exact attention can be made fast* by carefully designing the kernel around hardware constraints, rather than by approximating the operator.

In addition to the core symbols, the following symbols will also be used in this chapter



<table class="flash-table">
  <thead>
    <tr>
      <th>Category</th>
      <th>Symbol / Abbreviation</th>
      <th>Definition & Meaning</th>
    </tr>
  </thead>
  <tbody>

    <!-- Streaming softmax -->
    <tr>
      <td class="category streaming" rowspan="12">🌀 Streaming Softmax</td>
      <td class="symbol math">$B_r$</td>
      <td class="definition">Row tile size (blocking size for $Q$)</td>
    </tr>
    <tr><td class="symbol math">$B_c$</td><td class="definition">Column tile size (blocking size for $K,V$)</td></tr>
    <tr><td class="symbol math">$i$</td><td class="definition">Row index (query index or row in a row tile)</td></tr>
    <tr><td class="symbol math">$j$</td><td class="definition">Column tile index (key/value tile)</td></tr>
    <tr><td class="symbol math">$T_c = \lceil N / B_c \rceil$</td><td class="definition">Number of column tiles</td></tr>
    <tr><td class="symbol math">$\tilde m_i^{(j)}$</td><td class="definition">Row maximum inside tile $j$</td></tr>
    <tr><td class="symbol math">$\tilde \ell_i^{(j)}$</td><td class="definition">Row sum of exponentials inside tile $j$</td></tr>
    <tr><td class="symbol math">$\tilde u_i^{(j)}$</td><td class="definition">Tile-local unnormalized weighted sum of values</td></tr>
    <tr><td class="symbol math">$m_i^{(j)}$</td><td class="definition">Global row maximum after tile $j$</td></tr>
    <tr><td class="symbol math">$\ell_i^{(j)}$</td><td class="definition">Global row exp-sum after tile $j$</td></tr>
    <tr><td class="symbol math">$\tilde O_i^{(j)}$</td><td class="definition">Global unnormalized weighted sum after tile $j$</td></tr>
    <tr><td class="symbol math">$O_i = \tilde O_i^{(T_c)}/\ell_i^{(T_c)}$</td><td class="definition">Final normalized row output</td></tr>


  </tbody>
</table>





<h1 id="section4.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">4.1 FlashAttention-V1: I/O-Aware Tiling and Online Softmax</h1>

FlashAttention v1 [^flashv1] is a landmark algorithm designed to accelerate the attention mechanism in Transformers. Its innovation lies **not** in changing the mathematical output of attention but in fundamentally restructuring its computation to be "I/O-aware," thereby overcoming the primary performance bottleneck on modern GPUs.


<h1 id="section4.1.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">4.1.1  Why Standard Attention is Inefficient on GPUs</h1>

To understand FlashAttention v1, one must first understand the GPU memory hierarchy and its performance characteristics. GPUs have two main types of memory:

1.  **HBM (High-Bandwidth Memory)**: This is the large, off-chip GPU memory (e.g., 24GB, 80GB). It has high capacity and bandwidth but is relatively **slow** to access.

2.  **SRAM (Static RAM)**: This is the small, on-chip cache located next to the GPU's computing cores. It is extremely **fast**—orders of magnitude faster than HBM—but has a very limited capacity (a few megabytes).

The standard attention computation, $O = \text{softmax}((QK^T))V$ (we ignore the normalization factor $\sqrt{d}$ for convenience), involves several distinct steps that lead to inefficient memory access patterns:

- **Step 1:  Compute and materialize $S = QK^T$**. GPUs compute $S$ tile by tile: a small block of $Q$ ($Q_i$) and a small block of $K$  ($K_j$) are loaded into on-chip SRAM (registers/shared memory), a micro-GEMM produces one tile $S_{i,j}=Q_iK^T_j$, and that tile is immediately written to HBM, Repeat until every $S_{i,j}$ has been produced and stored, so that the full matrix $S$ now exists in HBM.

  ![tradition_step_1](/images/posts/2025-03-28-blog-post/tradition_step_1.jpg)

- **Step 2:  Softmax over $S\,(rowwise)**. To compute a numerically stable softmax, the kernel must scan each row to get the rowwise max, then scan again to accumulate. Read tiles of $S$ ($S_i$) from HBM, 

  $$
  A_{i, j} = \text{softmax}(S_{i, j}) = \frac{e^{s_{i, j}}}{\sum_{k=1}^{d}{e^{i, k}}} = \frac{e^{s_{i, j}-m_i}}{\sum_{k=1}^{d}{e^{s_{i, k}- m_i}}}
  $$

  where $m_i = \max {s_{i,j} (j=1,\dots,N})$. The resulting probabilities are written back to HBM (again tile by tile).

  ![tradition_step_2](/images/posts/2025-03-28-blog-post/tradition_step_2.jpg)

- **Step 3:  Multiply $A$ with $V$ to get output $O$**. pull a tile of $A$ ($A_i$) from HBM and a tile of $V$ ($V_j$) from HBM onto the SM, multiply and write it back to HBM. After all tiles are done, accumulate and the full $O$ lives in HBM.

  ![tradition_step_3](/images/posts/2025-03-28-blog-post/tradition_step_3.jpg)

**The root problem**: The huge $N \times N$ intermediate matrices ($A$ and $S$) are repeatedly read from and written to the slow HBM. When the sequence length $N$ is large (e.g., 8K), this matrix becomes enormous ($8K \times 8K$), and the time spent on memory I/O dwarfs the time spent on actual computation (FLOPs). the dominant cost is **I/O, not FLOPs**. The kernel sequence is therefore **memory-bandwidth bound** and forces peak memory to scale as $O(N^2)$.

---

<h1 id="section4.1.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">4.1.2  The Core Idea of FlashAttention-V1</h1>


FlashAttention-V1 rethinks attention as an **I/O-aware, fused kernel**:

- **Tile the computation** so that only small blocks of $Q,K,V$ live in on-chip memory at any time.

- **Compute and consume scores immediately**, via an **online (streaming) softmax**, so that neither $S$ nor $A$ are ever materialized (saved) in HBM.

- **Fuse the entire forward pass into a single kernel**: $QKᵀ$ -> $\text{softmax}$ -> multiply by $V$ -> accumulate into $O$.  This ensures that all intermediate results are kept in fast SRAM without being written back to HBM.

- In backward, **recompute scores tile-by-tile** rather than storing them, while keeping only lightweight per-row statistics.

This preserves the exact semantics of dense attention but achieves **I/O-optimality**: the minimal possible data movement between HBM and on-chip SRAM, given realistic hardware constraints.


---

<h1 id="section4.1.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">4.1.3  Forward Pass: Tiled, Online Softmax</h1>

Let's focus on computing a single block-row of the output, $O_i$. We partition $K$ and $V$ into $B_c$ column-blocks and $Q$ into $B_r$ row-blocks. The algorithm uses nested loops: an outer loop over blocks of $K$ and $V$ and an inner loop over blocks of $Q$.

## <span style="color:#3498DB;">Step 1: Partition Q, K, V into tiles</span>

1. Split $Q$ with size $N \times d$ into **row tiles** of size $B_r\times d$. 
2. Split $K$ and $V$ into **column tiles** of size $B_c\times d$ and $B_c\times d_v$.
3. Assign each row tile of $Q$ to one SM (Streaming Multiprocessor). For example, $\text{SM}_i$ is responsible for all computations related to $Q_i$, this block will remain resident in on-chip memory for the duration of its computation.


## <span style="color:#3498DB;">Step 2: Stream K & V tiles for computation</span>

For each column tile $j$, Load $K_j$ and $V_j$ from HBM into SMEM/registers, compute local score block $S_{ij}=Q_iK_j^\top$. Within the SM, compute tile-local statistics:

  $$
  \tilde m_i^{(j)}=\max_{k\in j} S_{ik},\quad
  \tilde \ell_i^{(j)}=\sum_{k\in j} e^{S_{ik}-\tilde m_i^{(j)}},\quad
  \tilde u_i^{(j)}=\sum_{k\in j} e^{S_{ik}-\tilde m_i^{(j)}} V_k
  $$

Update global online softmax statistics (kept per row in registers/SMEM), discard $S_{ij}$ immediately after use—never stored to HBM.

  $$
  \begin{aligned}
  m_i^{(j)} &= \max(m_i^{(j-1)}, \tilde m_i^{(j)}), \\[10pt]
  \ell_i^{(j)} &= e^{m_i^{(j-1)}-m_i^{(j)}}\ell_i^{(j-1)} + e^{\tilde m_i^{(j)}-m_i^{(j)}}\tilde \ell_i^{(j)}
  \end{aligned}
  $$

After obtaining all the required intermediate results, update the output $O_i$:


$$
\begin{align}
O_i^j & = \frac{\sum_{k \in {1,\dots,j-1}} e^{s_{ik}-m_i^{(j)}} V_k + \sum_{k \in {j}} e^{s_{ik}-m_i^{(j)}} V_k}{l_i^j} \\[10pt]
& =  \frac{e^{m_i^{j-1} - m_i^j}\,l_i^{j-1}}{l_i^j}\,O_i^{j-1} + \frac{e^{\tilde m_i^{(j)} - m_i^j}}{l_i^j}\,\tilde u_i^{(j)}
\end{align}
$$

After all $K/V$ tiles are streamed, Write $O_i$ back to HBM. Only the per-row statistics $(m_i,\ell_i)$ are saved for backward.

This technique is a streaming/parallel version of the **log-sum-exp** trick. online softmax is an exact computation, identical to dense attention. However, only $Q,K,V,O$ move to/from HBM, $S$ and $A$ are never saved, the peak memory is $O(Nd)$ instead of $O(N^2)$.


---

<h1 id="section4.1.4" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">4.1.4  Backward Pass: Recomputation Instead of Storage</h1>

Computing in blocks and avoiding explicit storage of the large matrix **$A$ (or $S$)** significantly improves the I/O efficiency of the forward pass, enabling faster computation than the standard approach while maintaining numerical accuracy. However, this also introduces new challenges for backward gradient computation, the backward pass cannot simply reuse $A$, it must reconstruct the relevant probabilities on-the-fly within each tile. This design choice makes the backward algorithm structurally different from the standard one, even though the underlying gradient formulas remain mathematically identical.


<table class="flash-table">
  <thead>
    <tr>
      <th>Category</th>
      <th>Symbol / Abbreviation</th>
      <th>Definition & Meaning</th>
    </tr>
  </thead>
  <tbody>

    <tr>
      <td class="category backprop" rowspan="4">📉 Backpropagation</td>
      <td class="symbol math">$dO$</td>
      <td class="definition">Gradient of output $O$</td>
    </tr>
    <tr><td class="symbol math">$dQ, dK, dV$</td><td class="definition">Gradients of $Q, K, V$</td></tr>
    <tr><td class="symbol math">$dS$</td><td class="definition">Gradient of score matrix $S$</td></tr>
    <tr><td class="symbol">RNG mask</td><td class="definition">Dropout random mask (reconstructed deterministically)</td></tr>


  </tbody>
</table>


## <span style="color:#3498DB;">Traditional Gradient Calculation: full backward chain from $d_O$</span>


Let's brief recap backward propagation of traditional attention. Given the forward attention pass:

$$
O = \mathrm{softmax}(\frac{QK^\top}{\sqrt{d}}) V
$$

The objective is to compute the gradients of the loss function $\mathcal{L}$ with respect to all intermediate and parameter tensors, starting from the upstream gradient $dO = \partial \mathcal{L}/\partial O$. Gradient flow topology for traditional attention is as follows.

![Gradient Flow Topology for Traditional Attention](/images/posts/2025-03-28-blog-post/bp.jpg)

- **Step 1: Gradients $d_A$ and $d_V$ through the output linear combination.**  Since $O = A V$, the differentials are governed by standard matrix multiplication rules. The gradients with respect to $V$ and $A$ are

  $$
  d_V = A^\top \cdot d_O,\qquad d_A = d_O \cdot V^\top
  $$



- **Step 2: Gradients $d_S$ through the softmax function.**  This is the most intricate step due to the nature of the softmax derivative. Let $ S \in \mathbb{R}^{N \times N} $ be a matrix of input logits. The softmax function is applied **row-wise** to $S$ to produce the output matrix $A \in \mathbb{R}^{N \times N}$, where each element is given by:

  $$
  A_{ij} = \frac{\exp(S_{ij})}{\sum_{k=1}^{N} \exp(S_{ik})}.
  $$

  Given the upstream gradient $d_A = {\partial L}/{\partial A}$, our goal is to compute the gradient of the loss $ L $ with respect to the input logits $ S $, denoted as $ d_S = {\partial L}/{\partial S} $.

  Due to the row-wise application of the softmax, the output $ A_{ij} $ depends only on the elements of the $ i $-th row of $ S $. Therefore, by the chain rule, the gradient for each element $ S_{ij} $ is:

  $$
  d_{S_{ij}} = \frac{\partial L}{\partial S_{ij}} = \sum_{m=1}^{N} \sum_{n=1}^{N} \frac{\partial L}{\partial A_{mn}} \cdot \frac{\partial A_{mn}}{\partial S_{ij}}.
  $$


  For each row $a = \mathrm{softmax}(s)$, the Jacobian is

  $$
  \frac{\partial a}{\partial s} = \mathrm{diag}(a) - a a^\top.
  $$

  Consequently, for an upstream gradient $g = d_A$, the gradient with respect to the scores is

  $$
  d_S = \bigl(g - (A^\top g)\mathbf{1}\bigr) \odot A.
  $$

  Vectorizing over all rows yields

  $$
  d_S = \bigl(d_A - \operatorname{rowsum}(d_A \odot A)\bigr) \odot A,
  $$

  where $\operatorname{rowsum}(X)$ denotes the vector of row-wise sums, broadcast across columns.

- **Step 3: Gradients $d_Q$ and $d_K$  through the scaled dot-product scores.** The attention scores are defined as $S = \tfrac{1}{\sqrt{d}} Q \cdot K^\top$. Differentiating with respect to $Q$ and $K$ gives

  $$
  d_Q = \frac{1}{\sqrt{d}} d_S \cdot K, \qquad
  d_K = \frac{1}{\sqrt{d}} d_S^\top \cdot Q.
  $$

  These expressions correspond to propagating the gradient through the bilinear form $\langle Q_i, K_j\rangle$.

- **Step 4: Gradients through the linear projections.** The projections are linear transformations. For $Q = X W_Q$, we obtain

  $$
  d_{W_Q} = X^\top d_Q, \qquad d_{X^{(Q)}} = d_Q W_Q^\top.
  $$

  Similarly, for the key and value projections,

  $$
  \begin{align}
  d_{W_K} = X^\top d_K, \qquad d_{X^{(K)}} = d_K W_K^\top \\[10pt]
  d_{W_V} = X^\top d_V, \qquad d_{X^{(V)}} = d_V W_V^\top
  \end{align}
  $$


  The total gradient with respect to the input $X$ is the sum of contributions from all three branches:

  $$
  d_X = d_{X^{(Q)}} + d_{X^{(K)}} + d_{X^{(V)}}.
  $$


## <span style="color:#3498DB;">Key challenges</span>

From the above analysis, it can be seen that traditional attention backpropagation requires the complete matrix **$A$**. However, in the forward implementation of FlashAttention v1, we do not save this matrix for memory efficiency. Therefore, during the backward computation in FlashAttention v1, modifications are necessary. The core principle is **recomputation**.

FlashAttention-V1 avoids materializing $A$ and $S$ in forward. Instead, it saves **per-row softmax statistics** sufficient to reconstruct probabilities later. Besides $Q$, $K$, $V$, $O$, the forward process also saved two statistical vectors.

- For each row $i$, forward stores

  $$
  m_i = \max_j S_{ij},\qquad \ell_i = \sum_j e^{S_{ij}-m_i}.
  $$
- These are $O(N)$ scalars (often FP32). They suffice to reconstruct matrix $A$ whenever $S_{ij}$ is recomputed.

  $$
  A_{ij} = \frac{e^{S_{ij}-m_i}}{\ell_i}
  $$



## <span style="color:#3498DB;">Backward Computation in FlashAttention v1</span>



  
Backward is organized **tile-by-tile** over $Q$-row tiles ($B_r$) and $K/V$-column tiles ($B_c$). To achieve exact softmax gradients without storing **$A$ , $d_A$ , $S$ and $d_S$**, V1 uses a **two-pass algorithm** over the $K/V$ tiles for each $Q$-row tile.


- **Pass A — accumulate row scalars and $dV$.**

Reviewing our previous discussion, the gradients need to be solved using matrix A are $d_V$ and $d_S$. Therefore, the goal for pass A is:

For each row $i$ (token position), compute the scalar

$$
\alpha_i \;\;=\;\; \sum_{k=1}^{N} d_{A_{ik}}\, A_{ik},
$$

which is required by the rowwise softmax backward identity

$$
d_{S_{ij}} \;=\; \bigl(d_{A_{ij}} - \alpha_i\bigr)\, A_{ij}.
$$

In addition, accumulate the value gradient

$$
d_V \;=\; A^\top dO \;=\; \sum_{i=1}^{N} A_{i:}^\top dO_{i:},
$$

again **without** ever materializing $A$ (or $S$) as $N\times N$ tensors. Also, we **should not** save $d_A$ (or $d_S$), because they also require $N \times N$ storage space, which would violate the goal of FlashAttention v1.


- **Recompute $S_{ij}$, reconstruct $A_{ij}$** as forward process described above.

- **Form local $d_A$ without storing a global $d_A$:**

   $$
   d_A^{\text{(tmp)}}_{ij} \;=\; d_{O_i}\,V_j^\top,
   $$

   and if dropout was applied in forward, multiply elementwise by $\mathcal M_{ij}/p$.
   
3. **Accumulate the row scalar and $dV$:**

   $$
   \boxed{\ \alpha_i \mathrel{+}= \operatorname{rowsum}\bigl(dA^{\text{(tmp)}}_{ij}\odot A_{ij}\bigr)\ },\qquad
   \boxed{\ dV_j \mathrel{+}= A_{ij}^\top dO_i\ }.
   $$

No $N\times N$ tensor is stored; only per-row $\alpha_i$ and per-tile $dV_j$ are updated.

**Rationale.** In traditional softmax backward,

$$
dS_{ij} = \bigl(dA_{ij} - \alpha_i\bigr)\odot A_{ij},\qquad
\alpha_i=\sum_k dA_{ik}A_{ik}.
$$


- **Pass B — form $d_S$, then obtain $d_Q$, $d_K$.**


---


<h1 id="section4.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">4.2 FlashAttention-V2: Reducing Non-GEMM Work and Improving Parallelism</h1>

<h1 id="section4.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">4.3 FlashAttention-V3: Asynchronous Pipelines and FP8 Tensor Cores</h1>

---





---

<h1 id="section9" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">9. References</h1>


[^mobilenet]: Howard A G, Zhu M, Chen B, et al. Mobilenets: Efficient convolutional neural networks for mobile vision applications[J]. arXiv preprint arXiv:1704.04861, 2017.

[^Xception]: Chollet F. Xception: Deep learning with depthwise separable convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 1251-1258.

[^resnet]: He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.

[^Mobilenetv2]: Sandler M, Howard A, Zhu M, et al. Mobilenetv2: Inverted residuals and linear bottlenecks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 4510-4520.

[^EfficientNet]: Tan M, Le Q. Efficientnet: Rethinking model scaling for convolutional neural networks[C]//International conference on machine learning. PMLR, 2019: 6105-6114.

[^swin]: Liu Z, Lin Y, Cao Y, et al. Swin transformer: Hierarchical vision transformer using shifted windows[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2021: 10012-10022.

[^sparse]: Child R, Gray S, Radford A, et al. Generating long sequences with sparse transformers[J]. arXiv preprint arXiv:1904.10509, 2019.

[^long]: Beltagy I, Peters M E, Cohan A. Longformer: The long-document transformer[J]. arXiv preprint arXiv:2004.05150, 2020.

[^bigbird]: Zaheer M, Guruganesh G, Dubey K A, et al. Big bird: Transformers for longer sequences[J]. Advances in neural information processing systems, 2020, 33: 17283-17297.

[^reformer]: Kitaev N, Kaiser Ł, Levskaya A. Reformer: The efficient transformer[J]. arXiv preprint arXiv:2001.04451, 2020.


[^route]: Roy A, Saffar M, Vaswani A, et al. Efficient content-based sparse attention with routing transformers[J]. Transactions of the Association for Computational Linguistics, 2021, 9: 53-68.


[^efficient]: Shen Z, Zhang M, Zhao H, et al. Efficient attention: Attention with linear complexities[C]//Proceedings of the IEEE/CVF winter conference on applications of computer vision. 2021: 3531-3539.

[^linear]: Katharopoulos A, Vyas A, Pappas N, et al. Transformers are rnns: Fast autoregressive transformers with linear attention[C]//International conference on machine learning. PMLR, 2020: 5156-5165.


[^performer]: Choromanski K, Likhosherstov V, Dohan D, et al. Rethinking attention with performers[J]. arXiv preprint arXiv:2009.14794, 2020.

[^rfa]: Peng H, Pappas N, Yogatama D, et al. Random feature attention[J]. arXiv preprint arXiv:2103.02143, 2021.

[^flashv1]: Dao T, Fu D, Ermon S, et al. Flashattention: Fast and memory-efficient exact attention with io-awareness[J]. Advances in neural information processing systems, 2022, 35: 16344-16359.

[^flashv2]: Dao T. Flashattention-2: Faster attention with better parallelism and work partitioning[J]. arXiv preprint arXiv:2307.08691, 2023.

[^flashv3]: Shah J, Bikshandi G, Zhang Y, et al. Flashattention-3: Fast and accurate attention with asynchrony and low-precision[J]. Advances in Neural Information Processing Systems, 2024, 37: 68658-68685.