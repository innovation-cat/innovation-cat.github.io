---
title: 'Diffusion Architectures, Part II: Efficiency-Oriented Designs'
excerpt: "Efficiency is a defining challenge for diffusion models, which often suffer from high computational cost and slow inference. This article surveys architectural strategies that enhance efficiency, from latent-space diffusion and multi-resolution cascades to lightweight convolutional blocks, efficient attention mechanisms, and parameter-efficient modules like LoRA. We also examine distillation and inference-time acceleration techniques that drastically reduce sampling steps. Together, these approaches demonstrate how architectural design can expand the reach of diffusion models â€” from research labs to real-time and mobile applications."
date: 2025-03-28
permalink: /posts/2025/03/diffusion-architectures-efficiency/
tags:
  - Diffusion Model
  - UNET
  - Transformer
  - DiT
  - Stability
---


<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>ðŸ“š Table of Contents</strong></span>
  </summary>
  <ul style="margin:0; padding-left:1.1rem;">
	<li><a href="#section1">1. Introduction</a></li>
	<li><a href="#section2">2. Computational Efficiency (Faster)</a>
		<ul>
			<li><a href="#section2.1">2.1 Latent and Multi-Resolution Strategies</a>
				<ul>
					<li><a href="#section2.1.1">2.1.1 Latent Diffusion</a></li>
					<li><a href="#section2.1.2">2.1.2 Multi-Stage Cascades</a></li>
				</ul>
			</li>
			<li><a href="#section2.2">2.2 Lightweight Building Blocks</a></li>
			<li><a href="#section2.3">2.3 Token Reduction in Transformers</a></li>
			<li><a href="#section2.4">2.4 Dynamic Computation</a></li>
			<li><a href="#section2.5">2.5 Summary</a></li>
		</ul>
	</li>	
	<li><a href="#section3">3. Model Size Efficiency (Smaller)</a>
      <ul>
        <li><a href="#section3.1">3.1 Parameter-Efficient Fine-Tuning</a></li>
        <li><a href="#section3.2">3.2 Weight Sharing and Recurrent Blocks</a></li>
        <li><a href="#section3.3">3.3 Distillation and Compact Model Design</a></li>
      </ul>
    </li>
    <li><a href="#section4">4. Memory Efficiency (Leaner)</a></li>
    <li><a href="#section5">5. References</a></li>
  </ul>
</details>


In the previous article, we focused on **architectural strategies for stability**, showing how U-Nets and DiTs differ in their design philosophies and failure modes. In this article, we shift our attention to **efficiency** â€” how diffusion architectures can be redesigned to reduce memory footprint, accelerate training, and enable faster inference. From latent-space modeling (as in Stable Diffusion) to lightweight convolutional blocks, token compression in Transformers, and parameter-efficient modules such as LoRA, we examine a broad spectrum of techniques that make diffusion models more practical and deployable. Efficiency-oriented designs are not merely about saving computation: they fundamentally reshape the accessibility of diffusion models, making them viable on mobile devices, in real-time applications, and at scale across industries.

---

<h1 id="section1" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">1. Introduction</h1>



The last few years have marked a golden age for generative AI, largely powered by the astonishing capabilities of diffusion models. We have witnessed the creation of vivid landscapes from simple phrases, photorealistic portraits indistinguishable from reality, and artistic styles that blend human creativity with algorithmic precision. Models like Stable Diffusion XL, Midjourney, and their contemporaries have redefined the boundaries of digital creation, demonstrating a profound ability to translate abstract concepts into high-fidelity pixels.

Yet, beneath this stunning surface of progress lies a silent, growing bottleneck. Diffusion architectures are notoriously resource-hungry: training often requires large clusters of GPUs for weeks, and inference can involve hundreds of iterative steps with billions of multiplyâ€“accumulate operations. These costs limit accessibility, slow down research iteration, and present major barriers to real-world deployment on edge devices or interactive applications.

The critical question for the next era of generative AI is no longer just **Can we generate it?** but **Can we generate it efficiently, accessibly, and sustainably?** This shift moves the spotlight from raw generative capability to **architectural efficiency**. While algorithmic advances in sampling or optimization provide incremental improvements, the most profound and lasting gains come from rethinking the blueprint itselfâ€”the design of the U-Net or Transformer backbone. Architecture is not only a determinant of quality; it is the primary lever for controlling computational cost, scalability, and usability.

Building on Part I of this series, where we explored architectural strategies for *stability*, this article turns to the equally pressing question of **efficiency**. We examine how diffusion architectures are being redesigned to be not only powerful, but also **faster, smaller, and leaner**, organized into three complementary dimensions:


1. **Computational Efficiency (Faster):** Reducing FLOPs and latency through latent-space modeling, lightweight convolutions, efficient attention mechanisms, and token reduction strategies.
2. **Model Size Efficiency (Smaller):** Shrinking the parameter footprint via weight sharing, low-rank factorization, and distillation, enabling diffusion models to fit into constrained storage and deployment environments.
3. **Memory Efficiency (Leaner):** Lowering training and inference memory usage with techniques such as gradient checkpointing, quantization, pruning, and KV caching, making larger batch sizes and longer sequences feasible.


This article serves as a comprehensive guide to the efficiency-oriented architectural strategies that are making diffusion models practical at scale. Understanding these strategies is vital for both researchers aiming to push the frontier of generative modeling and practitioners seeking to bring diffusion systems into practical, scalable applications.


---

<h1 id="section2" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">2. Computational Efficiency (Faster)</h1>

Diffusion models are inherently iterative: each sample requires dozens to hundreds of denoising steps, with every step involving a full forward pass through a deep architecture. This makes computational efficiency the most visible bottleneck in practice. Architectural choicesâ€”how features are represented, how attention is computed, how convolutional kernels are structuredâ€”directly determine the number of floating-point operations (FLOPs) per step and thus the latency of inference. In this section, we review architectural innovations that reduce computation without sacrificing generation quality.

---

<h1 id="section2.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.1 Latent and Multi-Resolution Strategies</h1>

A unifying principle behind efficiency-oriented architectures is to **avoid running the full diffusion process at full pixel resolution**. Since the cost of convolutions and attention grows rapidly with spatial size, denoising directly in pixel space becomes prohibitive at high resolutions. Two dominant strategies address this challenge: (i) **latent diffusion**, which compresses images into a smaller latent space and performs the entire diffusion process there; and (ii) **multi-stage cascades**, which decompose generation into multiple diffusion stages of increasing resolution. Both approaches drastically reduce computation, but they differ in how the process is structured.

---

<h1 id="section2.1.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">2.1.1 Latent Diffusion</h1>

A key leap in efficiency came with **Latent Diffusion Models (LDMs)** (Rombach et al., 2022). Instead of applying the diffusion process directly on pixel space $\mathbf{x} \in \mathbb{R}^{H \times W \times 3}$, which is computationally expensive, an autoencoder first maps images into a compressed latent space:

$$
\mathbf{z} = \mathcal{E}(\mathbf{x}), \quad \mathbf{x} \approx \mathcal{D}(\mathbf{z}),
$$

where $\mathcal{E}$ and $\mathcal{D}$ are encoderâ€“decoder pairs trained with perceptual and adversarial losses. The diffusion process then operates on $\mathbf{z} \in \mathbb{R}^{h \times w \times c}$, with $h = H/f, \; w = W/f$, and $f \in \{4,8\}$ as the downsampling factor.

This reduces computation approximately by

$$
\text{FLOPs}_{\text{latent}} \approx \frac{1}{f^2} \cdot \text{FLOPs}_{\text{pixel}},
$$

while maintaining perceptual quality through a powerful decoder. In practice, Stable Diffusion achieves up to **16â€“64Ã— savings** compared to pixel-space diffusion.

- **Strength:** The entire denoising process happens in a **single latent space**, greatly reducing FLOPs and memory, while preserving semantics.
- **Weakness:** Performance is tied to the **quality of the autoencoder**; poor reconstruction leads to artifacts.


---

<h1 id="section2.1.2" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">2.1.2 Multi-Stage Cascades</h1>

Another efficiency-oriented design is the use of **multi-stage cascaded models** (Ho et al., 2022; Saharia et al., 2022; Podell et al., 2023). instead of compressing the space, they **split generation into multiple independent diffusion stages**, each operating at progressively higher resolutions. Formally, for a target image $\mathbf{x}_T$:

$$
p(\mathbf{x}_T) \approx p_{\text{base}}(\mathbf{x}_{\text{low}}) \cdot p_{\text{refiner}}(\mathbf{x}_T \mid \mathbf{x}_{\text{low}}),
$$

where the **base model** generates a coarse low-resolution sample (e.g., 64Ã—64), and **refiner models** upsample and add detail at higher resolutions.

During training, each stage is a **standard diffusion model trained at its target resolution**. At inference, all stages are executed in sequence:

- **Base stage:** many denoising steps (e.g., 50â€“100) at low resolution, where compute is cheap.

- **Refiner stage(s):** fewer steps (e.g., 10â€“20) at high resolution, where compute is expensive but limited.

The core idea of cascaded model is to break down this generative task into a series of simpler tasks, which are accomplished in succession by multiple independent diffusion models. Each model plays the role of an independent expert. For instance, the basic model focuses on ensuring the global composition of the image but does not care about the details, while the subsequent refined model conducts more refined and detailed processing based on the previous output.



- **Strength:** Efficient **compute allocation**â€”global structure from the base, fine details from refiners. Modular design allows refiners to be swapped or specialized.

- **Weakness:** Requires **training and storing multiple diffusion models**, increasing system complexity.

---

<h1 id="section2.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.2 Lightweight Building Blocks</h1>

Beyond operating at reduced resolutions, another path to efficiency is to redesign the **fundamental building blocks** of diffusion backbones. Convolution and attention are the two dominant computational modules: U-Netâ€“based architectures rely heavily on convolutions, while Transformer-based DiTs are dominated by self-attention. Both can be optimized for efficiency without severely compromising quality.


---

<h1 id="section2.2.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">2.2.1 Efficient Convolutions</h1>

In U-Net style backbones, most computation comes from repeated $3\times3$ convolutions. A standard convolution of kernel size $K$ with input channels $C_{\text{in}}$ and output channels $C_{\text{out}}$ has cost:

$$
\text{FLOPs}_{\text{conv}} = H \cdot W \cdot K^2 \cdot C_{\text{in}} \cdot C_{\text{out}}.
$$

The number of parameters is:

$$
\text{Param}_{\text{conv}} = K^2 \cdot C_{\text{in}} \cdot C_{\text{out}}
$$

To reduce cost, several lightweight alternatives are widely adopted:


## <span style="color:#3498DB;">A: Depthwise-Separable Convolution</span>

depthwise-separable convolution [^Xception] [^mobilenet] decomposes a standard convolution into two lightweight components:  
- **depthwise convolution**: that applies a single spatial filter per input channel. specifically, each input channel is filtered independently with a spatial kernel of size $ K \times K \times 1 $. The output remains $ C_{\text{in}} $ channels. The computational cost (in FLOPs) and the number of parameters are:
  
  $$
  \text{FLOPs}_{\text{dw}} =  H' \cdot W' \cdot K^2 \cdot C_{\text{in}},\qquad \text{Param}_{\text{dw}} = K^2 \cdot C_{\text{in}}
  $$


- **pointwise convolution**: A 1Ã—1 convolution is applied to project the $ C_{\text{in}} $-channel feature map into $ C_{\text{out}} $ channels. The computational cost (in FLOPs) and the number of parameters are:
  
  $$
  \text{FLOPs}_{\text{pw}} =  H' \cdot W' \cdot C_{\text{in}} \cdot C_{\text{out}},\qquad \text{Param}_{\text{pw}} = C_{\text{in}} \cdot C_{\text{out}}
  $$
  
The total computational cost (in FLOPs) and the total number of parameters are:

$$
\begin{align}
  & \text{FLOPs}_{\text{seq}} = \text{FLOPs}_{\text{dw}} + \text{FLOPs}_{\text{pw}} = H' \cdot W' \cdot \left( K^2 \cdot C_{\text{in}} + C_{\text{in}} \cdot C_{\text{out}} \right) \\[10pt]
  & \text{Param}_{\text{seq}} = \text{FLOPs}_{\text{dw}} + \text{FLOPs}_{\text{pw}} = K^2 \cdot C_{\text{in}} + C_{\text{in}} \cdot C_{\text{out}}
\end{align}
$$

The relative computational reduction compared to standard convolution is:

$$
\frac{\text{FLOPs}_{\text{seq}}}{\text{FLOPs}_{\text{conv}}} \approx \frac{1}{C_{\text{out}}} + \frac{1}{K^2}
$$

For typical values (e.g., $ K=3, C_{\text{out}} \geq 64 $), this yields more than **85%** reduction in FLOPs.


## <span style="color:#3498DB;">B: Group Convolution</span>

Divide the input channel and the output channel into $g$ groups, and convolution is only computed within each groups separately.



The total computational cost (in FLOPs) and the total number of parameters are:

$$
 \text{FLOPs}_{\text{group}} = H' \cdot W' \cdot \left( K^2 \cdot \frac{C_{\text{in}} \cdot C_{\text{out}}}{g} \right)\,\qquad \text{Param}_{\text{group}} =  K^2 \frac{C_{\text{in}} \cdot C_{\text{out}}}{g}
$$



---

<h1 id="section2.2.2" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">2.2.1 Efficient Attention</h1>

For Transformer-based DiTs, the bottleneck lies in the **quadratic cost of self-attention**. Standard self-attention on $N$ tokens of dimension $d$:

$$
\text{FLOPs}_{\text{attn}} = O(N^2 \cdot d).
$$

At high resolution ($N = H \cdot W$), this quickly dominates compute. Several architectural strategies address this:

## <span style="color:#3498DB;">A: Linear Attention</span>

## <span style="color:#3498DB;">B: Local Attention</span>

## <span style="color:#3498DB;">C: Sparse / Low-Rank Attention</span>

## <span style="color:#3498DB;">D: FlashAttention</span>

---

<h1 id="section9" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">9. References</h1>


[^mobilenet]: Howard A G, Zhu M, Chen B, et al. Mobilenets: Efficient convolutional neural networks for mobile vision applications[J]. arXiv preprint arXiv:1704.04861, 2017.

[^Xception]: Chollet F. Xception: Deep learning with depthwise separable convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 1251-1258.