---
title: 'Controlled Generation with Diffusion Models'
date: 2025-07-03
excerpt: "Over the past few years, controllable generation has become the central theme in the evolution of diffusion models. What began as a purely stochastic process for unconditional image synthesis has transformed into a programmable system capable of following text prompts, sketches, poses, depth maps, and even personalized identities. From Classifier-Free Guidance that amplifies conditional gradients, to Textual Inversion and DreamBooth that learn new concepts, to LoRA and ControlNet that extend controllability through lightweight adapters‚Äîeach technique represents a different way of injecting intention into noise. This article traces the unifying logic behind these seemingly independent methods, revealing that all controllable diffusion approaches ultimately share a common goal: to reshape the diffusion trajectory so that generation obeys human-defined constraints while preserving creativity and diversity."
permalink: /posts/2025/07/controlled-generation/
tags:
  - cool posts
  - category1
  - category2
---


<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>üìö Table of Contents</strong></span>
  </summary>
  <ul style="margin:0; padding-left:1.1rem;">
	<li><a href="#section1">1. Introduction: From Unconditional to Controllable Generation</a></li>
	<li><a href="#section2">2. Steering the Diffusion Process with Global Guidance</a>
		<ul>
			<li><a href="#section2.1">2.1 A Unified Principle for Guidance</a></li>
			<li><a href="#section2.2">2.2 Classifier Guidance</a></li>
			<li><a href="#section2.3">2.3 Classifier-Free Guidance (CFG)</a></li>
			<li><a href="#section2.4">2.4 Multi-Condition Guidance in Diffusion Models</a>
				<ul>
					<li><a href="#section2.4.1">2.4.1 Problem Setup and Notation</a></li>
					<li><a href="#section2.4.2">2.4.2 Training with Multiple Channels: Per-Channel Dropout</a></li>
					<li><a href="#section2.4.3">2.4.3 Inference: Channel-Wise Guidance and Composition</a></li>
				</ul>
			</li>
		</ul>
	</li>	
	<li><a href="#section3">3. Local Guidance: Latent and Semantic Control</a></li>
	<li><a href="#section4">4. Personalized Control: Injecting New Concepts into Diffusion Models</a></li>
	<li><a href="#section5">5. Structured Condition Control</a></li>
	<li><a href="#section6">6. Advanced Paradigms: Hybrid and Multimodal Controls</a></li>
    <li><a href="#section7">7. References</a></li>
  </ul>
</details>



Diffusion models have emerged as a dominant paradigm for high-fidelity image synthesis, yet their native formulation remains largely **uncontrolled**, relying on stochastic sampling from an unconditional generative process. As real-world applications increasingly demand specificity, consistency, and interpretability, the field has rapidly evolved toward **controllable diffusion generation**‚Äîa unified framework for steering the sampling trajectory according to external conditions, semantic concepts, or structural priors.

 

In the previous articles, we have explored various aspects of knowledge related to diffusion models, covering their theoretical foundations, core algorithms, and architectural designs, as well as the training stability and efficiency. However, all the generation methods we discussed earlier are unconditional generation. This post provides a comprehensive analysis of controllable generation from a unified theoretical and architectural perspective.

---

<h1 id="section1" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">1. Introduction</h1>





Current research on controllable diffusion models can be systematically organized into three major categories, defined by where and how the control signal interacts with the diffusion process.

- **Category 1: Guidance-based Control**. This type of technology does not modify the model itself nor learn new knowledge. During inference, it adjusts the generation direction through algorithms to enhance the model's adherence to existing instructions (primarily text prompts).

- **Category 2: Finetuning-based Control**. This type of technology injects new, personalized knowledge into the model‚Äîsuch as specific faces, objects, or artistic styles‚Äîby finetuning part or all of the model's parameters.

- **Category 3: Structure-based Control**. This type of technology represents a major revolution in the field of controllable generation. Instead of relying on ambiguous text, it precisely controls the composition, layout, and structure of the generated image by providing explicit, pixel-level spatial information (such as skeletal poses, depth maps, edge maps, etc.).


Specifically, Guidance-based Control can be further divided into Global and Local (Semantic) subtypes, we summarize and compare these three Principal categories of Controllable Diffusion Models as follows.


<table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; width: 100%; text-align: center;">
  <thead>
    <tr>
      <th style="text-align: center; vertical-align: middle;">Category</th>
      <th style="text-align: center; vertical-align: middle;">Subtype / Control Level</th>
      <th style="text-align: center; vertical-align: middle;">Representative Methods</th>
      <th style="text-align: center; vertical-align: middle;">Model Modified</th>
      <th style="text-align: center; vertical-align: middle;">Learns New Knowledge</th>
      <th style="text-align: center; vertical-align: middle;">Control Scope</th>
      <th style="text-align: center; vertical-align: middle;">Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="2" style="text-align: center; vertical-align: middle;"><strong>1. Guidance-based Control</strong></td>
      <td style="text-align: center; vertical-align: middle;"><strong>(a) Global Guidance</strong></td>
      <td style="text-align: center; vertical-align: middle;">Classifier Guidance, Classifier-Free Guidance</td>
      <td style="text-align: center; vertical-align: middle;">‚ùå No</td>
      <td style="text-align: center; vertical-align: middle;">‚ùå No</td>
      <td style="text-align: center; vertical-align: middle;">Global</td>
      <td style="text-align: center; vertical-align: middle;">Steers the overall sampling trajectory by adjusting score or noise predictions across all time steps.</td>
    </tr>
    <tr>
      <td style="text-align: center; vertical-align: middle;"><strong>(b) Local / Semantic Guidance</strong></td>
      <td style="text-align: center; vertical-align: middle;">Attention Control, Prompt-to-Prompt, Null-Text Optimization, DiffEdit, EDICT</td>
      <td style="text-align: center; vertical-align: middle;">‚ùå No</td>
      <td style="text-align: center; vertical-align: middle;">‚ùå No</td>
      <td style="text-align: center; vertical-align: middle;">Local / Semantic</td>
      <td style="text-align: center; vertical-align: middle;">Modifies intermediate latent or attention representations to achieve localized, interpretable control.</td>
    </tr>
    <tr>
      <td style="text-align: center; vertical-align: middle;"><strong>2. Model Personalization</strong></td>
      <td style="text-align: center; vertical-align: middle;">Parameter-level Control</td>
      <td style="text-align: center; vertical-align: middle;">LoRA, DreamBooth, Textual Inversion, Custom Diffusion</td>
      <td style="text-align: center; vertical-align: middle;">‚úÖ Yes</td>
      <td style="text-align: center; vertical-align: middle;">‚úÖ Yes</td>
      <td style="text-align: center; vertical-align: middle;">Instance / Concept</td>
      <td style="text-align: center; vertical-align: middle;">Fine-tunes part or all of the model to inject new, personalized or domain-specific knowledge.</td>
    </tr>
    <tr>
      <td style="text-align: center; vertical-align: middle;"><strong>3. Structured Condition Control</strong></td>
      <td style="text-align: center; vertical-align: middle;">Input-level Control</td>
      <td style="text-align: center; vertical-align: middle;">ControlNet, T2I-Adapter, IP-Adapter, SDEdit</td>
      <td style="text-align: center; vertical-align: middle;">‚ùå No</td>
      <td style="text-align: center; vertical-align: middle;">‚ùå No</td>
      <td style="text-align: center; vertical-align: middle;">Spatial / Structural</td>
      <td style="text-align: center; vertical-align: middle;">Incorporates explicit spatial conditions (pose, depth, edge, segmentation) to control composition and layout.</td>
    </tr>
  </tbody>
</table>


Finally, we discuss emerging trends including **compositional and multimodal control**, it is not a fundamentally new mechanism.
Rather, it extends the above three paradigms to multi-signal or multi-modal conditioning ‚Äî for example, combining text + pose (ControlNet), text + image exemplar (IP-Adapter), or text + video + audio (AnimateDiff, VideoFusion). It operates as a compositional layer across the three categories, not a standalone paradigm.


---

<h1 id="section2" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">2. Steering the Diffusion Process with Global Guidance</h1>

Diffusion models were originally designed as **unconditional** generators that progressively denoise Gaussian noise to recover clean data samples. In many real applications, however, users desire to steer the generation toward specific conditions‚Äîtext prompts, categories, objects, or visual attributes‚Äîwithout retraining the model. **Inference-time guidance** methods accomplish exactly this: they introduce an external ‚Äúforce field‚Äù that modifies the sampling trajectory so that the generated sample follows the direction favored by a given condition $c$. The model itself remains fixed; only the sampling dynamics are altered.


---

<h1 id="section2.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.1 A Unified Principle for Global Guidance</h1>

**Goal.** We want to steer a **fixed** diffusion model at **inference time** ‚Äî without retraining ‚Äî so that the generated sample $x_0$ better matches a user condition $c$ (text, class label, image cue, etc.). All methods in this family can be seen as injecting a **condition-dependent force field** into the reverse denoising dynamics. Recall the reverse-time sampler (SDE or ODE) of the **unconditional** model be

$$
\mathrm{d}x_t =\Big[f(t)x_t - g^2(t)\nabla_{x_t}\log p_t(x_t)\Big]\mathrm{d}t + g(t)\,\mathrm{d}\bar w_t,
$$


  
where $p_t(x_t)$ is the marginal density at time $t$. Replacing the unconditional score by the conditional one yields:

$$
\mathrm{d}x_t =\Big[f(t)x_t - g^2(t)\nabla_{x_t}\log p_t(x_t \mid c)\Big]\mathrm{d}t + g(t)\,\mathrm{d}\bar w_t,
$$

where $c$ is condition signal. By Bayes' rule, the conditional score can be decomposed into two components.

$$
\nabla_{x_t}\log p(x_t \mid c) 
= \nabla_{x_t}\log p(c \mid x_t) \;+\; \nabla_{x_t}\log p(x_t)\label{eq:score_based_lens}
$$

Replacing the conditional score and yields:

$$
\mathrm{d}x_t=\Big[f(t)x_t - g^2(t)\underbrace{\nabla_{x_t}\log p_t(x_t)}_{\text{unconditional score}} - g^2(t)\underbrace{\nabla_{x_t}\log p_t(c \mid x_t)}_{\text{classifier gradient}} \Big]\mathrm{d}t  + g(t)\,\mathrm{d}\bar w_t\label{eq:cg}

$$

This equation shows that **conditional generation** can be achieved by **adding a drift term** proportional to $\nabla_{x_t}\log p(c\mid x_t)$‚Äîan external force directing samples toward class $c$. **Different guidance families differ only in how they approximate the guidance direction term.**


Equation \ref{eq:score_based_lens} is derived from the perspective of score prediction; in fact, we can generalize this conclusion to other objectives: For all four common parameterizations‚Äî**$\epsilon$-prediction**, **$v$-prediction**, **score-prediction**, and **$x_0$-prediction** ‚Äî guidance reduces to the **same mixing rule** :

$$
g_{\text{total}} \;=\; g_{\text{uncond}} \;+\; w\,g_{\text{guide}},
\qquad g\in{\epsilon,\,v,\,s,\,x_0}.
$$

For the convenience of discussion, we will base all subsequent discussions on $\epsilon$-prediction.

















---

<h1 id="section2.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.2 Classifier Guidance</h1>


Classifier Guidance [^cg] was the first practical approach to achieve **controllable generation** in diffusion models **without retraining the generative model itself**. It introduces an **external classifier** to steer the diffusion sampling process toward samples consistent with a target condition‚Äîtypically a class label $c$.



Since $p(y\mid x_t)$ is unknown, an auxiliary **noisy-image classifier** $p_\phi(y\mid x_t, t)$ is trained on images corrupted with the same diffusion noise. Its gradient approximates the desired term:

$$
\nabla_{x_t}\log p(c \mid x_t) \;\approx\; \nabla_{x_t}\log p_\phi(c \mid x_t)
$$

This is why it is called classifier guidance. Substituting into \ref{eq:cg} gives the **classifier-guided reverse SDE**:


$$
\mathrm{d}x_t=\Big[f(t)x_t - g^2(t)\nabla_{x_t}\log p_t(x_t) - s(t)\nabla_{x_t}\log p_\phi(c\mid x_t) \Big]\mathrm{d}t  + g(t)\,\mathrm{d}\bar w_t,

$$

where $s(t)$ controls the **guidance strength** (often increasing as noise decreases). The classifier gradient can be efficiently obtained by **backpropagating** through the classifier with respect to its input $x_t$.




<strong style="color: blue;">Summary: </strong> Classifier Guidance marked the first practical demonstration that diffusion trajectories can be steered at inference time by introducing an external gradient field derived from a classifier. Its theoretical contribution was fundamental: it established the now-standard view that conditional generation equals unconditional generation plus a conditional gradient term. However, the approach suffers from several inherent limitations:

- Firstly, it requires training an additional classifier on noisy images for every diffusion timestep, incurs substantial computational cost due to per-step backpropagation.

- Secondly, and most critically, it offers control only over discrete semantic categories rather than fine-grained attributes or spatial structures. 

As a result, its applicability remains narrow and inefficient for complex or continuous conditions. Nevertheless, Classifier Guidance‚Äôs conceptual insight directly inspired subsequent advances such as Classifier-Free Guidance, Attention-based Control, and ControlNet, which internalized or generalized the same principle into far more flexible and efficient conditioning mechanisms.


---

<h1 id="section2.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.3 Classifier-Free Guidance (CFG)</h1>

Classifier-Free Guidance [^cfg] is the most widely used inference-time guidance method in modern diffusion systems (e.g., Stable Diffusion, Imagen, SDXL). It **internalizes** the conditional gradient used by Classifier Guidance, but **removes the external classifier**: during training, the same denoiser learns both **conditional** and **unconditional** predictions. At inference, we combine these two predictions to emulate ascent on the conditional likelihood‚Äîrecovering the unified ‚Äúexternal force‚Äù view while keeping the model frozen.


<strong style="color: blue;">Training setup: </strong> Let $c$ denote conditioning (text, label, etc.). During training, with probability $p_{\text{drop}}$ we **replace $c$ by a special null token** $\varnothing$. The standard denoising loss  is

$$
\min_\theta\;\mathbb{E}_{x_0,c,t,\epsilon}
\Big\|\epsilon - \epsilon_\theta(x_t,\tilde c,t)\Big\|^2,
\quad
\tilde c=\begin{cases}
c & \text{w.p. } 1-p_{\text{drop}},\\ \\
\varnothing & \text{w.p. } p_{\text{drop}}.
\end{cases}
$$

Thus the **same network** learns two behaviors: **Conditional branch**: $\epsilon_\theta(x_t,c,t)$, and  **Unconditional branch**: $\epsilon_\theta(x_t,\varnothing,t)$.

This duality is the key to estimating $\nabla_{x_t}\log p(c\mid x_t)$ **without** a classifier.  By Bayes,

$$
\begin{align}
\nabla_{x_t}\log p(c\mid x_t)
& =\nabla_{x_t}\log p(x_t\mid c)-\nabla_{x_t}\log p(x_t) \\[10pt]
& \approx -\frac{1}{\sigma_t}\Big(\epsilon_\theta(x_t,c,t)-\epsilon_\theta(x_t,\varnothing,t)\Big)
\end{align}
$$

This means CFG replaces the external estimator with an **internal contrast** between the model‚Äôs conditional and unconditional predictions. 

$$
{\epsilon}_{\text{guide}} = {\epsilon}_{\text{cond}} - {\epsilon}_{\text{uncond}}   
$$


Plug this approximation into the unified dynamics and absorb constants into $\lambda(t)$. In **discrete sampling** (DDPM/DDIM/ODE), this becomes the familiar **guided denoiser**:

$$
\begin{align}
{\epsilon}_{\text{total}} & = {\epsilon}_{\text{uncond}} + w(t)\cdot{\epsilon}_{\text{guide}} = {\epsilon}_{\text{uncond}} + w(t)\cdot\left( {\epsilon}_{\text{cond}} - {\epsilon}_{\text{uncond}} \right) \\[10pt]
& =\epsilon_\theta(x_t,\varnothing,t) + w(t)\cdot\left(\epsilon_\theta(x_t,c,t)\;-\epsilon_\theta(x_t,\varnothing,t) \right) \\[10pt]
& = w(t)\cdot\epsilon_\theta(x_t,c,t) + (1-w(t)) \cdot\epsilon_\theta(x_t,\varnothing,t) 
\end{align}
$$

where $w(t)\ge 0$ is the **guidance scale**. Large $w$ increases faithfulness to $c$ (mode-seeking), small $w$ preserves diversity (mode-covering).


<strong style="color: blue;">Summary: </strong> Classifier-Free Guidance (CFG) represents a pivotal evolution of the inference-time guidance paradigm first introduced by Classifier Guidance (CG).

- CFG eliminates the need for a noisy-image classifier, drastically reducing computational and training cost.
- It also generalizes beyond discrete categories, allowing conditions to be continuous, multimodal, or high-dimensional (e.g., text embeddings, style vectors).
- It also generalizes beyond discrete categories, allowing conditions to be continuous, multimodal, or high-dimensional (e.g., text embeddings, style vectors).

For this reason, CFG became the standard backbone of controllable diffusion systems such as Stable Diffusion, Imagen, and SDXL.




---

<h1 id="section2.4" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.4 Multi-Condition Guidance in Diffusion Models</h1>


In practice, Conditional diffusion models are rarely driven by a single signal in practice. Real deployments combine **heterogeneous condition types (channels)**‚Äîe.g., **Text**, **Layout/Boxes**, **Segmentation**, **Depth/Normals**, **Reference Image/Style**‚Äîto control complementary aspects of content and geometry. Moreover, conditions may be **positive** (encouraging the presence of attributes) or **negative** (suppressing undesired content). These signals can be **statistically independent**, **weakly dependent**, or **directly conflicting** (e.g., a style that clashes with layout geometry).

In previous section, we discuss the classical guidance mechanisms (e.g., classifier guidance (CG) and classifier-free guidance (CFG)) with single condition. Extending them **systematically** to multiple, potentially competing conditions requires a careful treatment of the following questions:

- How to train with multiple channels, this is crucial for the guidance sampling with multiple conditions.

- How to combine multi-condition at inference, and get the final guidance directions.

- Handling **dependencies and conflicts** across channels, including **priority** and **scheduling**.


<div class="qa-block">

<p><strong>Definition (Channel):</strong> A channel is a condition type with its own encoder and null state (e.g., text tokens and a text-null token; depth image and a "depth-off" code).</p>



<p><strong>Key clarification:</strong>  Multi-condition refers to <strong>multiple condition types/channels</strong>, <strong>not</strong> the count of inputs within a type. One hundred sentences still constitute <strong>one text channel</strong>; one depth map per target image constitutes <strong>one depth channel</strong>. When desired, a single channel (e.g., text) may be factorized into sub-conditions for finer control, but it remains one modality/channel.</p>
  
</div>




---

<h1 id="section2.4.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">2.4.1 Problem Setup and Notation</h1>

Let $x_0\sim p_{\text{data}}$, and $x_t=\alpha(t)x_0+\sigma(t)\epsilon$ be a VP-type forward process. Let channels 

$$\mathcal{K}=\{\text{text},\text{layout},\text{seg},\text{depth},\text{normal},\text{ref},\dots\}$$ 

For channel $k\in\mathcal{K}$, denote its input by $c_k$ as **positive** conditions,   $c^-_k$ as **negative** conditions and its **channel-specific null** by $\varnothing^{(k)}$. Formally, we can uniformly represent the set of multiple conditions as


$$
\mathcal{C}\;=\;\{c_1,\ldots,c_K\}\;\cup\;\{c^-_1,\ldots,c^-_K\}
$$

where $c_k$ are **positive** conditions to be promoted and $c^-_m$ are **negative** conditions to be suppressed. The **unconditional** probability-flow ODE is

$$
\frac{dx_t}{dt}=f_\theta(x_t,t).
$$

Guidance augments the drift by a vector field

$$
\frac{dx_t}{dt}=f_\theta(x_t,t)+\lambda(t),g(x_t,t),
\quad
g(x_t,t)=\nabla_{x_t}\log p(\mathcal{C}\mid x_t),
$$


We adopt $\epsilon$-prediction for exposition; formulas transpose to $x_0$ and $v$-prediction.





---

<h1 id="section2.4.2" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">2.4.2 Training with Multiple Channels: Per-Channel Dropout</h1>




Training must align the model with all **subsets of channels** it may face at inference. We therefore use **independent per-channel dropout** (a.k.a. classifier-free dropout per channel).

To this end, we define **Sampling masks** For each channel $k$, draw a keep mask

$$
m_k\sim\mathrm{Bernoulli}\,\big(k_k(t)\big),\quad k_k(t)=1-q_k(t),
$$

and form the training input

$$
\tilde c_k=\begin{cases}
c_k & \text{if } m_k=1\ \text{and channel is available},\\ \\
\varnothing_k & \text{otherwise}.
\end{cases}
$$

The loss (for $\epsilon$-prediction) is

$$
\mathcal{L}=\mathbb{E}_{x_0,t,\epsilon}\ \mathbb{E}_{{m_k}}\ \big\|\epsilon-\epsilon_\theta\big(x_t,\,t;\tilde c_1,\ldots,\tilde c_{|\mathcal{K}|}\big)\big\|_2^2.
$$

**Unconditional frequency.** If a target unconditional rate (u\in[0.1,0.2]) is desired,

$$
\Pr\{\text{all dropped}\}=\prod_{k} q_k(t)\approx u,
$$

e.g., with equal rates $q_k=q$ one can set $q=u^{1/\|\mathcal{K}\|}$. Optionally use **SNR/$\sigma$-aware** $q_k(t)$ (higher dropout at high noise).

**Implementation essentials:** Each channel must have its **own null embedding** . There is no "negative condition" in training; negative guidance only exists in inference.




---

<h1 id="section2.4.3" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">2.4.3 Inference: Channel-Wise Guidance and Composition</h1>


We consider a set of conditions consists of **positive** conditions $\mathcal{P}$ and **negative** conditions $\mathcal{N}$

$$
\mathcal{C} \;=\; \underbrace{\{c_1,\ldots,c_K\}}_{\text{positive conditions} \mathcal{P}} \;\cup\; \underbrace{\{c^-_1,\ldots,c^-_M\}}_{\text{negative conditions} \mathcal{N}}
$$

where $c_k$ are **positive** conditions to be promoted and $c^-_m$ are **negative** conditions to be suppressed. Assume the conditions are **conditionally independent given $x_t$**. Using a first-order sum-of-experts surrogate,

$$
\nabla_{x_t}\log p_t(\mathcal{C} \mid x_t)
\;\approx\;\sum_{k=1}^K\nabla_{x_t}\log p_t(c_k\mid x_t)\;-\;\sum_{m=1}^M\nabla_{x_t}\log p_t(c^-_m\mid x_t).
$$



At inference we form **channel-wise conditional‚Äìunconditional differences** and linearly compose them. Let

$$
\epsilon_u=\epsilon_\theta(x_t,t;\ \forall k:\ \varnothing^{(k)}),\quad
\epsilon_{k}=\epsilon_\theta\big(x_t,t;\ c^{(k)}\text{ on},\ \text{others as configured}\big).
$$

A simple and robust choice is to define the **per-channel difference**

$$
d_k=\epsilon_{k}-\epsilon_u,
$$

and the **guided prediction**

$$
\epsilon_{\text{total}}
=\epsilon_u+\underbrace{\sum_{k\in\mathcal{P}} w_k(t)\,d_k}_{\text{positive guide}} \;-\; \underbrace{\sum_{k\in\mathcal{N}} w_k^-(t)\,d_k}_{\text{negative guide}}
$$

for positive channels $\mathcal{P}$ and negative channels $\mathcal{N}$ (negative prompts use negative weights). Plug $\epsilon_{\text{total}}$ into any sampler (DDIM, Heun/RK, DPM-Solver, ‚Ä¶).
**SNR/(\sigma)-aware scheduling.** Use (w_k(t)=\bar w_k\cdot\mathrm{SNR}(t)^{\alpha_s}) or (w_k(\sigma)=\bar w_k\cdot \sigma^{-\beta}) to weaken guidance at high noise and avoid late oversharpening (typical (\alpha_s,\beta\in[0.3,0.7])).

*Note.* If a single channel (e.g., text) is deliberately split into ‚Äúsub-prompts‚Äù, (1) still applies with the *sub-prompts* indexed by (k). This is a *within-channel* factorization for finer control, not a new modality.



---

<h1 id="section3" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">3. Local Guidance: Latent and Semantic Control</h1>


While global guidance methods like CFG are remarkably effective at strengthening the alignment between a generated image and its condition, the guidance scale w amplifies the conditional signal across the entire image, pushing the whole generation trajectory in a uniform direction. This approach lacks the finesse needed for region-specific edits, object-level manipulation, or preserving the layout of an image while changing its content.


To overcome these limitations, a new family of methods emerged: Local or Semantic Guidance. Instead of modifying the final output vector, local/semantic control intervenes inside the diffusion trajectory‚Äîon selected tokens, spatial regions, or intermediate features‚Äîso edits are localized and interpretable (e.g., "cat‚Üídog" without touching background). The model remains frozen; we alter the representations it manipulates.

---

<h1 id="section3.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.1 A Unified Principle for Local Guidance</h1>

Classifier-Free Guidance (CFG) applies a global, scalar steering to the entire sampling trajectory‚Äîmodifying the overall denoising direction for all pixels and semantics.

$$
{\epsilon}_{\text{total}}  = \underbrace{\epsilon_\theta(x_t, t, \varnothing)}_{\text{uncondition}} + w(t)\cdot \left( \underbrace{\overbrace{\epsilon_\theta(x_t, t, c)}^{\text{condition}} - \overbrace{\epsilon_\theta(x_t, t, \varnothing)}^{\text{uncondition}}}_{\text{guide direction}} \right)
$$

whereas Local Guidance performs spatially or semantically localized steering, altering only selected latent regions or attention features while keeping the rest of the image unchanged.

$$
{\epsilon}_{\text{total}} = \epsilon_\theta(x_t, t, c) + \lambda(t)\, M \odot \Delta(x_t, t, e)
$$

where:

<table border="1" cellpadding="10" cellspacing="0" style="border-collapse: collapse; width: 100%; text-align: center;">
  <thead>
    <tr>
      <th style="text-align: center; vertical-align: middle;">Symbol</th>
      <th style="text-align: center; vertical-align: middle;">Meaning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center; vertical-align: middle;">$ x_t \in \mathbb{R}^{H\times W\times d} $</td>
      <td style="text-align: center; vertical-align: middle;">latent representation at timestep $ t $</td>
    </tr>
    <tr>
      <td style="text-align: center; vertical-align: middle;">$ c $</td>
      <td style="text-align: center; vertical-align: middle;">original condition (e.g., text prompt)</td>
    </tr>
    <tr>
      <td style="text-align: center; vertical-align: middle;">$ e $</td>
      <td style="text-align: center; vertical-align: middle;">edit or local control specification (e.g., ‚Äúcat ‚Üí dog‚Äù, ‚Äúmake sky blue‚Äù)</td>
    </tr>
    <tr>
      <td style="text-align: center; vertical-align: middle;">$ M \in [0,1]^{H\times W} $</td>
      <td style="text-align: center; vertical-align: middle;">spatial or attention-based mask defining the region or semantic scope of the edit</td>
    </tr>
    <tr>
      <td style="text-align: center; vertical-align: middle;">$ \lambda(t) $</td>
      <td style="text-align: center; vertical-align: middle;">step-dependent modulation strength, often proportional to $ \mathrm{SNR}(t)^\alpha $</td>
    </tr>
    <tr>
      <td style="text-align: center; vertical-align: middle;">$ \Delta(x_t, t, e) $</td>
      <td style="text-align: center; vertical-align: middle;">method-specific residual encoding the local semantic adjustment</td>
    </tr>
    <tr>
      <td style="text-align: center; vertical-align: middle;">$ \odot $</td>
      <td style="text-align: center; vertical-align: middle;">element-wise product applying the residual only within the masked region</td>
    </tr>
  </tbody>
</table>

This formulation unifies diverse local-control techniques under a single principle: All local guidance methods realize masked residual steering within the latent trajectory. Within this unified framework, different algorithms correspond to different realizations of the residual term $\Delta(\cdot)$ and mask $M$:


<table border="1" cellpadding="10" cellspacing="0" style="border-collapse: collapse; width: 100%; text-align: center;">
  <thead>
    <tr>
      <th style="text-align: center; vertical-align: middle;">Sub-family</th>
      <th style="text-align: center; vertical-align: middle;">Mechanism</th>
      <th style="text-align: center; vertical-align: middle;">Instantiation of ($\Delta$) and (M)</th>
      <th style="text-align: center; vertical-align: middle;">Representative Works</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center; vertical-align: middle;"><strong>Attention-space substitution</strong></td>
      <td style="text-align: center; vertical-align: middle;">Replace or interpolate cross-attention maps corresponding to edited tokens.</td>
      <td style="text-align: center; vertical-align: middle;">($\Delta = \mathbf{A}^{\text{tgt}} - \mathbf{A}^{\text{src}}$), (M) = attention window</td>
      <td style="text-align: center; vertical-align: middle;"><em>Prompt-to-Prompt</em>, <em>Attention Control</em></td>
    </tr>
    <tr>
      <td style="text-align: center; vertical-align: middle;"><strong>Latent inversion & re-editing</strong></td>
      <td style="text-align: center; vertical-align: middle;">Invert image to latent trajectory, then apply semantic offset before re-denoising.</td>
      <td style="text-align: center; vertical-align: middle;">($\Delta = \partial \epsilon_\theta/\partial e$ estimated along inversion path)</td>
      <td style="text-align: center; vertical-align: middle;"><em>DDIM Inversion</em>, <em>Null-Text Optimization</em></td>
    </tr>
    <tr>
      <td style="text-align: center; vertical-align: middle;"><strong>Masked diffusion editing</strong></td>
      <td style="text-align: center; vertical-align: middle;">Re-noise a spatial region and resample with new condition.</td>
      <td style="text-align: center; vertical-align: middle;">(M) = explicit mask, ($\Delta = \epsilon_\theta(x_t,t,c') - \epsilon_\theta(x_t,t,c)$)</td>
      <td style="text-align: center; vertical-align: middle;"><em>DiffEdit</em>, <em>SDEdit</em></td>
    </tr>
    <tr>
      <td style="text-align: center; vertical-align: middle;"><strong>Consistency / energy-based editing</strong></td>
      <td style="text-align: center; vertical-align: middle;">Minimize energy enforcing bi-directional consistency between original and edited features.</td>
      <td style="text-align: center; vertical-align: middle;">($\Delta = \nabla_x E(x_t,e)$) from learned or analytical energy</td>
      <td style="text-align: center; vertical-align: middle;"><em>EDICT</em>, <em>SeD</em></td>
    </tr>
  </tbody>
</table>

---

<h1 id="section3.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.2 Prompt-to-Prompt Image Editing</h1>


**Prompt-to-Prompt (P2P) [^p2p] ** aims to enable **semantic-level editing** of generated images by modifying only the textual prompt‚Äîsuch as replacing nouns, adding attributes, or inserting short phrases‚Äîwhile keeping the underlying diffusion model frozen. Its core objective is to ensure that the resulting image faithfully reflects the new prompt (c') while **preserving all visual aspects** of the original prompt (c) that remain unchanged. In other words, P2P seeks to achieve *prompt-driven local editing*: producing the desired modification (e.g., changing ‚Äúa cat on a sofa‚Äù to ‚Äúa dog on a sofa‚Äù) **without altering the scene layout, lighting, or other unaffected content**, thus realizing controllable and consistent image manipulation purely through cross-attention guidance during inference.


Some classic applications of P2P are as follows:


- **Word Swap Examples:** In this case, we swap tokens of the original prompt with others, e.g., "a basket with apples." to "a basket with oranges.".

  ![Word Swap Examples](/images/posts/2025-07-03-blog-post/p2p_word_swap.jpg)
  
- **Prompt Refinement Examples:** By extending the initial prompt (adding or removing objects) while preserving layout, we perform local or global editing.

  ![Prompt Refinement Examples](/images/posts/2025-07-03-blog-post/p2p_prompt_refinment.jpg)
  
  
- **Attention Re-weighting Examples:** By reducing or increasing the cross-attention of specific words (marked with an arrow), we control the extent to which it influences the generation.

  ![Attention Re-weighting Examples](/images/posts/2025-07-03-blog-post/p2p_reweighting.jpg)
  
  
- **Text-to-Image Style Transfer:** By adding a style description to the prompt while injecting the source attention maps, we can create various images in the new desired styles that preserve the structure of the original image.



  ![Text-to-Image Style Transfer](/images/posts/2025-07-03-blog-post/p2p_style_transfer.jpg)  
  
  
---

<h1 id="section3.2.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">3.2.1 Core Mechanism: Cross-Attention Replacement/Blending</h1>

Classifier-Free Guidance (CFG), which can only enforce global alignment between the text prompt and the generated image, lacking the ability to make fine-grained, localized edits. Similarly, we cannot directly input the modified prompt into the model. This is because even minor changes to the text prompt often result in generated outputs that are drastically different from the original image, thereby violating the principle that some content from the original image must be preserved.

Prompt-to-Prompt overcomes this by intervening directly in the **cross-attention maps** of the diffusion UNet, where each token‚Äôs attention defines where that concept appears. By synchronizing two sampling processes (source and target prompts) and **replacing or blending** the attention columns of edited tokens (e.g., ‚Äúcat‚Äù‚Üí‚Äúdog‚Äù) while keeping others fixed, it enables localized semantic modifications without retraining or global distortion.


In a cross-attention layer

$$
\mathrm{Attn}(Q,K,V)=\mathrm{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right)V.
$$

where $Q$ come from image space, whereas $K$ and $V$ come from text token space. The **attention map** $A=\mathrm{softmax}(QK^\top/\sqrt{d_k})$ determines **‚Äúwhere each token looks‚Äù** ‚Äî i.e., spatial locations in the image associated with that token.

- Element $A_{i,j} \in A$ measures the strength of association between spatial position $i$ and text token $j$. A high value means that the pixel $i$ strongly attends to word $j$.

- Column $j$ of $A$ represents the spatial footprint or attention map of the $j^{th}$ text token. It shows where in the image this word exerts influence.

Let $A_s$ denoted as source attention map, $A_t$ denoted as source attention map, the fusion result $\tilde{A}^{(l)}$ is 

$$
\tilde{A}^{(l)} = M^{(l)} \odot A_t^{(l)} + (1 - M^{(l)}) \odot A_s^{(l)}
$$

where $M^{(l)}$ is a mask matrix that controls which columns come from the target and which from the source. The modification operations of the attention map vary depending on the differences between the target prompt and the source prompt, with common operations including

<table border="1" cellpadding="10" cellspacing="0" style="border-collapse: collapse; width: 100%; text-align: center;">
  <thead>
    <tr>
      <th style="text-align: center; vertical-align: middle;">Operation Type</th>
      <th style="text-align: center; vertical-align: middle;">Example (Source ‚Üí Target)</th>
      <th style="text-align: center; vertical-align: middle;">Attention Map Modification</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center; vertical-align: middle;"><strong>Token Replacement</strong></td>
      <td style="text-align: center; vertical-align: middle;">‚Äúa <strong>dog</strong> on the grass‚Äù ‚Üí ‚Äúa <strong>cat</strong> on the grass‚Äù</td>
      <td style="text-align: center; vertical-align: middle;">$\tilde{A}_{:,col(dog)} = A_t[:,col(cat)]$<br>Other columns unchanged, copy from $A_s$</td>
    </tr>
    <tr>
      <td style="text-align: center; vertical-align: middle;"><strong>Modifier Insertion</strong></td>
      <td style="text-align: center; vertical-align: middle;">‚Äúa dog‚Äù ‚Üí ‚Äúa <strong>brown</strong> dog‚Äù</td>
      <td style="text-align: center; vertical-align: middle;">$\tilde{A} = [A_s[:,a], A_t[:,brown], A_s[:,dog]]$<br>Re-normalize after insertion</td>
    </tr>
    <tr>
      <td style="text-align: center; vertical-align: middle;"><strong>Degree Word Insertion</strong></td>
      <td style="text-align: center; vertical-align: middle;">‚Äúa cute dog‚Äù ‚Üí ‚Äúa <strong>very</strong> cute dog‚Äù</td>
      <td style="text-align: center; vertical-align: middle;">$\tilde{A}_{:,cute}=(1+\lambda)A_s[:,cute]$<br>$\tilde{A}_{:,very}=\eta A_s[:,cute]$</td>
    </tr>
    <tr>
      <td style="text-align: center; vertical-align: middle;"><strong>Token Deletion</strong></td>
      <td style="text-align: center; vertical-align: middle;">‚Äúa <strong>brown</strong> dog‚Äù ‚Üí ‚Äúa dog‚Äù</td>
      <td style="text-align: center; vertical-align: middle;">Delete ‚Äúbrown‚Äù column: $\tilde{A}=[A_s[:,a],A_s[:,dog]]$</td>
    </tr>
    <tr>
      <td style="text-align: center; vertical-align: middle;"><strong>Attribute Strengthening</strong></td>
      <td style="text-align: center; vertical-align: middle;">‚Äúa cute dog‚Äù ‚Üí ‚Äúa <strong>very</strong> cute dog‚Äù or enhance ‚Äúcute‚Äù effect</td>
      <td style="text-align: center; vertical-align: middle;">$\tilde{A}_{:,cute}=(1+\lambda)A_s[:,cute]$</td>
    </tr>
    <tr>
      <td style="text-align: center; vertical-align: middle;"><strong>Attribute Weakening</strong></td>
      <td style="text-align: center; vertical-align: middle;">‚Äúa <strong>very</strong> cute dog‚Äù ‚Üí ‚Äúa cute dog‚Äù</td>
      <td style="text-align: center; vertical-align: middle;">$\tilde{A}_{:,cute}=(1-\lambda)A_s[:,cute]$</td>
    </tr>
    <tr>
      <td style="text-align: center; vertical-align: middle;"><strong>Token Reordering</strong></td>
      <td style="text-align: center; vertical-align: middle;">‚Äúa red big dog‚Äù ‚Üí ‚Äúa big red dog‚Äù</td>
      <td style="text-align: center; vertical-align: middle;">Reorder columns: $\tilde{A}=[A_s[:,a],A_s[:,big],A_s[:,red],A_s[:,dog]]$</td>
    </tr>
    <tr>
      <td style="text-align: center; vertical-align: middle;"><strong>Style Injection</strong></td>
      <td style="text-align: center; vertical-align: middle;">‚Äúa dog‚Äù ‚Üí ‚Äúa dog in <strong>watercolor</strong> style‚Äù</td>
      <td style="text-align: center; vertical-align: middle;">High-level blending: $\tilde{A}^{(l)}=(1-\alpha)A_s^{(l)}+\alpha A_t^{(l)}$<br>($\alpha‚âà0.2$)</td>
    </tr>
  </tbody>
</table>



---

<h1 id="section3.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.3 Real Image Inversion for Diffusion-based Editing</h1>

Prompt-to-Prompt (P2P) enables semantic-level editing by replacing or interpolating specific columns in the cross-attention maps between a **source prompt** $c_s$ and a **target prompt** $c_t$, while keeping other parts of the scene intact. During sampling, both prompts are processed under the same noise initialization $x_T$, and the attention maps are fused step-by-step to ensure that unchanged tokens preserve their original spatial correspondences.


However, this mechanism assumes the availability of the initial noise $x_T$, which is only known for synthetic images generated by the diffusion model itself. In real-world applications, we start from a **given real image** $x_0$ instead of a noise latent, and thus cannot directly apply P2P.  Consequently, a central research problem has emerged:



> **How can we map a real image $ x_0 $ into the latent space of a pre-trained diffusion model so that it can be faithfully reconstructed and semantically edited like a generated image?**

This challenge‚Äîknown as **diffusion inversion**‚Äîaims to find a noise latent $ x_T $ (and potentially auxiliary parameters) such that the forward diffusion trajectory under the model‚Äôs learned dynamics reproduces the given real image.

---

<h1 id="section3.3.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">2.4.1 Optimization-based Inversion</h1>




---

<h1 style="color: #212121; font-size: 21px; font-weight: bold; ">1Ô∏è‚É£  Null-Text Inversion (NTI)</h1>

NTI [^nti] is the most widely adopted optimization-based approach. It proceeds in two stages:

- **Stage I. Pivotal inversion.** We first construct a stable **reference** (pivot) trajectory $\{x_t^\star\}_{t=0}^{T}$ by **inverting** $x_0$ with a **guidance-neutral** setting (guidance scale $w=1$). Concretely, for DDIM with $\bar\alpha_t=\prod_{s=1}^t(1-\beta_s)$ and $\eta=0$ (deterministic ODE limit), define the denoiser used in inversion as

$$
\widehat\epsilon_t^{\text{inv}} \,=\, \epsilon_\theta\,\big(x_t^\star,\,t,\,c\big) \quad (\text{equivalently CFG with }w{=}1).
$$

One inversion step $t \to t{+}1$ is

$$
\hat x_0^{(t)} = \frac{x_t^\star - \sqrt{1-\bar\alpha_t}\,\widehat\epsilon_t^{\text{inv}}}{\sqrt{\bar\alpha_t}}\quad\Longrightarrow \quad
x_{t+1}^\star  = \sqrt{\bar\alpha_{t+1}}\;\hat x_0^{(t)} \;+\; \sqrt{1-\bar\alpha_{t+1}}\;\widehat\epsilon_t^{\text{inv}}.
$$

Initialization sets $x_0^\star \,\leftarrow\, \text{VAE.encode}(x_0)$. Iterating $t=0!\to!T{-}1$ yields the **pivotal latent** $x_T^\star$ and the full reference path.

**Why $w{=}1$ here?** Large guidance scalars nonlinearly amplify modeling/rounding errors and make the inverse ODE numerically fragile. Using (w{=}1) produces a smooth, model-consistent path that is easier to ‚Äúclose‚Äù in Stage II.

  

2. **Null-text optimization:**
   Re-run the reverse process under the typical guidance scale ( w!=!7.5 ), but treat the *null-text embeddings* ( {\mathcal{O}*t} ) as optimizable variables.
   These embeddings are iteratively updated to minimize the reconstruction loss:
   [
   \mathcal{L} = \sum_t |,x*{t-1}^* - \text{DDIMstep}(x_t; \mathcal{O}_t, c, w)|^2.
   ]
   This effectively learns a per-timestep correction that compensates for the asymmetry between inversion and sampling.

NTI yields high-fidelity reconstructions and enables subsequent P2P editing of real images.
Its limitations lie in optimization cost (several seconds per image) and dependence on gradient tuning.


---

<h1 style="color: #212121; font-size: 21px; font-weight: bold; ">2Ô∏è‚É£  Negative-Prompt Inversion (NPI)</h1>

---

<h1 id="section4" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">4. Personalized Control: Injecting New Concepts into Diffusion Models</h1>


While **guidance-based controllable generation** (e.g., classifier guidance, ControlNet, or adapter-based conditioning) allows diffusion models to *steer* the generation process toward a desired attribute, these methods fundamentally operate within the **semantic prior already encoded** in the pretrained model. In other words, they can *emphasize* or *suppress* existing concepts, but cannot truly **inject new ones**.

However, real-world creative applications‚Äîsuch as personalized avatars, user-specific art styles, product customization, and individualized storytelling‚Äîrequire the model to **learn and remember new entities**: a particular person, a novel object, a unique texture, or even a newly coined fictional concept that never appeared in the original dataset. This new requirement motivates a distinct research direction: **Personalization**.

Personalization extends controllable generation from *"How to guide the model?"* to *"How to teach the model something new?"* It answers the question of how a diffusion model can *absorb new visual or semantic knowledge* from a handful of examples, while preserving its generalization ability and aesthetic quality.





* The **attention map** (A=\mathrm{softmax}(QK^\top/\sqrt{d_k})) determines **‚Äúwhere each token looks‚Äù** ‚Äî i.e., spatial locations in the image associated with that token.
* The **value vectors** (V) carry the **‚Äúwhat‚Äù information** (semantic embedding of the token).

Therefore:

* Replacing **columns of (A)** changes *which region* corresponds to ‚Äúdog‚Äù.
* Changing **(V)** would alter *how all textual features combine globally*, possibly disturbing the overall composition.





---

<h1 id="section5" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">5. Structured Condition Control</h1>


---

<h1 id="section9" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">9. References</h1>

[^cg]: Dhariwal P, Nichol A. Diffusion models beat gans on image synthesis[J]. Advances in neural information processing systems, 2021, 34: 8780-8794.


[^cfg]: Ho J, Salimans T. Classifier-free diffusion guidance[J]. arXiv preprint arXiv:2207.12598, 2022.

[^nti]: Mokady, Ron, et al. "Null-text inversion for editing real images using guided diffusion models." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023.

[^p2p]: Hertz, Amir, et al. "Prompt-to-prompt image editing with cross attention control." arXiv preprint arXiv:2208.01626 (2022).

[^npi]: Miyake D, Iohara A, Saito Y, et al. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models[C]//2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). IEEE, 2025: 2063-2072.

[^edict]: Wallace B, Gokul A, Naik N. Edict: Exact diffusion inversion via coupled transformations[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 22532-22541.

[^Samuel]: Samuel D, Meiri B, Maron H, et al. Lightning-fast image inversion and editing for text-to-image diffusion models[J]. arXiv preprint arXiv:2312.12540, 2023.