---
title: 'Controlled Generation with Diffusion Models'
date: 2025-07-03
excerpt: "Over the past few years, controllable generation has become the central theme in the evolution of diffusion models. What began as a purely stochastic process for unconditional image synthesis has transformed into a programmable system capable of following text prompts, sketches, poses, depth maps, and even personalized identities. From Classifier-Free Guidance that amplifies conditional gradients, to Textual Inversion and DreamBooth that learn new concepts, to LoRA and ControlNet that extend controllability through lightweight adapters‚Äîeach technique represents a different way of injecting intention into noise. This article traces the unifying logic behind these seemingly independent methods, revealing that all controllable diffusion approaches ultimately share a common goal: to reshape the diffusion trajectory so that generation obeys human-defined constraints while preserving creativity and diversity."
permalink: /posts/2025/07/controlled-generation/
tags:
  - cool posts
  - category1
  - category2
---


<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>üìö Table of Contents</strong></span>
  </summary>
  <ul style="margin:0; padding-left:1.1rem;">
	<li><a href="#section1">1. Introduction: From Unconditional to Controllable Generation</a></li>
	<li><a href="#section2">2. Introduction: From Unconditional to Controllable Generation</a>
		<ul>
			<li><a href="#section2.1">2.1 Latent Diffusion</a></li>
			<li><a href="#section2.2">2.2 Multi-Stage Cascades</a>
				<ul>
					<li><a href="#section2.2.1">2.2.1 Resolution-Based Cascades</a></li>
					<li><a href="#section2.2.2">2.2.2 SNR-Based Cascades</a></li>
					<li><a href="#section2.2.3">2.2.3 Conclusion</a></li>
				</ul>
			</li>
			<li><a href="#section2.3">2.3 Multi-Resolution Strateries</a>
				<ul>
					<li><a href="#section2.3.1">2.3.1 Motivation for hierarchical design</a></li>
					<li><a href="#section2.3.2">2.3.2 U-Net: inherently multi-resolution</a></li>
					<li><a href="#section2.3.3">2.3.3 DiT: single-scale by default, multi-resolution as an extension</a></li>
				</ul>
			</li>
		</ul>
	</li>	
    <li><a href="#section5">5. References</a></li>
  </ul>
</details>



Diffusion models have emerged as a dominant paradigm for high-fidelity image synthesis, yet their native formulation remains largely **uncontrolled**, relying on stochastic sampling from an unconditional generative process. As real-world applications increasingly demand specificity, consistency, and interpretability, the field has rapidly evolved toward **controllable diffusion generation**‚Äîa unified framework for steering the sampling trajectory according to external conditions, semantic concepts, or structural priors.

This post provides a comprehensive analysis of controllable generation from a unified theoretical and architectural perspective. We categorize existing techniques along four orthogonal axes‚Äî**control source**, **injection stage**, **mechanism**, and **granularity**‚Äîrevealing how diverse approaches such as *Classifier Guidance*, *Classifier-Free Guidance*, *Textual Inversion*, *DreamBooth*, *LoRA*, *ControlNet*, and *T2I-Adapter* can be interpreted as complementary implementations of conditional constraints within the diffusion process. We further distinguish *guidance mechanisms* (which bias the generative dynamics) from *control architectures* (which encode conditional information), clarifying their distinct yet synergistic roles.
Finally, we discuss emerging trends including *compositional and multimodal control*, *latent-space manipulation*, and *lightweight universal adapters*, outlining a path toward programmable and explainable generative systems. Our synthesis establishes a conceptual foundation that connects seemingly disparate controllability techniques under a single mathematical and practical framework.




---

<h1 id="section1" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">1. Introduction</h1>



In the previous articles, we have explored various aspects of knowledge related to diffusion models, covering their theoretical foundations, core algorithms, and architectural designs, as well as the training stability and efficiency. However, all the generation methods we discussed earlier are unconditional generation; in practical application scenarios, **controllable conditional generation** is the key link for diffusion models to exert their core value.

In controllable generation, the **goal** is not merely to generate diverse samples, but to steer the generative process toward satisfying a desired set of conditions, semantics, or user intentions. Control may originate from various modalities (text, image, audio, video, depth, segmentation, pose) and may be applied at different stages (during training, inference, or within latent manipulation). Achieving precise, consistent, and interpretable control, while preserving sample quality and diversity, has become one of the defining challenges in modern generative AI.


---

<h1 id="section2" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">2. Steering the Diffusion Process with Guidance</h1>

Diffusion models were originally designed as **unconditional** generators that progressively denoise Gaussian noise to recover clean data samples. In many real applications, however, users desire to steer the generation toward specific conditions‚Äîtext prompts, categories, objects, or visual attributes‚Äîwithout retraining the model. **Inference-time guidance** methods accomplish exactly this: they introduce an external ‚Äúforce field‚Äù that modifies the sampling trajectory so that the generated sample follows the direction favored by a given condition $c$. The model itself remains fixed; only the sampling dynamics are altered.


---

<h1 id="section2.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.1 A Unified Principle for Inference-Time Guidance</h1>

**Goal.** We want to steer a *fixed* diffusion model at **inference time**‚Äîwithout retraining‚Äîso that the generated sample (x_0) better matches a user condition (c) (text, class label, image cue, etc.). All methods in this family can be seen as injecting a **condition-dependent force field** into the reverse denoising dynamics.

