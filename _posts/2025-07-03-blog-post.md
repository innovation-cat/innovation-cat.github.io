---
title: 'Controlled Generation with Diffusion Models'
date: 2025-07-03
excerpt: "Over the past few years, controllable generation has become the central theme in the evolution of diffusion models. What began as a purely stochastic process for unconditional image synthesis has transformed into a programmable system capable of following text prompts, sketches, poses, depth maps, and even personalized identities. From Classifier-Free Guidance that amplifies conditional gradients, to Textual Inversion and DreamBooth that learn new concepts, to LoRA and ControlNet that extend controllability through lightweight adapters‚Äîeach technique represents a different way of injecting intention into noise. This article traces the unifying logic behind these seemingly independent methods, revealing that all controllable diffusion approaches ultimately share a common goal: to reshape the diffusion trajectory so that generation obeys human-defined constraints while preserving creativity and diversity."
permalink: /posts/2025/07/controlled-generation/
tags:
  - cool posts
  - category1
  - category2
---


<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>üìö Table of Contents</strong></span>
  </summary>
  <ul style="margin:0; padding-left:1.1rem;">
	<li><a href="#section1">1. Introduction: From Unconditional to Controllable Generation</a></li>
	<li><a href="#section2">2. Steering the Diffusion Process with Guidance</a>
		<ul>
			<li><a href="#section2.1">2.1 A Unified Principle for Guidance</a></li>
			<li><a href="#section2.2">2.2 Classifier Guidance</a>
				<ul>
					<li><a href="#section2.2.1">2.2.1 Resolution-Based Cascades</a></li>
					<li><a href="#section2.2.2">2.2.2 SNR-Based Cascades</a></li>
					<li><a href="#section2.2.3">2.2.3 Conclusion</a></li>
				</ul>
			</li>
			<li><a href="#section2.3">2.3 Multi-Resolution Strateries</a>
				<ul>
					<li><a href="#section2.3.1">2.3.1 Motivation for hierarchical design</a></li>
					<li><a href="#section2.3.2">2.3.2 U-Net: inherently multi-resolution</a></li>
					<li><a href="#section2.3.3">2.3.3 DiT: single-scale by default, multi-resolution as an extension</a></li>
				</ul>
			</li>
		</ul>
	</li>	
    <li><a href="#section5">5. References</a></li>
  </ul>
</details>



Diffusion models have emerged as a dominant paradigm for high-fidelity image synthesis, yet their native formulation remains largely **uncontrolled**, relying on stochastic sampling from an unconditional generative process. As real-world applications increasingly demand specificity, consistency, and interpretability, the field has rapidly evolved toward **controllable diffusion generation**‚Äîa unified framework for steering the sampling trajectory according to external conditions, semantic concepts, or structural priors.

This post provides a comprehensive analysis of controllable generation from a unified theoretical and architectural perspective. We categorize existing techniques along four orthogonal axes‚Äî**control source**, **injection stage**, **mechanism**, and **granularity**‚Äîrevealing how diverse approaches such as *Classifier Guidance*, *Classifier-Free Guidance*, *Textual Inversion*, *DreamBooth*, *LoRA*, *ControlNet*, and *T2I-Adapter* can be interpreted as complementary implementations of conditional constraints within the diffusion process. We further distinguish *guidance mechanisms* (which bias the generative dynamics) from *control architectures* (which encode conditional information), clarifying their distinct yet synergistic roles.
Finally, we discuss emerging trends including *compositional and multimodal control*, *latent-space manipulation*, and *lightweight universal adapters*, outlining a path toward programmable and explainable generative systems. Our synthesis establishes a conceptual foundation that connects seemingly disparate controllability techniques under a single mathematical and practical framework.




---

<h1 id="section1" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">1. Introduction</h1>



In the previous articles, we have explored various aspects of knowledge related to diffusion models, covering their theoretical foundations, core algorithms, and architectural designs, as well as the training stability and efficiency. However, all the generation methods we discussed earlier are unconditional generation; in practical application scenarios, **controllable conditional generation** is the key link for diffusion models to exert their core value.

In controllable generation, the **goal** is not merely to generate diverse samples, but to steer the generative process toward satisfying a desired set of conditions, semantics, or user intentions. Control may originate from various modalities (text, image, audio, video, depth, segmentation, pose) and may be applied at different stages (during training, inference, or within latent manipulation). Achieving precise, consistent, and interpretable control, while preserving sample quality and diversity, has become one of the defining challenges in modern generative AI.


---

<h1 id="section2" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">2. Steering the Diffusion Process with Guidance</h1>

Diffusion models were originally designed as **unconditional** generators that progressively denoise Gaussian noise to recover clean data samples. In many real applications, however, users desire to steer the generation toward specific conditions‚Äîtext prompts, categories, objects, or visual attributes‚Äîwithout retraining the model. **Inference-time guidance** methods accomplish exactly this: they introduce an external ‚Äúforce field‚Äù that modifies the sampling trajectory so that the generated sample follows the direction favored by a given condition $c$. The model itself remains fixed; only the sampling dynamics are altered.


---

<h1 id="section2.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.1 A Unified Principle for Guidance</h1>

**Goal.** We want to steer a **fixed** diffusion model at **inference time** ‚Äî without retraining ‚Äî so that the generated sample (x_0) better matches a user condition (c) (text, class label, image cue, etc.). All methods in this family can be seen as injecting a **condition-dependent force field** into the reverse denoising dynamics. Recall the reverse-time sampler (SDE or ODE) of the **unconditional** model be

$$
\mathrm{d}x_t =\Big[f(t)x_t - g^2(t)\nabla_{x_t}\log p_t(x_t)\Big]\mathrm{d}t + g(t)\,\mathrm{d}\bar w_t,
$$


  
where $p_t(x_t)$ is the marginal density at time $t$. Replacing the unconditional score by the conditional one yields:

$$
\mathrm{d}x_t =\Big[f(t)x_t - g^2(t)\nabla_{x_t}\log p_t(x_t \mid c)\Big]\mathrm{d}t + g(t)\,\mathrm{d}\bar w_t,
$$

where $c$ is condition signal, by Bayes,


$$
\nabla_{x_t}\log p(x_t \mid c) 
= \nabla_{x_t}\log p(c \mid x_t) \;+\; \nabla_{x_t}\log p(x_t).
$$

Replacing the conditional score and yields:

$$
\mathrm{d}x_t=\Big[f(t)x_t - g^2(t)\underbrace{\nabla_{x_t}\log p_t(x_t)}_{\text{unconditional score}} - g^2(t)\underbrace{\nabla_{x_t}\log p_t(c \mid x_t)}_{\text{classifier gradient}} \Big]\mathrm{d}t  + g(t)\,\mathrm{d}\bar w_t\label{eq:cg}

$$

This equation shows that **conditional generation** can be achieved by **adding a drift term** proportional to $\nabla_{x_t}\log p(c\mid x_t)$‚Äîan external force directing samples toward class $c$. **Different guidance families differ only in how they approximate the guidance direction term.**












---

<h1 id="section2.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.2 Classifier Guidance</h1>


Classifier Guidance [^cg] was the first practical approach to achieve **controllable generation** in diffusion models **without retraining the generative model itself**. It introduces an **external classifier** to steer the diffusion sampling process toward samples consistent with a target condition‚Äîtypically a class label $c$.



Since $p(y\mid x_t)$ is unknown, an auxiliary **noisy-image classifier** $p_\phi(y\mid x_t, t)$ is trained on images corrupted with the same diffusion noise. Its gradient approximates the desired term:

$$
\nabla_{x_t}\log p(c \mid x_t) \;\approx\; \nabla_{x_t}\log p_\phi(c \mid x_t)
$$

This is why it is called classifier guidance. Substituting into \ref{eq:cg} gives the **classifier-guided reverse SDE**:


$$
\mathrm{d}x_t=\Big[f(t)x_t - g^2(t)\nabla_{x_t}\log p_t(x_t) - s(t)\nabla_{x_t}\log p_\phi(c\mid x_t) \Big]\mathrm{d}t  + g(t)\,\mathrm{d}\bar w_t,

$$

where $s(t)$ controls the **guidance strength** (often increasing as noise decreases). The classifier gradient can be efficiently obtained by **backpropagating** through the classifier with respect to its input $x_t$.




<strong style="color: blue;">Summary: </strong> Classifier Guidance marked the first practical demonstration that diffusion trajectories can be steered at inference time by introducing an external gradient field derived from a classifier. Its theoretical contribution was fundamental: it established the now-standard view that conditional generation equals unconditional generation plus a conditional gradient term. However, the approach suffers from several inherent limitations:

- Firstly, it requires training an additional classifier on noisy images for every diffusion timestep, incurs substantial computational cost due to per-step backpropagation.

- Secondly, and most critically, it offers control only over discrete semantic categories rather than fine-grained attributes or spatial structures. 

As a result, its applicability remains narrow and inefficient for complex or continuous conditions. Nevertheless, Classifier Guidance‚Äôs conceptual insight directly inspired subsequent advances such as Classifier-Free Guidance, Attention-based Control, and ControlNet, which internalized or generalized the same principle into far more flexible and efficient conditioning mechanisms.


---

<h1 id="section2.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.3 Classifier-Free Guidance (CFG)</h1>

Classifier-Free Guidance [^cfg] is the most widely used inference-time guidance method in modern diffusion systems (e.g., Stable Diffusion, Imagen, SDXL). It **internalizes** the conditional gradient used by Classifier Guidance, but **removes the external classifier**: during training, the same denoiser learns both **conditional** and **unconditional** predictions. At inference, we combine these two predictions to emulate ascent on the conditional likelihood‚Äîrecovering the unified ‚Äúexternal force‚Äù view while keeping the model frozen.


<strong style="color: blue;">Training setup: </strong> Let $c$ denote conditioning (text, label, etc.). During training, with probability $p_{\text{drop}}$ we **replace $c$ by a special null token** $\varnothing$. The standard denoising loss  is

$$
\min_\theta\;\mathbb{E}_{x_0,c,t,\epsilon}
\Big\|\epsilon - \epsilon_\theta(x_t,\tilde c,t)\Big\|^2,
\quad
\tilde c=\begin{cases}
c & \text{w.p. } 1-p_{\text{drop}},\\
\varnothing & \text{w.p. } p_{\text{drop}}.
\end{cases}
$$

Thus the **same network** learns two behaviors: **Conditional branch**: $\epsilon_\theta(x_t,c,t)$, and  **Unconditional branch**: $\epsilon_\theta(x_t,\varnothing,t)$.

This duality is the key to estimating $\nabla_{x_t}\log p(c\mid x_t)$ **without** a classifier.  By Bayes,

$$
\begin{align}
\nabla_{x_t}\log p(c\mid x_t)
& =\nabla_{x_t}\log p(x_t\mid c)-\nabla_{x_t}\log p(x_t) \\[10pt]
& \approx -\frac{1}{\sigma_t}\Big(\epsilon_\theta(x_t,c,t)-\epsilon_\theta(x_t,\varnothing,t)\Big)
\end{align}
$$

This means CFG replaces the external estimator with an **internal contrast** between the model‚Äôs conditional and unconditional predictions. Plug this approximation into the unified dynamics and absorb constants into $\lambda(t)$. In **discrete sampling** (DDPM/DDIM/ODE), this becomes the familiar **guided denoiser**:

$$
\begin{align}
{\epsilon}_{\text{guide}}
& =\epsilon_\theta(x_t,\varnothing,t) + w(t)\,(\epsilon_\theta(x_t,c,t)\;-\epsilon_\theta(x_t,\varnothing,t)) \\[10pt]
& = w(t)\,(\epsilon_\theta(x_t,c,t) + (1-w(t)) \epsilon_\theta(x_t,\varnothing,t) 
\end{align}
$$

where $w(t)\ge 0$ is the **guidance scale**. Large $w$ increases faithfulness to $c$ (mode-seeking), small $w$ preserves diversity (mode-covering).


<strong style="color: blue;">Summary: </strong> Classifier-Free Guidance (CFG) represents a pivotal evolution of the inference-time guidance paradigm first introduced by Classifier Guidance (CG).

- CFG eliminates the need for a noisy-image classifier, drastically reducing computational and training cost.
- It also generalizes beyond discrete categories, allowing conditions to be continuous, multimodal, or high-dimensional (e.g., text embeddings, style vectors).
- It also generalizes beyond discrete categories, allowing conditions to be continuous, multimodal, or high-dimensional (e.g., text embeddings, style vectors).

For this reason, CFG became the standard backbone of controllable diffusion systems such as Stable Diffusion, Imagen, and SDXL.



---

<h1 id="section3" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">3. Multi-Condition Guidance in Diffusion Models</h1>


In practice, Conditional diffusion models are rarely driven by a single signal in practice. Real deployments combine **heterogeneous condition types (channels)**‚Äîe.g., **Text**, **Layout/Boxes**, **Segmentation**, **Depth/Normals**, **Reference Image/Style**‚Äîto control complementary aspects of content and geometry. Moreover, conditions may be **positive** (encouraging the presence of attributes) or **negative** (suppressing undesired content). These signals can be **statistically independent**, **weakly dependent**, or **directly conflicting** (e.g., a style that clashes with layout geometry).

In previous section, we discuss the classical guidance mechanisms (e.g., classifier guidance (CG) and classifier-free guidance (CFG)) with single condition. Extending them **systematically** to multiple, potentially competing conditions requires a careful treatment of the following questions:

- How to train with multiple channels, this is crucial for the guidance sampling with multiple conditions.

- sampler-agnostic multi-condition guidance at inference, how to combine multi-condition and get the final guidance directions.

- Handling **dependencies and conflicts** across channels, including **priority** and **scheduling**.


<div class="qa-block">

<p><strong>Definition (Channel):</strong> A channel is a condition type with its own encoder and null state (e.g., text tokens and a text-null token; depth image and a "depth-off" code).</p>



<p><strong>Key clarification:</strong>  Multi-condition refers to <strong>multiple condition types/channels</strong>, <strong>not</strong> the count of inputs within a type. One hundred sentences still constitute <strong>one text channel</strong>; one depth map per target image constitutes <strong>one depth channel</strong>. When desired, a single channel (e.g., text) may be factorized into sub-conditions for finer control, but it remains one modality/channel.</p>
  
</div>




---

<h1 id="section3.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.1 Problem Setup and Notation</h1>

Let $x_0\sim p_{\text{data}}$, and $x_t=\alpha(t)x_0+\sigma(t)\epsilon$ be a VP-type forward process. Let channels 

$$\mathcal{K}=\{\text{text},\text{layout},\text{seg},\text{depth},\text{normal},\text{ref},\dots\}$$ 

For channel $k\in\mathcal{K}$, denote its input by $c_k$ as **positive** conditions,   $c^-_k$ as **negative** conditions and its **channel-specific null** by $\varnothing^{(k)}$. Formally, we can uniformly represent the set of multiple conditions as


$$
\mathcal{C}\;=\;\{c_1,\ldots,c_K\}\;\cup\;\{c^-_1,\ldots,c^-_K\}
$$

where $c_k$ are **positive** conditions to be promoted and $c^-_m$ are **negative** conditions to be suppressed. The **unconditional** probability-flow ODE is

$$
\frac{dx_t}{dt}=f_\theta(x_t,t).
$$

Guidance augments the drift by a vector field

$$
\frac{dx_t}{dt}=f_\theta(x_t,t)+\lambda(t),g(x_t,t),
\quad
g(x_t,t)=\nabla_{x_t}\log p(\mathcal{C}\mid x_t),
$$


We adopt $\epsilon$-prediction for exposition; formulas transpose to $x_0$ and $v$-prediction.




---

<h1 id="section3.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.2 Training with Multiple Channels: Per-Channel Dropout</h1>





Training must align the model with all **subsets of channels** it may face at inference. We therefore use **independent per-channel dropout** (a.k.a. classifier-free dropout per channel).

To this end, we define **Sampling masks** For each channel $k$, draw a keep mask

$$
m_k\sim\mathrm{Bernoulli}\,\big(k_k(t)\big),\quad k_k(t)=1-q_k(t),
$$

and form the training input

$$
\tilde c_k=\begin{cases}
c_k & \text{if } m_k=1\ \text{and channel is available},\\
\varnothing_k & \text{otherwise}.
\end{cases}
$$

The loss (for $\epsilon$-prediction) is

$$
\mathcal{L}=\mathbb{E}_{x_0,t,\epsilon}\ \mathbb{E}_{{m_k}}\ \big\|\epsilon-\epsilon_\theta\big(x_t,\,t;\tilde c_1,\ldots,\tilde c_{|\mathcal{K}|}\big)\big\|_2^2.
$$

**Unconditional frequency.** If a target unconditional rate (u\in[0.1,0.2]) is desired,

$$
\Pr[\text{all dropped}]=\prod_{k} q_k(t)\approx u,
$$

e.g., with equal rates (q_k=q) one can set $q=u^{1/\|\mathcal{K}\|}$. Optionally use **SNR/$\sigma$-aware** $q_k(t)$ (higher dropout at high noise).

**Implementation essentials.** Each channel must have its **own null embedding** and, for pixel channels, an explicit *off* flag; unavailable channels are hard-off (no sampling).



There is no "negative condition" in training; negative guidance only exists in inference.







---

<h1 id="section2.4.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">2.4.1 Independent Conditions</h1>


We consider a set of conditions

$$
\mathcal{C}\;=\;\{c_1,\ldots,c_K\}\;\cup\;\{c^-_1,\ldots,c^-_M\}
$$

where $c_k$ are **positive** conditions to be promoted and $c^-_m$ are **negative** conditions to be suppressed. Assume the conditions are **conditionally independent given $x_t$**. Using a first-order sum-of-experts surrogate,

$$
\nabla_{x_t}\log p_t(\mathcal{C} \mid x_t)
\;\approx\;\sum_{k=1}^K\nabla_{x_t}\log p_t(c_k\mid x_t)\;-\;\sum_{m=1}^M\nabla_{x_t}\log p_t(c^-_m\mid x_t).
$$

In **$\epsilon$-prediction** (VP-style forward process), we know the guide direction can be estimated with an internal contrast between the model‚Äôs conditional and unconditional predictions

$$
\nabla_{x_t}\log p_t(c_k\mid x_t)\approx;
-\frac{1}{\sigma(t)}\Big[\epsilon_\theta(x_t,t,c)-\epsilon_\theta(x_t,t,\varnothing)\Big].
$$

Let $$\epsilon_u=\epsilon_\theta(x_t,t,\varnothing)$$ and $$\epsilon_c=\epsilon_\theta(x_t,t,c)$$. We compose a **guided prediction**

$$
\epsilon_{\text{guid}} =\epsilon_u
+\underbrace{\sum_{k=1}^K w_k(t)\big(\epsilon_{c_k}-\epsilon_u\big)}_{\text{positive guidance}}
-\underbrace{\sum_{m=1}^M w^-_m(t)\big(\epsilon_{c^-_m}-\epsilon_u\big)}_{\text{negative guidance}}
$$



---

<h1 id="section2.4.2" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">2.4.2 Dependent or Conflicting Conditions</h1>



---

<h1 id="section9" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">9. References</h1>

[^cg]: Dhariwal P, Nichol A. Diffusion models beat gans on image synthesis[J]. Advances in neural information processing systems, 2021, 34: 8780-8794.


[^cfg]: Ho J, Salimans T. Classifier-free diffusion guidance[J]. arXiv preprint arXiv:2207.12598, 2022.

[^null]: Mokady, Ron, et al. "Null-text inversion for editing real images using guided diffusion models." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023.

[^p2p]: Hertz, Amir, et al. "Prompt-to-prompt image editing with cross attention control." arXiv preprint arXiv:2208.01626 (2022).