---
title: 'Controlled Generation with Diffusion Models'
date: 2025-07-03
excerpt: "Over the past few years, controllable generation has become the central theme in the evolution of diffusion models. What began as a purely stochastic process for unconditional image synthesis has transformed into a programmable system capable of following text prompts, sketches, poses, depth maps, and even personalized identities. From Classifier-Free Guidance that amplifies conditional gradients, to Textual Inversion and DreamBooth that learn new concepts, to LoRA and ControlNet that extend controllability through lightweight adapters‚Äîeach technique represents a different way of injecting intention into noise. This article traces the unifying logic behind these seemingly independent methods, revealing that all controllable diffusion approaches ultimately share a common goal: to reshape the diffusion trajectory so that generation obeys human-defined constraints while preserving creativity and diversity."
permalink: /posts/2025/07/controlled-generation/
tags:
  - cool posts
  - category1
  - category2
---


<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>üìö Table of Contents</strong></span>
  </summary>
  <ul style="margin:0; padding-left:1.1rem;">
	<li><a href="#section1">1. Introduction: From Unconditional to Controllable Generation</a></li>
	<li><a href="#section2">2. Steering the Diffusion Process with Guidance</a>
		<ul>
			<li><a href="#section2.1">2.1 A Unified Principle for Guidance</a></li>
			<li><a href="#section2.2">2.2 Classifier Guidance</a>
				<ul>
					<li><a href="#section2.2.1">2.2.1 Resolution-Based Cascades</a></li>
					<li><a href="#section2.2.2">2.2.2 SNR-Based Cascades</a></li>
					<li><a href="#section2.2.3">2.2.3 Conclusion</a></li>
				</ul>
			</li>
			<li><a href="#section2.3">2.3 Multi-Resolution Strateries</a>
				<ul>
					<li><a href="#section2.3.1">2.3.1 Motivation for hierarchical design</a></li>
					<li><a href="#section2.3.2">2.3.2 U-Net: inherently multi-resolution</a></li>
					<li><a href="#section2.3.3">2.3.3 DiT: single-scale by default, multi-resolution as an extension</a></li>
				</ul>
			</li>
		</ul>
	</li>	
    <li><a href="#section5">5. References</a></li>
  </ul>
</details>



Diffusion models have emerged as a dominant paradigm for high-fidelity image synthesis, yet their native formulation remains largely **uncontrolled**, relying on stochastic sampling from an unconditional generative process. As real-world applications increasingly demand specificity, consistency, and interpretability, the field has rapidly evolved toward **controllable diffusion generation**‚Äîa unified framework for steering the sampling trajectory according to external conditions, semantic concepts, or structural priors.

This post provides a comprehensive analysis of controllable generation from a unified theoretical and architectural perspective. We categorize existing techniques along four orthogonal axes‚Äî**control source**, **injection stage**, **mechanism**, and **granularity**‚Äîrevealing how diverse approaches such as *Classifier Guidance*, *Classifier-Free Guidance*, *Textual Inversion*, *DreamBooth*, *LoRA*, *ControlNet*, and *T2I-Adapter* can be interpreted as complementary implementations of conditional constraints within the diffusion process. We further distinguish *guidance mechanisms* (which bias the generative dynamics) from *control architectures* (which encode conditional information), clarifying their distinct yet synergistic roles.
Finally, we discuss emerging trends including *compositional and multimodal control*, *latent-space manipulation*, and *lightweight universal adapters*, outlining a path toward programmable and explainable generative systems. Our synthesis establishes a conceptual foundation that connects seemingly disparate controllability techniques under a single mathematical and practical framework.




---

<h1 id="section1" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">1. Introduction</h1>



In the previous articles, we have explored various aspects of knowledge related to diffusion models, covering their theoretical foundations, core algorithms, and architectural designs, as well as the training stability and efficiency. However, all the generation methods we discussed earlier are unconditional generation; in practical application scenarios, **controllable conditional generation** is the key link for diffusion models to exert their core value.

In controllable generation, the **goal** is not merely to generate diverse samples, but to steer the generative process toward satisfying a desired set of conditions, semantics, or user intentions. Control may originate from various modalities (text, image, audio, video, depth, segmentation, pose) and may be applied at different stages (during training, inference, or within latent manipulation). Achieving precise, consistent, and interpretable control, while preserving sample quality and diversity, has become one of the defining challenges in modern generative AI.


---

<h1 id="section2" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">2. Steering the Diffusion Process with Guidance</h1>

Diffusion models were originally designed as **unconditional** generators that progressively denoise Gaussian noise to recover clean data samples. In many real applications, however, users desire to steer the generation toward specific conditions‚Äîtext prompts, categories, objects, or visual attributes‚Äîwithout retraining the model. **Inference-time guidance** methods accomplish exactly this: they introduce an external ‚Äúforce field‚Äù that modifies the sampling trajectory so that the generated sample follows the direction favored by a given condition $c$. The model itself remains fixed; only the sampling dynamics are altered.


---

<h1 id="section2.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.1 A Unified Principle for Guidance</h1>

**Goal.** We want to steer a *fixed* diffusion model at **inference time**‚Äîwithout retraining‚Äîso that the generated sample (x_0) better matches a user condition (c) (text, class label, image cue, etc.). All methods in this family can be seen as injecting a **condition-dependent force field** into the reverse denoising dynamics.

Let the reverse-time sampler (SDE or ODE) of the **unconditional** model be

$$
\frac{d x_t}{dt}\;=\; f_\theta(x_t,t)
$$

Inference-time guidance augments this with a condition-dependent drift:

$$
\frac{d x_t}{dt}\;=\; f_\theta(x_t,t)\;+\;\lambda(t)\,{\hat g}(x_t, c)
$$

where (\hat{g}(x_t,c)) is an approximate gradient that enforces the conditional preference, and $\lambda(t)$ controls its strength.


---

<h1 id="section2.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.2 Classifier Guidance</h1>


Classifier Guidance [^cg] was the first practical approach to achieve **controllable generation** in diffusion models **without retraining the generative model itself**. It introduces an **external classifier** to steer the diffusion sampling process toward samples consistent with a target condition‚Äîtypically a class label (c).


The essence of conditional generation is to modify the probability density $p(x)$ of unconditional generation into the conditional probability density $p(x \mid c)$ with condition .

$$
\begin{align}
p(x \mid c) & = \frac{p(x) \cdot p(c\mid x)}{p(c)} \\[10pt] 
\Longrightarrow \qquad \log p(x \mid c) & = \log p(x) + \log  p(c\mid x) - \log p(c)  \\[10pt]
\Longrightarrow \qquad \nabla_{x}\log p(x \mid c) & = \underbrace{ \nabla_{x}\log p(x)}_{\text{unconditional score}} + \underbrace{\nabla_{x}\log p(c\mid x)}_{\text{classifier gradient}}
\end{align}
$$

The first term is the unconditional score (already learned by the diffusion model), and the second term is the **guidance gradient** that attracts the sample toward the conditional manifold.


Consider the standard unconditional reverse-time SDE for the  diffusion process:

$$
\mathrm{d}x_t =\Big[f(t)x_t - g^2(t)\nabla_{x_t}\log p_t(x_t)\Big]\mathrm{d}t + g(t)\,\mathrm{d}\bar w_t,
$$
  
where $p_t(x_t)$ is the marginal density at time (t). Replacing the unconditional score by the conditional one yields:

$$
\mathrm{d}x_t=\Big[f(t)x_t - g^2(t)\nabla_{x_t}\log p_t(x_t) - g^2(t)\nabla_{x_t}\log p_t(c|x_t) \Big]\mathrm{d}t  + g(t)\,\mathrm{d}\bar w_t\label{eq:cg}

$$

This equation shows that **conditional generation** can be achieved by **adding a drift term** proportional to $\nabla_{x_t}\log p(c\mid x_t)$‚Äîan external force directing samples toward class $c$.


---

<h1 id="section2.2.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.2.1 Estimating the Conditional Gradient via a Noisy Classifier</h1>


Since $p(y\mid x_t)$ is unknown, an auxiliary **noisy-image classifier** $p_\phi(y\mid x_t, t)$ is trained on images corrupted with the same diffusion noise. Its gradient approximates the desired term:

$$
\nabla_{x_t}\log p(c \mid x_t) \;\approx\; \nabla_{x_t}\log p_\phi(c \mid x_t)
$$

This is why it is called classifier guidance. Substituting into \ref{eq:cg} gives the **classifier-guided reverse SDE**:


$$
\mathrm{d}x_t=\Big[f(t)x_t - g^2(t)\nabla_{x_t}\log p_t(x_t) - s(t)\nabla_{x_t}\log p_\phi(c\mid x_t) \Big]\mathrm{d}t  + g(t)\,\mathrm{d}\bar w_t,

$$

where $s(t)$ controls the **guidance strength** (often increasing as noise decreases).

---

<h1 id="section2.2.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.2.2 Computing the Classifier Gradient</h1>

Let the classifier output logits $\ell(x_t)\in\mathbb{R}^K$ for $K$ classes. 

$$
\ell(x_t) = [\ell_1(x_t), \ell_2(x_t), \dots, \ell_K(x_t)]
$$

Then:

$$
p_\phi(c \mid x_t)=\frac{e^{\ell_c(x_t)}}{\sum_k e^{\ell_k(x_t)}}
$$

The desired gradient is:

$$
\begin{align}
\nabla_{x_t}\log p_\phi(c\mid x_t) & = \nabla_{x_t}\ell_y(x_t) - \nabla_{x_t} \log \left(\sum_k e^{\ell_k(x_t)} \right) \\[10pt]
& = \nabla_{x_t}\ell_y(x_t) - \frac{1}{\sum_k e^{\ell_k(x_t)}} {\sum_m \left( e^{\ell_m(x_t)} \nabla_{x_t}\ell_m(x_t) \right)} \\[10pt]
& =  \nabla_{x_t}\ell_y(x_t)  - \sum_m \left({\frac{e^{\ell_m(x_t)}}{\sum_k e^{\ell_k(x_t)}}} \nabla_{x_t}\ell_m(x_t) \right) \\[10pt]
& =  \nabla_{x_t}\ell_y(x_t)  - \sum_m \left(p_\phi(m \mid x_t) \nabla_{x_t}\ell_m(x_t) \right)
\end{align}
$$


  This can be efficiently obtained by **backpropagating** through the classifier with respect to its input $x_t$.



---

<h1 id="section2.2.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.2.3 Summary</h1>

Classifier Guidance marked the first practical demonstration that diffusion trajectories can be steered at inference time by introducing an external gradient field derived from a classifier. Its theoretical contribution was fundamental: it established the now-standard view that conditional generation equals unconditional generation plus a conditional gradient term. However, the approach suffers from several inherent limitations:

- Firstly, it requires training an additional classifier on noisy images for every diffusion timestep, incurs substantial computational cost due to per-step backpropagation.

- Secondly, and most critically, it offers control only over discrete semantic categories rather than fine-grained attributes or spatial structures. 

As a result, its applicability remains narrow and inefficient for complex or continuous conditions. Nevertheless, Classifier Guidance‚Äôs conceptual insight directly inspired subsequent advances such as Classifier-Free Guidance, Attention-based Control, and ControlNet, which internalized or generalized the same principle into far more flexible and efficient conditioning mechanisms.


---

<h1 id="section2.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.3 Classifier-Free Guidance (CFG)</h1>

Classifier-Free Guidance [^cfg] is the most widely used inference-time guidance method in modern diffusion systems (e.g., Stable Diffusion, Imagen, SDXL). It **internalizes** the conditional gradient used by Classifier Guidance, but **removes the external classifier**: during training, the same denoiser learns both **conditional** and **unconditional** predictions. At inference, we combine these two predictions to emulate ascent on the conditional likelihood‚Äîrecovering the unified ‚Äúexternal force‚Äù view while keeping the model frozen.

---

<h1 id="section2.3.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.3.1 Training setup</h1>

Let $c$ denote conditioning (text, label, etc.). During training, with probability $p_{\text{drop}}$ we **replace $c$ by a special null token** $\varnothing$. The standard denoising loss  is

$$
\min_\theta;\mathbb{E}_{x_0,c,t,\epsilon}
\Big|\epsilon - \epsilon_\theta(x_t,\tilde c,t)\Big|^2,
\quad
\tilde c=\begin{cases}
c & \text{w.p. } 1-p_{\text{drop}},\
\varnothing & \text{w.p. } p_{\text{drop}}.
\end{cases}
$$

Thus the **same network** learns two behaviors:

- **Conditional branch**: $\epsilon_\theta(x_t,c,t)$
- **Unconditional branch**: $\epsilon_\theta(x_t,\varnothing,t)$

This duality is the key to estimating $\nabla_{x_t}\log p(c\mid x_t)$ **without** a classifier.






---

<h1 id="section9" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">9. References</h1>

[^cg]: Dhariwal P, Nichol A. Diffusion models beat gans on image synthesis[J]. Advances in neural information processing systems, 2021, 34: 8780-8794.


[^cfg]: Ho J, Salimans T. Classifier-free diffusion guidance[J]. arXiv preprint arXiv:2207.12598, 2022.