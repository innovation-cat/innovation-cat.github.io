---
title: 'The Consistency Family: From Discrete-Time Constraints to Continuous-Time Flows'
excerpt: "Consistency models (CMs) have recently emerged as a powerful paradigm for accelerating diffusion sampling by directly learning mappings that preserve consistency across noisy representations of the same data. This paper provides a comprehensive study of the Consistency Family, tracing its evolution from discrete consistency models to continuous-time formulations and trajectory-based extensions. We begin by revisiting the foundational motivation behind CMs and systematically derive their discrete and continuous objectives under both consistency distillation and consistency training paradigms."
date: 2025-04-15
permalink: /posts/2025/04/consistency-model/
tags:
  - Diffusion Model
  - Consistency Model
  - Flow
  - Sampling
  - Stability
---


<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>ðŸ“š Table of Contents</strong></span>
  </summary>
  <ul style="margin:0; padding-left:1.1rem;">
	<li><a href="#section1">1. Why Architecture Matters for Stability</a>
		<ul>
			<li><a href="#section1.1">1.1 Gradient Flow, Conditioning, and Stability</a></li>
			<li><a href="#section1.2">1.2 Balancing Capacity vs. Robustness</a></li>
			<li><a href="#section1.3">1.3 Architectureâ€“Noise Schedule Coupling</a></li>
		</ul>
	</li>
	<li><a href="#section2">2. Evolution of Diffusion Architectures</a>
		<ul>
			<li><a href="#section2.1">2.1 Classical U-Net Foundations</a></li>
			<li><a href="#section2.2">2.2 ADM Improvements (Attention, Class Conditioning)</a></li>
			<li><a href="#section2.3">2.3 Latent U-Net (Stable Diffusion, SDXL)</a></li>
			<li><a href="#section2.4">2.4 Transformer-based Designs (DiT, MMDiT-X, Hybrid Models)</a></li>
			<li><a href="#section2.5">2.5 Extensions to Video and 3D Diffusion (Video U-Net, Gaussian Splatting)</a></li>
			<li><a href="#section2.6">2.6 Lightweight & Memory-efficient Designs (MobileDiffusion, LightDiffusion)</a></li>
		</ul>
	</li>	
	<li><a href="#section3">3. Stability-Oriented Architectural Designs</a>
      <ul>
        <li><a href="#section3.1">3.1 Architectural Philosophies: U-Net vs. DiT</a></li>
		  <ul>
            <li><a href="#section3.1.1">3.1.1 U-Net Macro Topology</a></li>
            <li><a href="#section3.1.2">3.1.2 DiT Macro Topology</a></li>
            <li><a href="#section3.1.3">3.1.3 Summary of Divergence</a></li>
          </ul>
        <li><a href="#section3.2">3.2 Stabilization in U-Net Architectures</a>
		  <ul>
            <li><a href="#section3.2.1">3.2.1 The Control System: Conditioning via AdaGN</a></li>
            <li><a href="#section3.2.2">3.2.2 The Signal Pathways: Skip Connections and Residual Innovations</a></li>
          </ul>
		</li>
        <li><a href="#section3.3">3.3 Stabilization in DiT Architectures</a>
		  <ul>
            <li><a href="#section3.3.1">3.3.1 The Control System: Conditioning via AdaLN</a></li>
            <li><a href="#section3.3.2">3.3.2 The Signal Pathways: Enabling Deep Stacks in DiT</a></li>
			<li><a href="#section3.3.3">3.3.3 Component Stability: Taming the Attention Mechanism</a></li>
          </ul>
		</li>
        <li><a href="#section3.4">3.4 Cross-Paradigm Stabilization Strategies</a>
          <ul>
            <li><a href="#section3.4.1">3.4.1 Universal Training Harness</a></li>
            <li><a href="#section3.4.2">3.4.2 General Architectural Practices</a></li>
            <li><a href="#section3.4.3">3.4.3 Numerical and Training-Free Stability</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#section4">4. Efficiency-Oriented Architectural Designs</a>
      <ul>
        <li><a href="#section4.1">4.1 Token Compression and Merging for Efficiency & Stability</a></li>
        <li><a href="#section4.2">4.2 Automated Architecture Search (DiffusionNAG)</a></li>
        <li><a href="#section4.3">4.3 Multi-resolution Networks with Time-dependent Norms</a></li>
        <li><a href="#section4.4">4.4 Parallel vs. Sequential Transformer Architectures</a></li>
        <li><a href="#section4.5">4.5 Parameter-efficient Modules: LoRA, DoRA, T-Fixup</a></li>
      </ul>
    </li>
    <li><a href="#section5">5. Multi-modality & Generalization-Oriented Designs</a>
      <ul>
        <li><a href="#section5.1">5.1 Multi-modal Token Conditioning (Cross-Attention, LoRA, Deep Fusion)</a></li>
        <li><a href="#section5.2">5.2 Diffusion Transformers (DiT) and Scaling Laws</a></li>
        <li><a href="#section5.3">5.3 Decoupled Design (DDT: Encoderâ€“Decoder Separation)</a></li>
        <li><a href="#section5.4">5.4 State Space Models (S4, Mamba) as Alternatives to Transformers</a></li>
      </ul>
    </li>
    <li><a href="#section6">6. Architectureâ€“Schedule Co-Design (Integrative Paradigm)</a>
      <ul>
        <li><a href="#section6.1">6.1 Preconditioning with AdaLN-Zero</a></li>
        <li><a href="#section6.2">6.2 Hybridization of Architectural and Regularization Strategies</a></li>
        <li><a href="#section6.3">6.3 Structurally Balanced Design for Training Stability</a></li>
        <li><a href="#section6.4">6.4 Architectureâ€“Noise Schedule Co-design</a></li>
      </ul>
    </li>
    <li><a href="#section7">7. Practical Takeaways</a>
      <ul>
        <li><a href="#section7.1">7.1 Stability-oriented Designs</a></li>
        <li><a href="#section7.2">7.2 Efficiency-oriented Designs</a></li>
        <li><a href="#section7.3">7.3 Generalization-oriented Designs</a></li>
        <li><a href="#section7.4">7.4 Fidelity & Accuracy as Evaluation Metrics</a></li>
      </ul>
    </li>
    <li><a href="#section8">8. Conclusion</a></li>
    <li><a href="#section9">9. References</a></li>
  </ul>
</details>



Consistency models (CMs) have recently emerged as a powerful paradigm for accelerating diffusion sampling by directly learning mappings that preserve consistency across noisy representations of the same data. This paper provides a comprehensive study of the **Consistency Family**, tracing its evolution from **discrete consistency models** to **continuous-time formulations**. We begin by revisiting the foundational motivation behind CMs and systematically derive their discrete and continuous objectives under both **consistency distillation** and **consistency training** paradigms. Building on these foundations, we analyze recent advances that enhance the **stability and scalability** of CM trainingâ€”such as normalization strategies, Jacobian regularization, and variance-preserving loss formulationsâ€”highlighting insights.




---

<h1 id="section1" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">1. Introduction: Motivation and Problem Definition </h1>


Let $\Phi_{t\to s}$ denote the exact PF-ODE flow that maps a state at time $t$ to time $s\le t$. 

$$
\Phi_{t\to s}(x_t) \,=\, x_s \,\quad\, \forall\; 0\le s\le t\le T.
$$

The **endpoint map** is $\Phi_{t\to 0}(\,\cdot\,)$. Consistency demands that flowing to any intermediate $s$ and then to $0$ equals flowing directly to $0$:

$$
\Phi_{s\to 0}\!\big(\Phi_{t\to s}(x_t)\big)\;=\;\Phi_{t\to 0}(x_t)\qquad \forall\; 0\le s\le t\le T.
$$

A **consistency function** $f_\theta(x_t, t)$ is trained to approximate this endpoint map:

$$
f_\theta(x_t, t)\;\approx\;\Phi_{t\to 0}(x_t),
$$

and should satisfy the **consistency identity**, as shown in the following figure.

$$
f_\theta\big(x_t, t\big)\;\approx\; f_\theta\big(\Phi_{t\to s}(x_t),\, s\big)\qquad \text{(self-consistency across times)}.
$$

![Consistency Model](/images/posts/2025-05-20-blog-post/CM.jpg)

The objective of CM is to minimize the consistency loss between different noise levels,

$$
\mathcal L_{\text{consistency loss}} \,=\, \mathbb E_{t, s} \big[d(f_\theta(x_t,t),\;f_\theta(x_s,s))\big],\quad\,\forall 0<s<t.
$$

where $d$ is a distance metric, such as L2 norm, L1 norm,  or LPIPS. However, the model could trivially drive the loss to zero by outputting a **constant image** for all inputs and time steps,

$$
f_\theta(x_t,t)\equiv c,\quad \forall,x_t,t,
$$

since both sides of the loss would be identical.
Such a degenerate solution makes the model independent of the input and destroys its generative ability. In practice, We also add an **endpoint boundary** constraint:

$$
f_\theta(x_\epsilon, t_{\epsilon})\;\approx\;x_\epsilon
$$


where $\epsilon$ is a time step that approachs to 0. This boundary constraint plays a crucial role in preventing the **collapse problem** during training. we **anchor** the modelâ€™s behavior at the noise-free endpoint: it must reproduce the original clean image when $t{=}0$. This boundary condition breaks the trivial constant solution, forcing the network to learn a **non-trivial mapping** that depends on both the noisy input $x_t$ and the noise level $t$. Intuitively, the constraint defines a fixed point on the data manifold that every predicted trajectory must converge to, ensuring that the model remains meaningful and that its outputs at all noise levels are self-consistency while still faithful to the data distribution.

The core issue of CM lies in how to find $x_{t_{n-1}}$ when given $x_{t_n}$, since $x_{t_n}$ and $x_{t_{n-1}}$ are on the same ODE trajectory, theoretically, given $x_t$, $x_s$ can be obtained by solving the ODE

$$
x_{t_{n-1}} = x_{t_n} + \int_{t_n}^{t_{n-1}} (-t s_{\theta}(x_t, t)) dt
$$

Here, we use EDM-Style ODE form: $$f(t)=0, g(t)=\sqrt{2t}$$. Depending on the different ways of solving the integrals, there can be two different training methods: consistency distillation and consistency training.


---

<h1 id="section2" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">2. From Discrete to Continuous Consistency Models</h1>


The consistency model (CM) family can be formulated either in **discrete** or **continuous** time. Both variants share the same fundamental objective â€” to ensure that predictions made from noisy samples at different time steps remain **mutually consistent** along the diffusion trajectory.

This section presents the discrete formulation and then derives its continuous-time limit, showing that the two are mathematically equivalent.

Given a clean data sample $x_0$ and a forward noising process:

$$
x_t = s(t)\,x_0 + \sigma(t)\,\epsilon, \qquad \epsilon \sim \mathcal N(0, I),
$$

we define a time-dependent consistency model $f_\theta(x_t, t)$ that predicts a **consistent reconstruction** across noise levels $t \in [0, 1]$. Intuitively, for the same underlying $x_0$, predictions made from different noisy realizations should coincide:

$$
f_\theta(x_{t_n}, t_n) \approx f_\theta(x_{t_{n-1}}, t_{n-1}).
$$

This constraint enforces **pathwise invariance**, i.e., $f_\theta(x_t, t)$ remains constant along the forward diffusion trajectory of $x_t$.

---

<h1 id="section2.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.1 Discrete-Time Consistency Model</h1> 

Consider a pre-defined sequence of noise levels

$$0 = t_0 < t_1 < \cdots < t_N = 1$$

The discrete-time consistency loss penalizes deviations in model predictions between adjacent time steps:

$$
\mathcal{L}_{\text{disc}} = \mathbb{E}_{x_0, \epsilon, n}
\Big[\| f_\theta(x_{t_n}, t_n) - f_{\theta^-}(x_{t_{n-1}}, t_{n-1}) |*2^2\Big],
$$

where $$f_{\theta^-}$$ denotes an **exponential moving average (EMA)** copy of the model, providing a temporally stable target.

This discrete formulation encourages the model to produce consistent outputs across pairs of noisy inputs derived from the same clean sample $x_0$. Practically, it acts as a **self-distillation mechanism**, where (f_\theta) learns to match its own EMA prediction at a slightly smaller noise level.

---


<h1 id="section9" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">9. References</h1>

# <a id="section6"></a>

[^unet]: Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation[C]//International Conference on Medical image computing and computer-assisted intervention. Cham: Springer international publishing, 2015: 234-241.

[^adm]: Dhariwal P, Nichol A. Diffusion models beat gans on image synthesis[J]. Advances in neural information processing systems, 2021, 34: 8780-8794.

[^biggan]: Brock A, Donahue J, Simonyan K. Large scale GAN training for high fidelity natural image synthesis[J]. arXiv preprint arXiv:1809.11096, 2018.

[^film]: Perez E, Strub F, De Vries H, et al. Film: Visual reasoning with a general conditioning layer[C]//Proceedings of the AAAI conference on artificial intelligence. 2018, 32(1).


[^sd]: Rombach R, Blattmann A, Lorenz D, et al. High-resolution image synthesis with latent diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 10684-10695.

[^sdxl]: Podell D, English Z, Lacey K, et al. Sdxl: Improving latent diffusion models for high-resolution image synthesis[J]. arXiv preprint arXiv:2307.01952, 2023.

[^dit]: Peebles W, Xie S. Scalable diffusion models with transformers[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2023: 4195-4205.

[^transformer]: Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.


[^bn]: Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[C]//International conference on machine learning. pmlr, 2015: 448-456.

[^ln]: Ba J L, Kiros J R, Hinton G E. Layer normalization[J]. arXiv preprint arXiv:1607.06450, 2016.

[^in]: Ulyanov D, Vedaldi A, Lempitsky V. Instance normalization: The missing ingredient for fast stylization[J]. arXiv preprint arXiv:1607.08022, 2016.

[^gn]: Wu Y, He K. Group normalization[C]//Proceedings of the European conference on computer vision (ECCV). 2018: 3-19.

[^swiglu]: Shazeer N. Glu variants improve transformer[J]. arXiv preprint arXiv:2002.05202, 2020.

[^stylegan2]: Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., & Aila, T. (2020). Analyzing and Improving the Image Quality of StyleGAN. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).

[^iedm]: Karras T, Aittala M, Lehtinen J, et al. Analyzing and improving the training dynamics of diffusion models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 24174-24184.

[^edm]: Karras T, Aittala M, Aila T, et al. Elucidating the design space of diffusion-based generative models[J]. Advances in neural information processing systems, 2022, 35: 26565-26577.

[^Kingma]: Kingma D, Gao R. Understanding diffusion objectives as the elbo with simple data augmentation[J]. Advances in Neural Information Processing Systems, 2023, 36: 65484-65516.

[^Salimans]: Salimans T, Ho J. Progressive distillation for fast sampling of diffusion models[J]. arXiv preprint arXiv:2202.00512, 2022.

[^p2]: Choi J, Lee J, Shin C, et al. Perception prioritized training of diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 11472-11481.

[^min_snr]: Hang T, Gu S, Li C, et al. Efficient diffusion training via min-snr weighting strategy[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2023: 7441-7451.

[^max_snr]: Salimans T, Ho J. Progressive distillation for fast sampling of diffusion models[J]. arXiv preprint arXiv:2202.00512, 2022.

[^snr_based]: Kingma D, Gao R. Understanding diffusion objectives as the elbo with simple data augmentation[J]. Advances in Neural Information Processing Systems, 2023, 36: 65484-65516.

[^ddpm]: Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[J]. Advances in neural information processing systems, 2020, 33: 6840-6851.

[^iddpm]: Nichol A Q, Dhariwal P. Improved denoising diffusion probabilistic models[C]//International conference on machine learning. PMLR, 2021: 8162-8171.

[^ZTSNR]: Lin S, Liu B, Li J, et al. Common diffusion noise schedules and sample steps are flawed[C]//Proceedings of the IEEE/CVF winter conference on applications of computer vision. 2024: 5404-5411.

[^transformer]: Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.

[^glu]: Dauphin Y N, Fan A, Auli M, et al. Language modeling with gated convolutional networks[C]//International conference on machine learning. PMLR, 2017: 933-941.

[^iglu]: Shazeer N. Glu variants improve transformer[J]. arXiv preprint arXiv:2002.05202, 2020.