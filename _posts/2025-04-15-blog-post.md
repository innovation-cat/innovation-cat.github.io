---
title: 'The Consistency Family: From Discrete-Time Constraints to Continuous-Time Flows'
excerpt: "Consistency models (CMs) have recently emerged as a powerful paradigm for accelerating diffusion sampling by directly learning mappings that preserve consistency across noisy representations of the same data. This paper provides a comprehensive study of the Consistency Family, tracing its evolution from discrete consistency models to continuous-time formulations and trajectory-based extensions. We begin by revisiting the foundational motivation behind CMs and systematically derive their discrete and continuous objectives under both consistency distillation and consistency training paradigms."
date: 2025-04-15
permalink: /posts/2025/04/consistency-model/
tags:
  - Diffusion Model
  - Consistency Model
  - Flow
  - Sampling
  - Stability
---


Diffusion models (DMs) and score-based generative models (SGMs) have demonstrated remarkable generative capability, but their iterative sampling procedures require hundreds of function evaluations to approximate the underlying ODE or SDE trajectories accurately. Thus, recent research aims to accelerate generation by distilling multi-step samplers into compact one-step or few-step models.


**Consistency Models (CMs)** were proposed as a principled framework to enable **one-step generation** while retaining the expressive power of diffusion processes. The central idea is to directly learn a **consistent mapping** across the continuous trajectory of the diffusion ODE, such that predictions made at any two time steps $ t $ and $ s $ remain consistent with the same underlying data point $ x_0 $.

![Consistency Model](/images/posts/2025-05-20-blog-post/CM.jpg)




---
<h1 id="section1" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">1. Preliminaries</h1>



---

<h1 id="section1.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">1.1 Problem Definition</h1> 


Let $$ \{x_t\}_{t\in [0,1]} $$ denote the trajectory of the probability flow ODE corresponding to a diffusion model:

$$
\frac{dx_t}{dt} = v(x_t, t), \quad x_1 \sim \mathcal{N}(0, I), \quad x_0 \sim p_{\text{data}}(x).
$$

and let $\Phi_{t\to s}$ denote the **exact** PF-ODE flow that maps a state at time $t$ to time $s\le t$. 

$$
\Phi_{t\to s}(x_t) \,=\, x_s \,\quad\, \forall\; 0\le s\le t\le T.
$$

A **consistency function** $$ f_\theta(x_t, t) $$ is defined such that it is invariant along the ODE trajectory:

$$
f_\theta(x_t, t) = f_\theta(x_s, s), \quad \forall, s < t,
$$

where $$x_s=\Phi_{t\to s}(x_t)$$.  $f_\theta$ satisfies the boundary condition

$$
f_\theta(x_0, 0) = x_0.
$$

This **consistency condition** implies that $ f_\theta $ remains constant along the trajectory of any valid ODE solution, effectively representing the same clean data point regardless of the diffusion time.

$$
f_\theta(x_t, t)\;\approx\;\Phi_{t\to 0}(x_t),
$$

Intuitively, $ f_\theta(x_t, t) $ should project any noisy sample $ x_t $ back to its corresponding clean example $ x_0 $, while maintaining mutual consistency between predictions across all time steps.


---


<h1 id="section1.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">1.2 From Definition to Training Paradigms</h1> 



Although the definition of a consistency function $f_\theta(x_t, t)$ is conceptually elegant—requiring invariance along the diffusion ODE trajectory— it is **non-trivial to enforce directly** in practice.

In real data, we only observe noisy samples $ x_t $ drawn from the marginal distribution $ p_t(x) $, without access to the exact trajectory or the corresponding clean target $ x_0 $. Therefore, we must design tractable training objectives that approximate this ideal invariance property. Two complementary approaches naturally emerge:

1. **Consistency Distillation (CD):** If a **teacher** generative model (e.g., a pretrained diffusion model or an ODE solver) is available, it can serve as a proxy for the true trajectory. The student consistency model learns to reproduce the teacher’s behavior by enforcing that its prediction at time $ t $ remains consistent with the teacher’s prediction at a previous time $ s<t $.
   
   This yields a **teacher-supervised** learning paradigm that leverages the high-fidelity trajectory of the pretrained diffusion model.

2. **Consistency Training (CT):** When no teacher model is available, consistency can still be enforced **self-supervisedly**. The model ensures that its own predictions at two nearby timesteps are consistent with each other, effectively learning to preserve invariance along the diffusion path.
   
   This strategy removes external supervision but introduces greater challenges in stability and convergence.

Hence, the two paradigms differ primarily in **whether the supervision comes from a teacher model** (CD) or **from the model’s own temporal consistency** (CT). Both share the same theoretical goal: to make $ f_\theta(x_t, t) $ **constant along the ODE trajectory**, thereby enabling direct one-step generation without explicit integration. The training objective is defined at two adjacent time steps with finite distance:

$$
\mathcal{L}_{\text{CM}} = \mathbb{E}_{x_0, \epsilon, n}
\Big[w(t)\| f_\theta(x_{t}, t) - f_{\theta}(x_{t-\Delta t}, t-\Delta t) \|_2^2\Big]
$$


where $w(t)$ is the weighting function, $$\Delta t > 0$$ is the distance between adjacent time steps, and $$d(\cdot, \cdot)$$ is a metric function which is used to measure the similarity between two high-dimensional vectors, common choices of $d$ including but not limited to: $L_2$ Loss, Pseudo-Huber Loss, and LPIPS Loss.  In our following discussion,  we will use **$L_2$ Loss** for analysis.



---

<h1 id="section2" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">2. Consistency Distillation</h1>


The **Consistency Distillation** (CD) paradigm assumes the existence of a **pretrained teacher model** $ g_\phi(x_t, t) $, such as a diffusion model or a deterministic sampler derived from it. The teacher provides a reliable target trajectory for supervision.

Given two time steps $t_n$ and $t_{n-1}=t_n-\Delta t$, the teacher integrates one ODE step to approximate the previous state:

$$
\hat{x}_{t_{n-1}} = \Phi_{t_n \to t_{n-1}}(x_{t_n}; g_\phi),
$$


The student consistency model $ f_\theta $ is then trained to match this target:

$$
\mathcal{L}_{\text{CD}} = \mathbb{E}_{x_0, t, \epsilon} \Big[w(t) \, \big\| f_\theta(x_t, t) - f_\theta(\hat{x}_{t_{n-1}}, t_{n-1}) \big\|_2^2\Big]\label{eq:cd},
$$



---

<h1 id="section2.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.1 Training pipeline of CD</h1> 

The consistency-distillation pipeline can be summarized into the following steps:


- **Step 1**: Sample $x_0$ from the dataset and create a noisy $x_{t_n}$ from forward diffusion process

  $$x_t = s(t)\,x_0 + \sigma(t)\,\epsilon,\qquad \epsilon \in \mathcal N(0,I)$$

- **Step 2**: Use the **pretrained diffusion model $s_{\theta}$** to integrate one ODE step to obtain adjacent position.
  
  $$\hat{x}_{t_{n-1}} = \Phi_{t_n \to t_{n-1}}(x_{t_n}; g_\phi)$$
  
  For example, if we have an **EDM-Style** pre-trained teacher model with $s(t)=1$ and $\sigma(t)=t$, then the PF-ODE is defined by:
  
  $$
  dx_t = -t\,s_{\theta}(x_t, t)\,dt 
  $$  
  
  Then, the adjacent position can be approximated by Euler one step.
  
  $$
  \hat{x}_{t_{n-1}} \,\approx\, x_{t_{n}} + (t_{n-1} - t_{n})t_n\,s_{\theta}(x_{t_n}, {t_n})
  $$
  
  We now have a pair sample $(x_{t_n}, \hat{x}_{t_{n-1}})$ that lies on the same path.
  
  
- **Step 3**:  Train the objective function (Eq. \ref{eq:cm}) using the collected sample pairs, and solve for the optimal model parameters with optimization algorithms such as gradient descent. In practice, the adjacent point $(x_{t_{n-1}}, t_{n-1})$ is estimated by an EMA network for stability.

  $$
  \mathcal L_{\text{CD}} = \mathbb{E}_{x_0, t, \epsilon} \Big[w(t) \, \big\| f_\theta(x_t, t) - f_{\theta^-}(\hat{x}_{t_{n-1}}, t_{n-1}) \big\|_2^2\Big]
  $$
  
  where $f_\theta$ and $f_{\theta^-}$ are referred to as the **student network** and  the **teacher netfork**. $ f_{\theta^-} $ is often implemented as an **exponential moving average (EMA)** of the student network $f_\theta$:

  $$
  \theta^- \leftarrow \text{stopgrad} (\mu {\theta^-} + (1-\mu){\theta})
  $$



---

<h1 id="section2.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.2 Why Use an EMA Teacher</h1> 



Using an EMA teacher provides two key benefits:

1. **Temporal Stability.** The EMA weights represent a temporally smoothed version of the student, filtering out high-frequency oscillations and noise introduced by stochastic gradient updates. This produces more coherent and reliable teacher predictions across training iterations, which is crucial for maintaining a stable distillation target.

2. **Implicit Self-Distillation.** By updating the teacher gradually, the student always learns from a **slightly older but more stable** version of itself. This creates a form of **self-distillation loop** that continuously refines the model’s temporal consistency, improving convergence and reducing overfitting to noisy instantaneous updates.





---


<h1 id="section3" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">3. Consistency Training From Scratch</h1>








---

<h1 id="section4" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">4. From Discrete to Continuous Consistency Models</h1>





---

<h1 id="section9" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">9. References</h1>


[^unet]: Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation[C]//International Conference on Medical image computing and computer-assisted intervention. Cham: Springer international publishing, 2015: 234-241.

[^adm]: Dhariwal P, Nichol A. Diffusion models beat gans on image synthesis[J]. Advances in neural information processing systems, 2021, 34: 8780-8794.

[^biggan]: Brock A, Donahue J, Simonyan K. Large scale GAN training for high fidelity natural image synthesis[J]. arXiv preprint arXiv:1809.11096, 2018.

[^film]: Perez E, Strub F, De Vries H, et al. Film: Visual reasoning with a general conditioning layer[C]//Proceedings of the AAAI conference on artificial intelligence. 2018, 32(1).


[^sd]: Rombach R, Blattmann A, Lorenz D, et al. High-resolution image synthesis with latent diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 10684-10695.

[^sdxl]: Podell D, English Z, Lacey K, et al. Sdxl: Improving latent diffusion models for high-resolution image synthesis[J]. arXiv preprint arXiv:2307.01952, 2023.

[^dit]: Peebles W, Xie S. Scalable diffusion models with transformers[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2023: 4195-4205.

[^transformer]: Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.


[^bn]: Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[C]//International conference on machine learning. pmlr, 2015: 448-456.

[^ln]: Ba J L, Kiros J R, Hinton G E. Layer normalization[J]. arXiv preprint arXiv:1607.06450, 2016.

[^in]: Ulyanov D, Vedaldi A, Lempitsky V. Instance normalization: The missing ingredient for fast stylization[J]. arXiv preprint arXiv:1607.08022, 2016.

[^gn]: Wu Y, He K. Group normalization[C]//Proceedings of the European conference on computer vision (ECCV). 2018: 3-19.

[^swiglu]: Shazeer N. Glu variants improve transformer[J]. arXiv preprint arXiv:2002.05202, 2020.

[^stylegan2]: Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., & Aila, T. (2020). Analyzing and Improving the Image Quality of StyleGAN. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).

[^iedm]: Karras T, Aittala M, Lehtinen J, et al. Analyzing and improving the training dynamics of diffusion models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 24174-24184.

[^edm]: Karras T, Aittala M, Aila T, et al. Elucidating the design space of diffusion-based generative models[J]. Advances in neural information processing systems, 2022, 35: 26565-26577.

[^Kingma]: Kingma D, Gao R. Understanding diffusion objectives as the elbo with simple data augmentation[J]. Advances in Neural Information Processing Systems, 2023, 36: 65484-65516.

[^Salimans]: Salimans T, Ho J. Progressive distillation for fast sampling of diffusion models[J]. arXiv preprint arXiv:2202.00512, 2022.

[^p2]: Choi J, Lee J, Shin C, et al. Perception prioritized training of diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 11472-11481.

[^min_snr]: Hang T, Gu S, Li C, et al. Efficient diffusion training via min-snr weighting strategy[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2023: 7441-7451.

[^max_snr]: Salimans T, Ho J. Progressive distillation for fast sampling of diffusion models[J]. arXiv preprint arXiv:2202.00512, 2022.

[^snr_based]: Kingma D, Gao R. Understanding diffusion objectives as the elbo with simple data augmentation[J]. Advances in Neural Information Processing Systems, 2023, 36: 65484-65516.

[^ddpm]: Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[J]. Advances in neural information processing systems, 2020, 33: 6840-6851.

[^iddpm]: Nichol A Q, Dhariwal P. Improved denoising diffusion probabilistic models[C]//International conference on machine learning. PMLR, 2021: 8162-8171.

[^ZTSNR]: Lin S, Liu B, Li J, et al. Common diffusion noise schedules and sample steps are flawed[C]//Proceedings of the IEEE/CVF winter conference on applications of computer vision. 2024: 5404-5411.

[^transformer]: Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.

[^glu]: Dauphin Y N, Fan A, Auli M, et al. Language modeling with gated convolutional networks[C]//International conference on machine learning. PMLR, 2017: 933-941.

[^iglu]: Shazeer N. Glu variants improve transformer[J]. arXiv preprint arXiv:2002.05202, 2020.