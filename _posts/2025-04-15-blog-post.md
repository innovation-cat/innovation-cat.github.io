---
title: 'The Consistency Family: From Discrete-Time Constraints to Continuous-Time Flows'
excerpt: "Consistency models (CMs) have recently emerged as a powerful paradigm for accelerating diffusion sampling by directly learning mappings that preserve consistency across noisy representations of the same data. This paper provides a comprehensive study of the Consistency Family, tracing its evolution from discrete consistency models to continuous-time formulations and trajectory-based extensions. We begin by revisiting the foundational motivation behind CMs and systematically derive their discrete and continuous objectives under both consistency distillation and consistency training paradigms."
date: 2025-04-15
permalink: /posts/2025/04/consistency-model/
tags:
  - Diffusion Model
  - Consistency Model
  - Flow
  - Sampling
  - Stability
---


<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>ðŸ“š Table of Contents</strong></span>
  </summary>
  <ul style="margin:0; padding-left:1.1rem;">
	<li><a href="#section1">1. Why Architecture Matters for Stability</a>
		<ul>
			<li><a href="#section1.1">1.1 Gradient Flow, Conditioning, and Stability</a></li>
			<li><a href="#section1.2">1.2 Balancing Capacity vs. Robustness</a></li>
			<li><a href="#section1.3">1.3 Architectureâ€“Noise Schedule Coupling</a></li>
		</ul>
	</li>
	<li><a href="#section2">2. Evolution of Diffusion Architectures</a>
		<ul>
			<li><a href="#section2.1">2.1 Classical U-Net Foundations</a></li>
			<li><a href="#section2.2">2.2 ADM Improvements (Attention, Class Conditioning)</a></li>
			<li><a href="#section2.3">2.3 Latent U-Net (Stable Diffusion, SDXL)</a></li>
			<li><a href="#section2.4">2.4 Transformer-based Designs (DiT, MMDiT-X, Hybrid Models)</a></li>
			<li><a href="#section2.5">2.5 Extensions to Video and 3D Diffusion (Video U-Net, Gaussian Splatting)</a></li>
			<li><a href="#section2.6">2.6 Lightweight & Memory-efficient Designs (MobileDiffusion, LightDiffusion)</a></li>
		</ul>
	</li>	
	<li><a href="#section3">3. Stability-Oriented Architectural Designs</a>
      <ul>
        <li><a href="#section3.1">3.1 Architectural Philosophies: U-Net vs. DiT</a></li>
		  <ul>
            <li><a href="#section3.1.1">3.1.1 U-Net Macro Topology</a></li>
            <li><a href="#section3.1.2">3.1.2 DiT Macro Topology</a></li>
            <li><a href="#section3.1.3">3.1.3 Summary of Divergence</a></li>
          </ul>
        <li><a href="#section3.2">3.2 Stabilization in U-Net Architectures</a>
		  <ul>
            <li><a href="#section3.2.1">3.2.1 The Control System: Conditioning via AdaGN</a></li>
            <li><a href="#section3.2.2">3.2.2 The Signal Pathways: Skip Connections and Residual Innovations</a></li>
          </ul>
		</li>
        <li><a href="#section3.3">3.3 Stabilization in DiT Architectures</a>
		  <ul>
            <li><a href="#section3.3.1">3.3.1 The Control System: Conditioning via AdaLN</a></li>
            <li><a href="#section3.3.2">3.3.2 The Signal Pathways: Enabling Deep Stacks in DiT</a></li>
			<li><a href="#section3.3.3">3.3.3 Component Stability: Taming the Attention Mechanism</a></li>
          </ul>
		</li>
        <li><a href="#section3.4">3.4 Cross-Paradigm Stabilization Strategies</a>
          <ul>
            <li><a href="#section3.4.1">3.4.1 Universal Training Harness</a></li>
            <li><a href="#section3.4.2">3.4.2 General Architectural Practices</a></li>
            <li><a href="#section3.4.3">3.4.3 Numerical and Training-Free Stability</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#section4">4. Efficiency-Oriented Architectural Designs</a>
      <ul>
        <li><a href="#section4.1">4.1 Token Compression and Merging for Efficiency & Stability</a></li>
        <li><a href="#section4.2">4.2 Automated Architecture Search (DiffusionNAG)</a></li>
        <li><a href="#section4.3">4.3 Multi-resolution Networks with Time-dependent Norms</a></li>
        <li><a href="#section4.4">4.4 Parallel vs. Sequential Transformer Architectures</a></li>
        <li><a href="#section4.5">4.5 Parameter-efficient Modules: LoRA, DoRA, T-Fixup</a></li>
      </ul>
    </li>
    <li><a href="#section5">5. Multi-modality & Generalization-Oriented Designs</a>
      <ul>
        <li><a href="#section5.1">5.1 Multi-modal Token Conditioning (Cross-Attention, LoRA, Deep Fusion)</a></li>
        <li><a href="#section5.2">5.2 Diffusion Transformers (DiT) and Scaling Laws</a></li>
        <li><a href="#section5.3">5.3 Decoupled Design (DDT: Encoderâ€“Decoder Separation)</a></li>
        <li><a href="#section5.4">5.4 State Space Models (S4, Mamba) as Alternatives to Transformers</a></li>
      </ul>
    </li>
    <li><a href="#section6">6. Architectureâ€“Schedule Co-Design (Integrative Paradigm)</a>
      <ul>
        <li><a href="#section6.1">6.1 Preconditioning with AdaLN-Zero</a></li>
        <li><a href="#section6.2">6.2 Hybridization of Architectural and Regularization Strategies</a></li>
        <li><a href="#section6.3">6.3 Structurally Balanced Design for Training Stability</a></li>
        <li><a href="#section6.4">6.4 Architectureâ€“Noise Schedule Co-design</a></li>
      </ul>
    </li>
    <li><a href="#section7">7. Practical Takeaways</a>
      <ul>
        <li><a href="#section7.1">7.1 Stability-oriented Designs</a></li>
        <li><a href="#section7.2">7.2 Efficiency-oriented Designs</a></li>
        <li><a href="#section7.3">7.3 Generalization-oriented Designs</a></li>
        <li><a href="#section7.4">7.4 Fidelity & Accuracy as Evaluation Metrics</a></li>
      </ul>
    </li>
    <li><a href="#section8">8. Conclusion</a></li>
    <li><a href="#section9">9. References</a></li>
  </ul>
</details>

Diffusion models (DMs) and score-based generative models (SGMs) have demonstrated remarkable generative capability, but their iterative sampling procedures require hundreds of function evaluations to approximate the underlying ODE or SDE trajectories accurately. Thus, recent research aims to accelerate generation by distilling multi-step samplers into compact one-step or few-step models.


**Consistency Models (CMs)** were proposed as a principled framework to enable **one-step generation** while retaining the expressive power of diffusion processes. The central idea is to directly learn a **consistent mapping** across the continuous trajectory of the diffusion ODE, such that predictions made at any two time steps $ t $ and $ s $ remain consistent with the same underlying data point $ x_0 $.

![Consistency Model](/images/posts/2025-05-20-blog-post/CM.jpg)




---
<h1 id="section1" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">1. Preliminaries</h1>



---

<h1 id="section1.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">1.1 Problem Definition</h1> 


Let $$ \{x_t\}_{t\in [0,1]} $$ denote the trajectory of the probability flow ODE corresponding to a diffusion model:

$$
\frac{dx_t}{dt} = v(x_t, t), \quad x_1 \sim \mathcal{N}(0, I), \quad x_0 \sim p_{\text{data}}(x).
$$

and let $\Phi_{t\to s}$ denote the **exact** PF-ODE flow that maps a state at time $t$ to time $s\le t$. 

$$
\Phi_{t\to s}(x_t) \,=\, x_s \,\quad\, \forall\; 0\le s\le t\le T.
$$

A **consistency function** $$ f_\theta(x_t, t) $$ is defined such that it is invariant along the ODE trajectory:

$$
f_\theta(x_t, t) = f_\theta(x_s, s), \quad \forall, s < t,
$$

where $$x_s=\Phi_{t\to s}(x_t)$$.  $f_\theta$ satisfies the boundary condition

$$
f_\theta(x_0, 0) = x_0.
$$

This **consistency condition** implies that $ f_\theta $ remains constant along the trajectory of any valid ODE solution, effectively representing the same clean data point regardless of the diffusion time.

$$
f_\theta(x_t, t)\;\approx\;\Phi_{t\to 0}(x_t),
$$

Intuitively, $ f_\theta(x_t, t) $ should project any noisy sample $ x_t $ back to its corresponding clean example $ x_0 $, while maintaining mutual consistency between predictions across all time steps.


---


<h1 id="section1.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">1.2 From Definition to Training Paradigms</h1> 



Although the definition of a consistency function $f_\theta(x_t, t)$ is conceptually elegantâ€”requiring invariance along the diffusion ODE trajectoryâ€” it is **non-trivial to enforce directly** in practice.

In real data, we only observe noisy samples $ x_t $ drawn from the marginal distribution $ p_t(x) $, without access to the exact trajectory or the corresponding clean target $ x_0 $. Therefore, we must design tractable training objectives that approximate this ideal invariance property. Two complementary approaches naturally emerge:

1. **Consistency Distillation (CD):** If a **teacher** generative model (e.g., a pretrained diffusion model or an ODE solver) is available, it can serve as a proxy for the true trajectory. The student consistency model learns to reproduce the teacherâ€™s behavior by enforcing that its prediction at time $ t $ remains consistent with the teacherâ€™s prediction at a previous time $ s<t $.
   
   This yields a **teacher-supervised** learning paradigm that leverages the high-fidelity trajectory of the pretrained diffusion model.

2. **Consistency Training (CT):** When no teacher model is available, consistency can still be enforced **self-supervisedly**. The model ensures that its own predictions at two nearby timesteps are consistent with each other, effectively learning to preserve invariance along the diffusion path.
   
   This strategy removes external supervision but introduces greater challenges in stability and convergence.

Hence, the two paradigms differ primarily in **whether the supervision comes from a teacher model** (CD) or **from the modelâ€™s own temporal consistency** (CT). Both share the same theoretical goal: to make $ f_\theta(x_t, t) $ **constant along the ODE trajectory**, thereby enabling direct one-step generation without explicit integration. The training objective is defined at two adjacent time steps with finite distance:

$$
\mathcal{L}_{\text{CM}} = \mathbb{E}_{x_0, \epsilon, n}
\Big[w(t)\| f_\theta(x_{t}, t) - f_{\theta}(x_{t-\Delta t}, t-\Delta t) \|_2^2\Big]
$$


where $w(t)$ is the weighting function, $$\Delta t > 0$$ is the distance between adjacent time steps, and $$d(\cdot, \cdot)$$ is a metric function which is used to measure the similarity between two high-dimensional vectors, common choices of $d$ including but not limited to: $L_2$ Loss, Pseudo-Huber Loss, and LPIPS Loss.  In our following discussion,  we will use **$L_2$ Loss** for analysis.



---

<h1 id="section2" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">2. Consistency Distillation</h1>


The **Consistency Distillation** (CD) paradigm assumes the existence of a **pretrained teacher model** $ g_\phi(x_t, t) $, such as a diffusion model or a deterministic sampler derived from it. The teacher provides a reliable target trajectory for supervision.

Given two time steps $t_n$ and $t_{n-1}=t_n-\Delta t$, the teacher integrates one ODE step to approximate the previous state:

$$
\hat{x}_{t_{n-1}} = \Phi_{t_n \to t_{n-1}}(x_{t_n}; g_\phi),
$$


The student consistency model $ f_\theta $ is then trained to match this target:

$$
\mathcal{L}_{\text{CD}} = \mathbb{E}_{x_0, t, \epsilon} \Big[w(t) \, \big\| f_\theta(x_t, t) - f_\theta(\hat{x}_{t_{n-1}}, t_{n-1}) \big\|_2^2\Big]\label{eq:cm},
$$



---

<h1 id="section2.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.1 Training pipeline of CD</h1> 

The consistency-distillation pipeline can be summarized into the following steps:


- **Step 1**: Sample $x_0$ from the dataset and create a noisy $x_{t_n}$ from forward diffusion process

  $$x_t = s(t)\,x_0 + \sigma(t)\,\epsilon,\qquad \epsilon \in \mathcal N(0,I)$$

- **Step 2**: Use the **pretrained diffusion model $s_{\theta}$** to integrate one ODE step to obtain adjacent position.
  
  $$\hat{x}_{t_{n-1}} = \Phi_{t_n \to t_{n-1}}(x_{t_n}; g_\phi)$$
  
  For example, if we have an **EDM-Style** pre-trained teacher model with $s(t)=1$ and $\sigma(t)=t$, then the PF-ODE is defined by:
  
  $$
  dx_t = -t\,s_{\theta}(x_t, t)\,dt 
  $$  
  
  Then, the adjacent position can be approximated by Euler one step.
  
  $$
  \hat{x}_{t_{n-1}} \,\approx\, x_{t_{n}} + (t_{n-1} - t_{n})t_n s_{\theta}(x_{t_n}, {t_n})\label{eq:cd}
  $$
  
  We now have a pair $x_{t_n}, x_{t_{n-1}}$ that lies on the same path.
  
  
- **Step 3**:  Train the objective function (Eq. \ref{eq:cm}) using the collected sample pairs, and solve for the optimal model parameters with optimization algorithms such as gradient descent.


Define the consistency function $f_{\theta}$ which is trained to produce consistent outputs for this pair. The loss function minimizes the distance between their predictions:
  
  $$
  \mathcal L_{\text{CD}} = \mathbb E[d(f_{\theta}(x_{t_n}, t_n), f_{\theta^-}(x_{t_{n-1}}, t_{n-1}))]
  $$
  
  where 
  
  $$
   f_{\theta}(x_t, t) = c_{\text{skip}}x_t + c_{\text{out}}F_{\theta}(c_{\text{in}}x_t, c_{\text{noise}})
  $$
  
  and $f_{\theta^-}$ is a target network, typically an exponential moving average (EMA) of $f_{\theta}$'s weights, which provides a more stable training target.
  
  $$
  f_{\theta^-} \leftarrow \text{stopgrad} (\mu {\theta^-} + (1-\mu){\theta})
  $$
  
  $\theta$ is the network being optimized, ${\theta^-}$ provides a smooth, stop-gradient target.





where ( x_t = \alpha_t x_0 + \sigma_t \epsilon ), ( \epsilon \sim \mathcal{N}(0, I) ), and ( w(t) ) is a time-dependent weighting term that balances signal-to-noise ratios across timesteps.

By distilling the teacherâ€™s multi-step behavior into a single mapping ( f_\theta ), the model inherits the generative ability of the diffusion process while achieving a dramatically shorter sampling trajectoryâ€”often reducing hundreds of steps to one or a few evaluations.


---


<h1 id="section3" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">3. Consistency Training From Scratch</h1>








---

<h1 id="section2" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">2. From Discrete to Continuous Consistency Models</h1>


The consistency model (CM) family can be formulated either in **discrete** or **continuous** time. Both variants share the same fundamental objective â€” to ensure that predictions made from noisy samples at different time steps remain **mutually consistent** along the diffusion trajectory.

This section presents the discrete formulation and then derives its continuous-time limit, showing that the two are mathematically equivalent.

Given a clean data sample $x_0$ and a forward noising process:

$$
x_t = s(t)\,x_0 + \sigma(t)\,\epsilon, \qquad \epsilon \sim \mathcal N(0, I),
$$

we define a time-dependent consistency model $f_\theta(x_t, t)$ that predicts a **consistent reconstruction** across noise levels $t \in [0, 1]$. Intuitively, for the same underlying $x_0$, predictions made from different noisy realizations should coincide:

$$
f_\theta(x_{t_n}, t_n) \approx f_\theta(x_{t_{n-1}}, t_{n-1}).
$$

This constraint enforces **pathwise invariance**, i.e., $f_\theta(x_t, t)$ remains constant along the forward diffusion trajectory of $x_t$.

---

<h1 id="section2.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.1 Discrete-Time Consistency Model</h1> 

Consider a pre-defined sequence of noise levels

$$0 = t_0 < t_1 < \cdots < t_N = 1$$

The discrete-time consistency loss penalizes deviations in model predictions between adjacent time steps:

$$
\mathcal{L}_{\text{disc}} = \mathbb{E}_{x_0, \epsilon, n}
\Big[w(t)\| f_\theta(x_{t_n}, t_n) - f_{\theta^-}(x_{t_{n-1}}, t_{n-1}) \|_2^2\Big],
$$

where $w(t)$ is a time-dependent weighting function, $$f_{\theta^-}$$ denotes an **exponential moving average (EMA)** copy of the model, providing a temporally stable target.

This discrete formulation encourages the model to produce consistent outputs across pairs of noisy inputs derived from the same clean sample $x_0$. Practically, it acts as a **self-distillation mechanism**, where $f_\theta$ learns to match its own EMA prediction at a slightly smaller noise level.



---

<h1 id="section2.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.2 From Discrete Consistency to Continuous-Time Dynamics</h1> 


To understand the continuous-time limit, consider two adjacent time steps $t_n$ and $t_{n-1}=t_n-\Delta t$. Applying a first-order Taylor expansion of $f_\theta$ around $(x_{t_n}, t_n)$:


$$
f_\theta(x_{t_n}, t_n) - \partial_x f_{\theta}(x_{t_n}, t_n) (x_{t_{n-1}} - x_{t_{n}}) - \partial_t f_{\theta}(x_{t_n}, t_n) ({t_{n-1}} - {t_{n}}) + \mathcal O({\Delta t}^2)
$$

where $({t_{n-1}} - {t_{n}})=\Delta t$, and 

$$
(x_{t_{n-1}} - x_{t_{n}}) \,\approx\, - \frac{dx_t}{dt}\Delta t + \mathcal O({\Delta t}^2)
$$

Substituting  into gives the **pairwise difference** between predictions:

$$
\begin{align}
f_\theta(x_{t_{n-1}}, t_{n-1}) = f_\theta(x_{t_n}, t_n) +(\partial_x f_{\theta} \frac{dx_t}{dt} + \partial_t f_{\theta})\Delta t \\[10pt]
& = f_\theta(x_{t_n}, t_n) + \frac{df_{\theta^-}}{dt}\Delta t + \mathcal O({\Delta t}^2)
\end{align}
$$

---







---

<h1 id="section9" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">9. References</h1>


[^unet]: Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation[C]//International Conference on Medical image computing and computer-assisted intervention. Cham: Springer international publishing, 2015: 234-241.

[^adm]: Dhariwal P, Nichol A. Diffusion models beat gans on image synthesis[J]. Advances in neural information processing systems, 2021, 34: 8780-8794.

[^biggan]: Brock A, Donahue J, Simonyan K. Large scale GAN training for high fidelity natural image synthesis[J]. arXiv preprint arXiv:1809.11096, 2018.

[^film]: Perez E, Strub F, De Vries H, et al. Film: Visual reasoning with a general conditioning layer[C]//Proceedings of the AAAI conference on artificial intelligence. 2018, 32(1).


[^sd]: Rombach R, Blattmann A, Lorenz D, et al. High-resolution image synthesis with latent diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 10684-10695.

[^sdxl]: Podell D, English Z, Lacey K, et al. Sdxl: Improving latent diffusion models for high-resolution image synthesis[J]. arXiv preprint arXiv:2307.01952, 2023.

[^dit]: Peebles W, Xie S. Scalable diffusion models with transformers[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2023: 4195-4205.

[^transformer]: Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.


[^bn]: Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[C]//International conference on machine learning. pmlr, 2015: 448-456.

[^ln]: Ba J L, Kiros J R, Hinton G E. Layer normalization[J]. arXiv preprint arXiv:1607.06450, 2016.

[^in]: Ulyanov D, Vedaldi A, Lempitsky V. Instance normalization: The missing ingredient for fast stylization[J]. arXiv preprint arXiv:1607.08022, 2016.

[^gn]: Wu Y, He K. Group normalization[C]//Proceedings of the European conference on computer vision (ECCV). 2018: 3-19.

[^swiglu]: Shazeer N. Glu variants improve transformer[J]. arXiv preprint arXiv:2002.05202, 2020.

[^stylegan2]: Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., & Aila, T. (2020). Analyzing and Improving the Image Quality of StyleGAN. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).

[^iedm]: Karras T, Aittala M, Lehtinen J, et al. Analyzing and improving the training dynamics of diffusion models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 24174-24184.

[^edm]: Karras T, Aittala M, Aila T, et al. Elucidating the design space of diffusion-based generative models[J]. Advances in neural information processing systems, 2022, 35: 26565-26577.

[^Kingma]: Kingma D, Gao R. Understanding diffusion objectives as the elbo with simple data augmentation[J]. Advances in Neural Information Processing Systems, 2023, 36: 65484-65516.

[^Salimans]: Salimans T, Ho J. Progressive distillation for fast sampling of diffusion models[J]. arXiv preprint arXiv:2202.00512, 2022.

[^p2]: Choi J, Lee J, Shin C, et al. Perception prioritized training of diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 11472-11481.

[^min_snr]: Hang T, Gu S, Li C, et al. Efficient diffusion training via min-snr weighting strategy[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2023: 7441-7451.

[^max_snr]: Salimans T, Ho J. Progressive distillation for fast sampling of diffusion models[J]. arXiv preprint arXiv:2202.00512, 2022.

[^snr_based]: Kingma D, Gao R. Understanding diffusion objectives as the elbo with simple data augmentation[J]. Advances in Neural Information Processing Systems, 2023, 36: 65484-65516.

[^ddpm]: Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[J]. Advances in neural information processing systems, 2020, 33: 6840-6851.

[^iddpm]: Nichol A Q, Dhariwal P. Improved denoising diffusion probabilistic models[C]//International conference on machine learning. PMLR, 2021: 8162-8171.

[^ZTSNR]: Lin S, Liu B, Li J, et al. Common diffusion noise schedules and sample steps are flawed[C]//Proceedings of the IEEE/CVF winter conference on applications of computer vision. 2024: 5404-5411.

[^transformer]: Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.

[^glu]: Dauphin Y N, Fan A, Auli M, et al. Language modeling with gated convolutional networks[C]//International conference on machine learning. PMLR, 2017: 933-941.

[^iglu]: Shazeer N. Glu variants improve transformer[J]. arXiv preprint arXiv:2002.05202, 2020.