---
title: 'Accelerating Diffusion Sampling: From Multi-Step to Single-step Generation'
date: 2025-10-01
modified: 2026-01-06
excerpt: "This article takes a deep dive into the evolution of diffusion model sampling techniques, tracing the progression from early score-based models with Langevin Dynamics, through discrete and non-Markov diffusion processes, to continuous-time SDE/ODE formulations, specialized numerical solvers, and cutting-edge methods such as consistency models, distillation, and flow matching.  Our goal is to provide both a historical perspective and a unified theoretical framework to help readers understand not only *how* these methods work but *why* they were developed."
permalink: /posts/2025/10/diffusion-sampling/
tags:
  - Diffusion Sampling
  - Diffusion Model
  - Numerical Computation
  - Model Distillation
  - Score Distillation
  - Trajectory Distillation
  - Adversarial Distillation
  - Flow Matching
  - Flow Map
---

<details class="custom-toc">
  <summary class="toc-summary">
    <div class="toc-title">
      <span style="font-size: 1.4rem;">üìò</span>
      <span>TABLE OF CONTENTS</span>
    </div>
    <svg class="toc-arrow" fill="none" stroke="currentColor" viewBox="0 0 24 24">
      <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 9l-7 7-7-7"></path>
    </svg>
  </summary>

  <div class="toc-content">
    <ul class="toc-list">
      <!-- Â∞Ü‰∏ÄÁ∫ßÊ†áÈ¢òÂåÖË£Ö‰∏∫ toc-item-main -->
      <li class="toc-item-main">
        <a href="#sectionPart1" class="toc-link-main">Part I ‚Äî Foundation and Preliminary</a>
        <ul class="toc-sublist">
          <li><a href="#section1">1. Introduction</a></li>
          <li><a href="#section2">2. Limitations of Classical Generative Models</a></li>
          <li><a href="#section3">3. The Rise of Diffusion Models</a></li>
          <li><a href="#section4">4. The Dual Origins of Diffusion Sampling</a>
            <ul class="toc-sublist">
              <li><a href="#section4.1">4.1 The Continuous-State Perspective: Score-Based Generative Models</a>
                <ul class="toc-sublist">
                  <li><a href="#section4.1.1">4.1.1 A Radical Shift: Learning the Gradient of data density</a></li>
                  <li><a href="#section4.1.2">4.1.2 Score Matching Generative Model</a></li>
                  <li><a href="#section4.1.3">4.1.3 NCSN and Annealed Langevin Dynamics</a></li>
                  <li><a href="#section4.1.4">4.1.4 Historical Limitations and Enduring Legacy</a></li>
                </ul>
              </li>
              <li><a href="#section4.2">4.2 The Discrete-Time Perspective: Denoising Diffusion Probabilistic Models</a>
                <ul class="toc-sublist">
                  <li><a href="#section4.2.1">4.2.1 The Core Idea: An Elegant Markov Chain</a></li>
                  <li><a href="#section4.2.2">4.2.2 Forward Diffusion and Reverse Denoising</a></li>
                  <li><a href="#section4.2.3">4.2.3 Theoretical Elegance and Practical Bottleneck</a></li>
                </ul>
              </li>
              <li><a href="#section4.3">4.3 The Initial Convergence: Tying the Two Worlds Together</a></li>
            </ul>
          </li>
        </ul> 
      </li>

      <li class="toc-item-main">
        <a href="#sectionPartII" class="toc-link-main">Part II ‚Äî Fast Sampling without Retraining</a>
        <ul class="toc-sublist">
          <li><a href="#section5">5. Breaking the Markovian Chain with DDIM</a>
          <ul class="toc-sublist">
            <li><a href="#section5.1">5.1 From Stochastic Denoising to Deterministic Prediction</a></li>
            <li><a href="#section5.2">5.2 The Freedom to Jump: Non-Markovian Skip-Step Sampling</a></li>
            <li><a href="#section5.3">5.3 The $\eta$ Parameter: A Dial for Randomness</a></li>
            <li><a href="#section5.4">5.4 Bridging to a Deeper Theory: The Lingering Question</a></li>
          </ul>
        </li>
        <li><a href="#section6">6. The Unifying Framework of SDEs and ODEs</a>
          <ul class="toc-sublist">
            <li><a href="#section6.1">6.1 From Discrete Steps to Continuous Time</a></li>
            <li><a href="#section6.2">6.2 The Probability Flow ODE: Discovering the Deterministic Path</a></li>
            <li><a href="#section6.3">6.3 Theoretical Completion: A New Paradigm for Sampling</a></li>
          </ul>
        </li>
        <li><a href="#section7">7. High-Order PF-ODE Solver in Diffusion Models</a>
          <ul class="toc-sublist">
            <li><a href="#section7.1">7.1 Why Classical ODE Solvers Fall Short</a></li>
            <li><a href="#section7.2">7.2 The Rise of Specialized Solvers</a></li>
          </ul>
        </li>
        </ul> 
      </li>

      <li class="toc-item-main">
        <a href="#sectionPartIII" class="toc-link-main">Part III ‚Äî Distillation for Fast Sampling</a>
        <ul class="toc-sublist">
          <li><a href="#section8">8. Trajectory-Based Distillation</a>
            <ul class="toc-sublist">
              <li><a href="#section8.1">8.1 Progressive Distillation</a></li>
              <li><a href="#section8.2">8.2 Transitive Closure Time-Distillation</a></li>
              <li><a href="#section8.3">8.3 Guided Distillation</a></li>
            </ul>
          </li>
          <li><a href="#section9">9. Adversarial-Based Distillation</a>
            <ul class="toc-sublist">
              <li><a href="#section9.1">9.1 Adversarial Diffusion Distillation (ADD)</a></li>
              <li><a href="#section9.2">9.2 Progressive Adversarial Diffusion Distillation (PADD)</a></li>
              <li><a href="#section9.3">9.3 Latent Adversarial Diffusion Distillation (LADD)</a></li>
            </ul>
          </li>
          <li><a href="#section10">10. Distribution-Based Distillation</a>
            <ul class="toc-sublist">
              <li><a href="#section10.1">10.1 Distribution Matching Distillation (DMD)</a></li>
              <li><a href="#section10.2">10.2 Improved Distribution Matching Distillation (DMD2)</a></li>
            </ul>
          </li>
          <li><a href="#section11">11. Score-Based Distillation</a>
            <ul class="toc-sublist">
              <li><a href="#section11.1">11.1 Score identity Distillation (SiD)</a></li>
              <li><a href="#section11.2">11.2 Score Implicit Matching (SIM)</a></li>
              <li><a href="#section11.3">11.3 Guided Score identity Distillation</a></li>
              <li><a href="#section11.4">11.4 Diff-Instruct</a></li>
            </ul>
          </li>
          <li><a href="#section12">12. Consistency-Based Distillation</a>
            <ul class="toc-sublist">
              <li><a href="#section12.1">12.1 Core Ingredients of Consistency Distillation</a></li>
              <li><a href="#section12.2">12.2 Canonical Prediction Space and the Consistency Constraint</a></li>
              <li><a href="#section12.3">12.3 The Generic Objective: Pairwise Time-Consistency</a></li>
              <li><a href="#section12.4">12.4 Boundary and Anchor Conditions</a></li>
              <li><a href="#section12.5">12.5 A Canonical Training Algorithm (Template)</a></li>
            </ul>
          </li>
        </ul> 
      </li>


      <li class="toc-item-main">
        <a href="#sectionPartIV" class="toc-link-main">Part IV ‚Äî Flow Matching: Looking for Straight Trajectory</a>
        <ul class="toc-sublist">
          <li><a href="#section13">13. Flow Matching</a></li>
        </ul> 
      </li>

      <li class="toc-item-main">
        <a href="#sectionPartV" class="toc-link-main">Part V ‚Äî From Trajectory to Operator</a>
        <ul class="toc-sublist">
          <li><a href="#section14">14. Core Concept and General discussion</a></li>
        </ul> 
      </li>

      <li class="toc-item-main">
        <a href="#section15" class="toc-link-main">15. References</a>
      </li>

    </ul>
  </div>  

</details>

This article systematically reviews the development history of sampling techniques in diffusion models. Starting from the two parallel technical routes of score-based models and DDPM, we explain how they achieve theoretical unification through the SDE/ODE framework. On this basis, we delve into various efficient samplers designed for solving the probability flow ODE (PF-ODE), analyzing the evolutionary motivations from the limitations of classical numerical methods to dedicated solvers such as DPM-Solver. Subsequently, the article shifts its perspective to innovations in the sampling paradigm itself, covering cutting-edge technologies such as consistency models and sampling distillation aimed at achieving single-step/few-step generation. Finally, we combine practical strategies such as hybrid sampling and guidance, conduct a comprehensive comparison of existing methods, and look forward to future research directions such as learnable samplers and hardware-aware optimizations.

<div style="display: block; margin: 0 auto; text-align: center;">
  <img src="/assets/images/2025-10-01-blog-post/fast_sampling.jpg" alt="outline" style="display: block; margin: 0 auto;" width="900" height="700">
  <div style="margin-top: 8px; font-size: 16px; color: #666; font-style: Times New Roman;">
     Figure 1: Summary of fast diffusion sampling
  </div>
</div>

<br>

---

<h1 id="sectionPartI" style="color: #1E3A8A; font-size: 30px; font-weight: bold; text-decoration: underline;">Part I ‚Äî Foundation and Preliminary</h1>


---

<h1 id="section1" style="color: #1E40AF; font-size: 26px; font-weight: bold; text-decoration: underline;">1. Introduction</h1>


The history of diffusion model sampling is a story of a relentless quest for an ideal balance within a challenging trilemma: **Speed** vs. **Quality** vs. **Diversity**.

- **Speed (Computational Cost)**: The pioneering DDPM (Denoising Diffusion Probabilistic Models) demonstrated remarkable generation quality but required thousands of sequential function evaluations (NFEs) to produce a single sample. This computational burden was a significant barrier to practical, real-time applications and iterative creative workflows. Consequently, the primary driver for a vast body of research has been the aggressive reduction of these sampling steps.

- **Quality (Fidelity)**: A faster sampler is useless if it compromises the model's generative prowess. The goal is to reduce steps while preserving, or even enhancing, the fidelity of the output. Many methods grapple with issues like error accumulation, which can lead to blurry or artifact-laden results, especially at very low step counts. High-quality sampling means faithfully following the path dictated by the learned model.

- **Diversity & Stability**: Sampling can be either stochastic (introducing randomness at each step) or deterministic (following a fixed path for a given initial noise). Stochastic samplers can generate a wider variety of outputs from the same starting point, while deterministic ones offer reproducibility. The choice between them is application-dependent, and the stability of the numerical methods used, especially for high-order solvers, is a critical area of research.

This perpetual negotiation between speed, quality, and diversity has fueled a Cambrian explosion of innovative sampling algorithms, each attempting to push the Pareto frontier of what is possible.



---

<h1 id="section2" style="color: #1E40AF; font-size: 26px; font-weight: bold; text-decoration: underline;">2. Classical Generative Models</h1>



The essence of any generative model is to build a distribution $p_{\theta}(x)$ to approximate the true data distribution $p_{\text{data}}(x)$,  that is,

$$p_{\theta}(x) \approx p_{\text{data}}(x)$$

Once we can evaluate or sample from $p_{\theta}(x)$, we can synthesize new, realistic data. 

<div style="display: block; margin: 0 auto; text-align: center;">
  <img src="/assets/images/2025-10-01-blog-post/gm.jpg" alt="outline" style="display: block; margin: 0 auto;" width="900" height="700">
  <div style="margin-top: 8px; font-size: 16px; color: #666; font-style: Times New Roman;">
     Figure 2: Generative Model PipeLine
  </div>
</div>

<br>

However, modeling high-dimensional probability densities is notoriously difficult for several intertwined reasons:

- **Intractable Normalization Constant.** Many distributions can be expressed as
  
  $$
  p_{\theta}(x)=\frac{1}{Z}\,\exp (f_{\theta}(x))
  $$
  
  where 
  
  $$Z=\int \exp (f_{\theta}(x))dx$$ 
  
  is the **partition function**. Computing $Z$ in high-dimensional space is intractable, making likelihood evaluation and gradient computation (with respect to $\theta$, **not** $x$) expensive.
  

- **Curse of Dimensionality.** Real-world data (images, audio, text) lies near intricate low-dimensional manifolds within vast ambient spaces.
   Directly fitting a normalized density over the full space is inefficient and unstable.
   
- **Difficult Likelihood Optimization.** To solve $p_{\theta}$, the most common strategy is to optimize $\log p_{\theta}(x)$ (Maximum likelihood estimation). However, maximizing $\log p_{\theta}(x)$ requires differentiating through complex transformations or Jacobian determinants‚Äîtractable only for carefully designed, such as flow models.
   


Although directly estimating the full data density is nearly impossible in high-dimensional space, different classes of generative models have developed specialized mechanisms to avoid or bypass the core obstacles outlined above. 


- **Variational Autoencoders (VAEs): Approximating the Density via Latent Variables.** VAEs sidestep the intractable normalization constant by introducing a latent variable $z$ and decomposing the joint distribution as

  $$
  p_\theta(x,z)=p_\theta(x \mid z) \cdot p(z),
  $$
  
  Instead of maximizing $\log p_\theta(x)$ directly, the VAE introduces an approximate posterior $q_\phi(z\mid x)$ and applies Jensen‚Äôs inequality:


  $$
  \mathcal L_{\text{MLE}} = \log p_\theta(x) \ge \underbrace{ \mathbb E_{q_\phi(z \mid x)}[\log p_\theta(x \mid z)] - \mathrm{KL}(q_\phi(z \mid x) \mid p(z))}_{\mathcal L_{\text{VLB}}}.
  $$

  Optimizing $\mathcal L_{\text{MLE}}$ requires to compute the gradient of $Z$, which is usually intractable. However, $\mathcal L_{\text{VLB}}$ contains only terms that are individually normalized and easy to compute:
  - $p(z)$ is a simple prior, typically $\mathcal N(0,I)$.
  - $q_\phi(z \mid x)$ is an approximate posterior, which is chosen from a tractable family, e.g.
    
	$$
    q_\phi(z\mid x) = \mathcal{N}(z; \mu_\phi(x), \mathrm{diag}(\sigma_\phi^2(x))).
    $$
    
	Its log-density is explicitly computable and differentiable with respect to $\phi$.
	
  - $\log p_\theta(x \mid z)$ is conditional likelihood, which is typically Gaussian or Bernoulli. Since the reconstruction loss is simply the negative log-likelihood of the assumed data distribution given the latent variable. the choice of conditional likelihood determines the form of the reconstructed loss function.
  
    | Conditional Likelihood $p_\theta(x \mid z)$ | Reconstruction Loss |
	|:-------------------------------------------:|:----------------------:|
	| $\mathcal{N}(x; \mu_\theta(z), \sigma^2 I)$ | Mean Squared Error (MSE) |
	| $\mathrm{Bernoulli}(x; \pi_\theta(z))$ | Binary Cross Entropy (BCE) |
	| $\mathrm{Categorical}(x; \pi_\theta(z))$ | Cross Entropy |
	| $\mathrm{Laplace}(x; \mu_\theta(z), b)$ | L1 Loss |


  Every term in the ELBO depends only on distributions that we define ourselves with explicit normalization constants, this turns an intractable integral into a differentiable objective. Sampling becomes straightforward: draw $z\,\sim\,p(z)$ and decode $x\,\sim\,p_\theta(x \mid z)$. The cost is **blurriness** in samples‚Äîan inevitable consequence of the Gaussian decoder and variational approximation.
  
  
- **Normalizing Flows: Enforcing Exact Likelihood via Invertible Transformations.** Flow-based models make the density tractable by designing the generative process as a sequence of invertible mappings:

  $$
  x = f_\theta(z),\qquad z\,\sim\,p(z).
  $$

  The change-of-variables formula yields an explicit, exact log-likelihood:

  $$
  \log p_\theta(x)=\log p(z)-\log|\det J_{f_\theta}(z)|.
  $$
  
  Thus, both density evaluation and sampling are efficient and exact. However, this convenience comes with architectural constraints: each transformation must be bijective with a Jacobian determinant that is computationally cheap to compute. To maintain this property, Flow models (e.g., RealNVP, Glow) restrict the network‚Äôs expressiveness compared with unconstrained architectures.
  
  
- **Generative Adversarial Networks (GANs): Learning without a Normalized Density.** GANs abandon likelihoods altogether. A generator $G_\theta(z)$ learns to produce samples whose distribution matches the data via an adversarial discriminator $D_\phi(x)$:

  $$
  \min_G\max_D ;
  \mathbb E_{x\sim p_{\text{data}}}\,[\log D(x)]
  +\mathbb E_{z\sim p(z)}\,[\log(1-D(G(z)))].
  $$

  This **implicit likelihood** approach avoids computing $Z$, $\log p(x)$, or any Jacobian. Sampling is trivial: feed a random latent $z$ through the generator. Yet, the absence of an explicit density leads to **instability and mode collapse**‚Äîthe model may learn to generate only a subset of modes of the true distribution.



- **Energy-Based Models (EBMs): Modeling Unnormalized Densities with MCMC Sampling.** EBMs keep the simplest formulation of density‚Äîan unnormalized energy function $U_\theta(x)$‚Äîbut delegate sampling to iterative stochastic processes like **Langevin dynamics** or **Contrastive Divergence**. The model defines:
  
  $$
  p_\theta(x)=\frac{1}{Z_\theta}\exp(-U_\theta(x)).
  $$

  Because $Z_\theta$ is intractable, training relies on estimating gradients using samples from the model distribution, obtained via slow MCMC chains. This approach retains full modeling flexibility but sacrifices sampling speed and stability.


---

<h1 id="section3" style="color: #1E40AF; font-size: 26px; font-weight: bold; text-decoration: underline;">3. The Rise of Diffusion Models</h1>


Despite their conceptual elegance and individual successes, the classical families of generative models share a common limitation: **they struggle to simultaneously ensure expressivity, stability, and tractable likelihoods**.

- VAEs approximate densities but pay the price of **blurry reconstructions** due to their variational relaxation.

- Flow models achieve **exact likelihoods**, yet the requirement of invertible, volume-preserving mappings imposes severe **architectural rigidity**.

- GANs generate **sharp but unstable** samples, suffering from mode collapse and non-convergent adversarial dynamics.

- EBMs enjoy the most general formulation but depend on **inefficient MCMC sampling** and are notoriously difficult to train at scale.

These obstacles reveal a deeper dilemma: *a good generative model must learn both an expressive data distribution and an efficient way to sample from it*, but existing paradigms could rarely achieve both at once.

It was precisely this tension‚Äîbetween **tractable density estimation** and **efficient, stable sampling**‚Äîthat motivated the birth of diffusion-based generative modeling.
Diffusion models approach generation from an entirely different angle: rather than directly parameterizing a complex distribution, they **construct it through a gradual denoising process**, transforming simple noise into structured data via a sequence of stochastic or deterministic dynamics.
In doing so, diffusion models inherit the statistical soundness of energy-based methods, the stability of likelihood-based training, and the flexibility of implicit generation‚Äîthereby overcoming the long-standing trade-offs that constrained previous approaches.


---

<h1 id="section4" style="color: #1E40AF; font-size: 26px; font-weight: bold; text-decoration: underline;">4. The Dual Origins of Diffusion Sampling</h1>


Before diffusion models became a unified SDE framework, two independent lines of thought evolved in parallel: One sought to model the gradient of data density in continuous space; the other built an explicit discrete Markov chain that learned to denoise. Both added noise, both removed it‚Äîyet for profoundly different reasons.


---

<h1 id="section4.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">4.1 The Continuous-State Perspective: Score-Based Generative Models</h1>


The first paradigm was rooted in a fundamental question: if we had a function that could always point us toward regions of higher data probability, could we generate data by simply "climbing the probability hill"?

---


<h1 id="section4.1.1" style="color: #1D4ED8; font-size: 21px; font-weight: bold; text-decoration: underline;">4.1.1 A Radical Shift ‚Äî Learning the Gradient of data density</h1>


Directly modeling $p(x)$ is both theoretically and practically challenging, traditional generative models bypass these challenges through approximation or network structure constraints, but this also limits the model's performance. One idea is that we do not learn the probability density, but instead learn the gradient of the probability density (also known as **score function**).

The **score function**, defined as

$$
s(x)=\nabla_x\log p(x) 
$$

As shown in the following figure, score function is a vector field which points toward regions of higher data probability. Learning this vector field‚Äîrather than the full density‚Äîoffers two decisive advantages.


<div style="display: block; margin: 0 auto; text-align: center;">
  <img src="/assets/images/2025-10-01-blog-post/score_vs_probability.jpg" alt="outline" style="display: block; margin: 0 auto;" width="900" height="700">
  <div style="margin-top: 8px; font-size: 16px; color: #666; font-style: Times New Roman;">
     Figure 3: Probability Density vs. Score Function
  </div>
</div>

<br>


- **Independence from the Normalization Constant**. Taking the gradient of $\log p(x)$ removes the partition function:
  
  $$
  \nabla_x\log p(x)
  =\nabla_x\log \exp{(f_{\theta}(x))}-\nabla_x\log Z
  =\nabla_x\log f_{\theta}(x),
  $$
  
  because $Z$ is constant w.r.t. $x$. Thus, we can learn meaningful gradients without ever computing $Z$.

- **Direction Instead of Magnitude**. The score tells us **which direction** in data space increases probability density the fastest. It defines a **probability flow** that guides samples toward high-density regions‚Äîakin to an energy-based model whose energy is $$U(x)=-\log p(x)$$.

- **The most important one**, is that the probability density and the score provide almost the same useful information, that's because the score function and the probability density function can be converted between each other: Given the probability density, we can obtain the score by taking the derivative; conversely, given a score, we can recover the probability density by computing integrals.




---

<h1 id="section4.1.2" style="color: #1D4ED8; font-size: 21px; font-weight: bold; text-decoration: underline;">4.1.2 Score Matching Generative Model</h1>



At the heart of this approach lies a powerful mathematical object: the **score function**, defined as the gradient of the log-probability density of the data, $\nabla_x \log p_{\text{data}}(x)$. Intuitively, the score at any point $x$ is a vector that points in the direction of the steepest ascent in data density. To calculate the score function of any input, we train a neural network $s_{\theta}(x)$ (score model) to learn the score

$$
s_{\theta}(x) \approx \nabla_x \log p_{\text{data}}(x)\label{eq:1}
$$

The loss function is to minimize the Fisher divergence between the true data distribution and the model predicted output:

$$
\mathcal{L}(\theta) = \mathbb{E}_{x \sim p_{\text{data}}}\left[
\big\|\,s_{\theta}(x) - \nabla_x \log p_{\text{data}}(x)\,\big\|_2^2
\right]\label{eq:2}
$$


The challenge, however, is that we do not know the true data distribution $p_{\text{data}}(x)$. This is where **Score Matching** comes in [^Aapo]. Hyv√§rinen showed that via **integration by parts** (under suitable boundary conditions), the objective (equation \ref{eq:2}) can be rewritten in a form **only involving** the model‚Äôs parameters:

$$
\begin{align}
\mathcal{L}_{\text{SM}}(\theta) & = \mathbb{E}_{x \sim p_{\text{data}}} \left[ \frac{1}{2} \| s_\theta(x) \|^2 + \nabla_x \cdot s_\theta(x) \right] \\[10pt]
& \approx \frac{1}{N}\sum_{i=1}^{N} \left[ \frac{1}{2} \| s_\theta(x_i) \|^2 + \nabla_x \cdot s_\theta(x_i) \right] 
\end{align}
$$

where $\nabla_x \cdot s_\theta(x) = {\text{trace}(\nabla_x s_\theta(x))}$ is the divergence of the score field. However, SM is not scalable especially for high-dimension data points, because the second term is the jacobin of score model. 

For this purpose, Vincent introduced denoising score matching (DSM) [^Vincent] , by adding Gaussian noise to the real data $x$, the score model becomes predicting the score field of noised data $\tilde{x} = x + \sigma$.

$$
s_{\theta}(\tilde{x}, \sigma) \approx \nabla_{\tilde{x}} \log p_{\sigma}(\tilde{x})\label{eq:5}
$$

where $p_{\sigma}$ is the data distribution convolved with Gaussian noise of scale $\sigma$, it is easy to verify that predicting $\nabla_{\tilde x} \log p_{\sigma}(\tilde x)$ is equivalent to predict $\nabla_{\tilde{x}} \log p_{\sigma}(\tilde{x} \mid x)$.

$$
\begin{align}
 & \mathbb{E}_{\tilde{x} \sim p_{\sigma}(\tilde{x})}\left[\big\|\,s_{\theta}(\tilde{x}, \sigma) - \nabla_{\tilde x} \log p_{\sigma}(\tilde x)\,\big\|_2^2\right]\label{eq:6} \\[10pt] 
 = \quad & \mathbb{E}_{x \sim p_{\text{data}}(x)}\mathbb{E}_{\tilde{x} \sim p_{\sigma}(\tilde{x} \mid x)}\left[\big\|\,s_{\theta}(\tilde{x}, \sigma) - \nabla_{\tilde{x}} \log p_{\sigma}(\tilde{x} \mid x)\,\big\|_2^2\right] + \text{const}\label{eq:7}
 \end{align}
$$

When $\sigma$ is small enough, $p_{\text{data}}(x) \approx p_\sigma(\tilde x)$. Optimizing Formula \ref{eq:7} does not require knowing the true data distribution, while also avoiding the computation of the complex Jacobian determinant.

$$
\begin{align}
\nabla_{\tilde{x}} \log p_{\sigma}(\tilde{x} \mid x) & = \nabla_{\tilde{x}} \log \frac{1}{\sqrt{2\pi}\sigma} {\exp}\left( -\frac{(\tilde{x}-x)^2}{2\sigma^2}\right)  = -\frac{(\tilde{x}-x)}{\sigma^2}
\end{align}
$$

---

<h1 id="section4.1.3" style="color: #1D4ED8; font-size: 21px; font-weight: bold; text-decoration: underline;">4.1.3 NCSN and Annealed Langevin Dynamics</h1>


The first major practical realization of this idea was the **Noise-Conditional Score Network (NCSN)** [^Song_2019]. However, the authors found that the generated samples by solving \ref{eq:7} with a single small $ \sigma $ are of poor quality, the core issue is that real-world data often lies on a complex, low-dimensional manifold (high-density areas), in the vast empty spaces between data points, the score is ill-defined and difficult to estimate (low-density areas), 
so low-density regions are underrepresented, leading to poor generalization by the network in those areas. Their solution was ingenious: perturb the data with Gaussian noise of varying magnitudes $\sigma$ ($ \sigma_1 > \sigma_2 > \dots > \sigma_K > 0 $). 

- **Large $ \sigma_i $**: The distribution is highly smoothed, covering the global space with small, stable scores in low-density areas (easier to learn).
  
- **Small $ \sigma_i $**: Focuses on local refinement, closer to $ p_{\text{data}} $.
 
The loss function now can be rewritten as:  

$$
\mathcal{L}(\theta) = \mathbb{E}_{\sigma \sim \mathcal{D}}\mathbb{E}_{x \sim p_{\text data}(x)}\mathbb{E}_{\tilde{x} \sim p_{\sigma}(\tilde{x} \mid x)}\left[\big\|\,s_{\theta}(\tilde{x}, \sigma) - \nabla_{\tilde{x}} \log p_{\sigma}(\tilde{x} \mid x)\,\big\|_2^2\right]\label{eq:8}
$$

With $\lambda{(\sigma)}$ balancing gradients across noise scales and $\mathcal{D}$ is a distribution over $\sigma$. To generate a sample, NCSN employed **Annealed Langevin Dynamics** (ALD) [^Song_2019].  LD [^Parisi] [^Grenander] treats sampling as **stochastic gradient ascent on log-density** with Gaussian noise injected to guarantees to converge to the target distribution. ALD is an iterative sampling process:

1. Start with a sample drawn from pure noise, corresponding to a very high noise level $\sigma_{\text large}$.

2. Iteratively update the sample using the Langevin dynamics equation:
   
   $$
   x_{i+1} \leftarrow x_i + \alpha  s_{\theta}(x_i, \sigma_i) + \sqrt{2\alpha}z_i
   $$
   
   where $\alpha$ is a step size and $z_i$ is fresh Gaussian noise. This update consists of a "climb" along the score-gradient and a small injection of noise to encourage exploration.
   
3. Gradually decrease (or "anneal") the noise level $\sigma_i$ from large to small. This process is analogous to starting with a blurry, high-level structure and progressively refining it with finer details until a clean sample emerges.



---

<h1 id="section4.1.4" style="color: #1D4ED8; font-size: 21px; font-weight: bold; text-decoration: underline;">4.1.4 Historical Limitations and Enduring Legacy</h1>


The NCSN approach was groundbreaking, but it had its limitations. The Langevin sampling process was inherently **stochastic** due to the injected noise $z$ and **slow**, requiring many small steps to ensure stability and quality. The annealing schedule itself was often heuristic.

However, its legacy is profound. NCSN established the **score function** as a fundamental quantity for generative modeling and introduced the critical technique of **conditioning on noise levels**. It provided the continuous-space intuition that would become indispensable for later theoretical breakthroughs.

---

<h1 id="section4.2" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">4.2 The Discrete-Time Perspective: Denoising Diffusion Probabilistic Models</h1>

Running parallel to the score-based work, a second paradigm emerged, built upon the more structured and mathematically elegant framework of **Markov chains**.


---

<h1 id="section4.2.1" style="color: #1D4ED8; font-size: 21px; font-weight: bold; text-decoration: underline;">4.2.1 The Core Idea: An Elegant Markov Chain</h1>

Independently, Ho et al. [^ddpm] proposed a probabilistic approach that traded continuous dynamics for a discrete, analytically tractable Markov chain. Instead of estimating scores directly, DDPM proposed a fixed, two-part process.

-  **Forward (Diffusion) Process:** Start with a clean data sample $x_0$. Gradually add a small amount of Gaussian noise at each discrete timestep $t$ over a large number of steps $T$ (typically 1000). This defines a Markov chain $x_0, x_1, \dots, x_T$ where $x_T$ is almost indistinguishable from pure Gaussian noise. This forward process, $q(x_t \mid x_{t-1})$, is fixed and requires no learning.

    $$
    q(x_t \mid x_{t-1}) = \mathcal{N}(x_t;\,\sqrt{\alpha_t}x_{t-1}, (1-\alpha_t)I) 
    $$

-  **Reverse (Denoising) Process:** Given a noisy sample $x_t$, how do we predict the slightly less noisy $x_{t-1}$?  The goal is to learn a reverse model $p_{\theta}(x_{t-1} \mid x_t)$ to inverts this chain. 


---

<h1 id="section4.2.2" style="color: #1D4ED8; font-size: 21px; font-weight: bold; text-decoration: underline;">4.2.2 Forward Diffusion and Reverse Denoising</h1>

According to previous post, the solution of DDPM aims to maximize the log-likelihood of $p_{\text data}(x_0)$, which can be reduced to maximizing the variational lower bound, and maximizing the variational lower bound can further be derived as minimizing the KL divergence between $p_{\theta}(x_{t-1} \mid x_t)$ and $q(x_{t-1} \mid x_t, x_0)$.


$$
 \mathcal{L}_\text{ELBO} = \mathbb{E}_q \left[ \log p_\theta(x_0 \mid x_1) - \log \frac{q(x_{T} \mid x_0)}{p_\theta(x_{T})} - \sum_{t=2}^T \log \frac{q(x_{t-1} \mid x_t, x_0)}{p_\theta(x_{t-1} \mid x_t)} \right]
$$

The true posterior $q(x_{t-1} \mid x_t, x_0)$ can be computed using Bayes‚Äô rule and the Markov property of the forward chain:

$$
q(x_{t-1} \mid x_t, x_0) = \frac{q(x_t \mid x_{t-1}) q(x_{t-1} \mid x_0)}{q(x_t \mid x_0)}
$$

Since all three terms in the right sides are Gaussian distribution, the posterior $ q(x_{t-1} \mid x_t, x_0) $ is also Gaussian:


$$
q(x_{t-1} \mid x_t, x_0) = \mathcal{N}\left( x_{t-1}; \tilde{\mu}_t(x_t, x_0), \tilde{\beta}_t I \right)
$$

with mean and and variance:



$$
\tilde{\mu}_t(x_t, x_0) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon \right) \quad , \quad \tilde{\beta}_t = \frac{(1 - \bar{\alpha}_{t-1}) \beta_t}{1 - \bar{\alpha}_t}\label{eq:14}
$$

Since our goal is to approximate distribution $ p_{\theta}(x_{t-1} \mid x_t) $ using distribution $q(x_{t-1} \mid x_t, x_0)$, this means that our reverse denoising process must also follow a Gaussian distribution, with the same mean and variance. Because the added noise in the reverse process is unknown, the training objective of DDPM is to predict this noise. We use $\epsilon_{\theta}(x_t, t)$ to represent the predicted noise output, substituting $ \epsilon $ with the noise predicted $\epsilon_{\theta}(x_t, t)$:


$$
p_{\theta}(x_{t-1} \mid x_t) = \mathcal{N}\left( x_{t-1}; {\mu}_{\theta}(x_t, x_0), {\beta}_{\theta} I \right)
$$

where:


$$
{\mu}_{\theta}(x_t, x_0) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right) \ \  , \ \  {\beta}_{\theta}=\tilde{\beta}_t = \frac{(1 - \bar{\alpha}_{t-1}) \beta_t}{1 - \bar{\alpha}_t}
$$


Sampling is the direct inverse of the forward process. One starts with pure noise $x_T \sim \mathcal{N}(0, I)$  and iteratively applies the learned reverse step for $ t = T, T-1, \dots, 1$, using the noise prediction $\epsilon_{\theta}(x_t, t)$ at each step to denoise the sample until a clean $x_0$ is obtained.

$$
x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right) + \sqrt{\frac{(1 - \bar{\alpha}_{t-1}) \beta_t}{1 - \bar{\alpha}_t}} \epsilon
$$


---

<h1 id="section4.2.3" style="color: #1D4ED8; font-size: 21px; font-weight: bold; text-decoration: underline;">4.2.3 Theoretical Elegance and Practical Bottleneck</h1>

DDPMs offered a strong theoretical foundation, stable training, and state-of-the-art sample quality. However, this came at a steep price: **the curse of a thousand steps**. The model's theoretical underpinnings relied on the Markovian assumption and small step sizes, forcing the sampling process to be painstakingly slow and computationally expensive.


---

<h1 id="section4.3" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">4.3 The Initial Convergence: Tying the Two Worlds Together</h1>


For a time, these two paradigms‚Äîone estimating a continuous gradient field, the other reversing a discrete noise schedule‚Äîseemed distinct. Yet, a profound and simple connection lay just beneath the surface. It was shown that the two seemingly different objectives were, in fact, two sides of the same coin.

The score function $s(x_t, t)$ at a given noise level is directly proportional to the optimal noise prediction $\epsilon(x_t, t)$:

$$
s_{\theta}(x_t, t) = \nabla_{x_t} \log p(x_t) = -\frac{\epsilon_{\theta}(x_t, t)} { \sigma_t}
$$

where $\sigma_t$ is the standard deviation of the noise at time $t$.

This equivalence is beautiful. **Predicting the noise is mathematically equivalent to estimating the score.** The score-based view provides the physical intuition of climbing a probability landscape, while the DDPM view provides a stable training objective and a concrete, discrete-time mechanism.

With this link established, the two parallel streams began to merge into a single, powerful river. This convergence set the stage for the next major leap: breaking free from the rigid, one-step-at-a-time sampling of DDPM, and ultimately, the development of a unified SDE/ODE theory that could explain and improve upon both.

<br>

---

<h1 id="sectionPartII" style="color: #1E3A8A; font-size: 30px; font-weight: bold; text-decoration: underline;">Part II ‚Äî Fast Sampling without Retraining</h1>

Inference-time acceleration (training-free) treats a **pretrained** diffusion/score model as a fixed oracle and speeds up generation solely by changing the *sampler*‚Äîthe time discretization, update rule, and numerical integration strategy‚Äîwithout modifying parameters or re-running training. Under this paradigm, we keep the same network evaluations $$f_\theta(x_t, t, c)$$ but reduce the number of required evaluations (**NFE**) and improve stability by exploiting better discretizations (e.g., deterministic DDIM-style updates), multistep ‚Äúpseudo-numerical‚Äù schemes (e.g., PNDM/PLMS), and higher-order ODE/SDE solvers tailored to diffusion dynamics (e.g., DEIS, DPM-Solver(++), UniPC). 

Conceptually, these methods replace a long sequence of small Euler-like steps with fewer, more accurate steps by injecting local polynomial/exponential approximations, error-correcting history terms, or probability-flow‚Äìconsistent updates. As a result, they offer a plug-and-play speed‚Äìquality trade-off: the model and training pipeline remain unchanged, and faster sampling is achieved purely through improved inference-time computation.



---

<h1 id="section5" style="color: #1E40AF; font-size: 26px; font-weight: bold; text-decoration: underline;">5. Breaking the Markovian Chain with DDIM</h1>


The stunning quality of DDPMs came at a steep price: the rigid, step-by-step Markovian chain. This constraint, while theoretically elegant, was a practical nightmare, demanding a thousand or more sequential model evaluations for a single image. The field desperately needed a way to accelerate this process without a catastrophic loss in quality. The answer came in the form of **Denoising Diffusion Implicit Models (DDIM)** [^ddim], a clever reformulation that fundamentally altered the generation process.


---

<h1 id="section5.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">5.1 From Stochastic Denoising to Deterministic Prediction</h1>




The core limitation of the DDPM sampler lies in its definition of the reverse process, which models $p_{\theta}(x_{t-1} \mid x_t)$. This conditional probability is what creates the strict, one-step-at-a-time dependency.


The DDIM paper posed a brilliant question: what if we don't try to predict $x_{t-1}$ directly? What if, instead, we use our noise-prediction network $\epsilon_{\theta}(x_t, t)$ to make a direct guess at the **final, clean image $x_0$**? This is surprisingly straightforward. Given $x_t$ and the predicted noise $\epsilon_{\theta}(x_t, t)$, we can rearrange the forward process formula to solve for an estimated $x_0$, which we'll call $\hat{x}_0$:

$$
\hat{x}_0 = \frac{(x_t - \sqrt{1 - \bar{\alpha}_t} \, \epsilon_{\theta}(x_t, t))}{\sqrt{\bar{\alpha}_t}}
$$

This single equation is the conceptual heart of DDIM. By first estimating the final destination $x_0$, we are no longer bound by the previous step. We have a "map" that points from our current noisy location $x_t$ directly to the origin of the trajectory. This frees us from the Markovian assumption and opens the door to a much more flexible generation process.

---

<h1 id="section5.2" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">5.2 The Freedom to Jump: Non-Markovian Skip-Step Sampling</h1>


With an estimate of pred $$\hat{x}_0$$ in hand, DDIM constructs the next sample $x_{t-1}$ in a completely different way. It essentially says: "Let's construct $x_{t-1}$ as a weighted average of three components":

- **The "Clean Image" Component**: The estimated final image predicted $\hat x_0$, pointing towards the data manifold.

- **The "Noise Direction" Component**: The direction pointing from the clean image $x_0$ back to the current noisy sample $x_t$, represented by $\epsilon_{\theta}(x_t, t)$.

- **A Controllable Noise Component**: An optional injection of fresh random noise.

The DDIM update equation is as follows:

$$
x_{t-1} = \sqrt{\bar \alpha_{t-1}}\,{\hat x_0} + \sqrt{1-\bar \alpha_{t-1}-\sigma_t^2}\, \epsilon_{\theta}(x_t, t) + \sigma_t \, z_t\label{eq:20}
$$

Here, $$z_t$$ is random noise, and $\sigma_t$ is a new hyperparameter that controls the amount of stochasticity. $\sigma_t = \eta \tilde{\beta}_t$, where $\tilde{\beta}_t$ is defined in DDPM (Equation \ref{eq:14})

This formulation \ref{eq:20} is non-Markovian because the calculation of $x_{t-1}$ explicitly depends on predicted $x_0$, which is an estimate of the trajectory's origin, not just the immediately preceding state $x_t$. Because this process is no longer Markovian, the "previous" step doesn't have to be $t-1$. We can choose an arbitrary subsequence of timesteps from the original $1, \dots, T$, for example, $1000, 980, 960, ..., 20, 0$. Instead of taking 1000 small steps, we can now take 50 large **jumps**. At each jump from $t$ to a much earlier $s < t$, the model predicts $x_0$ and then uses the DDIM update rule to deterministically interpolate the correct point $x_s$ on the trajectory. This "skip-step" capability was a game-changer. It allowed users to drastically reduce the number of function evaluations (NFEs) from 1000 down to 50, 20, or even fewer, providing a massive speedup with only a minor degradation in image quality.




Formula \ref{eq:20} also reveals an important theory: for any $x_{t-1}$, the marginal probability density obtained through DDIM sampling is consistent with the marginal probability density of DDPM.

$$
q(x_{t-1} \mid x_0 ) = \mathcal{N}(x_{t-1}; \sqrt{\bar{\alpha}_{t-1}} {x_0}, (1-\bar{\alpha}_{t-1})I)
$$


This is why DDIM can take large jumps and still produce high-quality images using a model trained for the one-step DDPM process.

---

<h1 id="section5.3" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">5.3 The $\eta$ Parameter: A Dial for Randomness</h1>


DDIM introduced another powerful feature: a parameter denoted as $\eta$ that explicitly controls the stochasticity of the sampling process.

- **$\eta = 0$: (Deterministic DDIM)**: When $\eta = 0$, the random noise term in the update rule is eliminated. For a given initial noise $x_T$, the sampling process will always produce the exact same final image `x_0`. This is the "implicit" in DDIM's name, as it defines a deterministic generative process. This property is incredibly useful for tasks that require reproducibility or image manipulation (like SDEdit), as the generation path is fixed.


- **$\eta = 1$ (Stochastic DDIM)**: When $\eta = 1$, DDIM adds a specific amount of stochastic noise at each step. It was shown that this choice makes the marginal distributions $p(x_t)$ match those of the original DDPM sampler. It behaves much like DDPM, offering greater sample diversity at the cost of reproducibility.

- **$0 < \eta < 1$ (Hybrid Sampling)**: Values between 0 and 1 provide a smooth interpolation between a purely deterministic path and a fully stochastic one, giving practitioners a convenient "dial" to trade off between diversity and consistency.

---

<h1 id="section5.4" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">5.4 Bridging to a Deeper Theory: The Lingering Question</h1>

DDIM was a massive practical leap forward. It provided the speed that was desperately needed and introduced the fascinating concept of a deterministic generative path. However, its derivation, while mathematically sound, was rooted in the discrete-time formulation of DDPM. The deterministic path ($\eta=0$) worked exceptionally well, but it raised a profound theoretical question:

**What is this deterministic path? If a continuous process underlies generation, can we describe this path more fundamentally?**

The success of deterministic DDIM strongly hinted that the stochasticity of Langevin Dynamics and DDPM sampling might not be strictly necessary. It suggested the existence of a more fundamental, deterministic flow from noise to data. Explaining the origin and nature of this flow was the next great challenge.

This question sets the stage for our next chapter, where we will discover the grand, unifying framework of **Stochastic and Ordinary Differential Equations (SDEs/ODEs)**, which not only provides the definitive answer but also reveals that Score-Based Models and DDPMs were two sides of the same mathematical coin all along.

---

<h1 id="section6" style="color: #1E40AF; font-size: 26px; font-weight: bold; text-decoration: underline;">6. The Unifying Framework of SDEs and ODEs</h1>

The insight behind DDIM‚Äîthat sampling can follow a deterministic path derived from a learned vector field‚Äîopens the door to a more general formulation: **diffusion models can be expressed as continuous-time stochastic or deterministic processes**. In this chapter, we formalize that insight by introducing the **Stochastic Differential Equation (SDE)** and the equivalent **Probability Flow Ordinary Differential Equation (ODE)** perspectives. This unified view provides the theoretical bedrock for nearly all subsequent developments in diffusion sampling, including adaptive solvers, distillation, and consistency models.


---

<h1 id="section6.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">6.1 From Discrete Steps to Continuous Time</h1>

The core insight is to re-imagine the diffusion process not as $T$ discrete steps, but as a continuous process evolving over a time interval, say $t \in \[0, T\]$. In this view, $x_0$ is the clean data at $t=0$, and $x_T$ is pure noise at $t=T$. The forward process is no longer a chain but a trajectory governed by a **Stochastic Differential Equation (SDE)** that continuously injects noise.

$$
dx_t=f(x_t, t)dt+g(t)dw_t
$$

Where $f(x_t, t)$ is a **drift** term, which describes the deterministic part of the SDE's evolution. $g(t)$ is a **diffusion** coefficient, which scales the magnitude of the random noise. $dw_t$ is a standard Wiener process, $dw_t=\sqrt{dt}\epsilon,\,\epsilon \sim \mathcal{N}(0, I)$. 

This continuous formulation is incredibly powerful because it allows us to leverage the mature and rigorous mathematics of stochastic calculus. More importantly, it was proven by Song et al. in their seminal work that there exists a corresponding **reverse-time SDE** that can transform pure noise at $t=T$ back into data at $t=0$. This reverse SDE is the true, continuous-time "master equation" for generation.

$$
dx_t = \left[ f(x_t, t) - g(t)^2 s_{\theta}(x_t, t) \right] dt + g(t) d{\bar w}_t
$$

Where $s_{\theta}(x_t, t)$ is the score function, learned by the neural network. $d{\bar w}_t$ An infinitesimal "kick" from a standard Wiener process (random noise).

This single equation serves as the grand unifier for the stochastic samplers from section [2](#section2). Both **Annealed Langevin Dynamics (from NCSN)** and the **DDPM sampling procedure** can be shown to be different numerical discretization schemes for this exact same SDE. They were, all along, just two different ways to approximate the solution to one underlying continuous process.


---

<h1 id="section6.2" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">6.2 The Probability Flow ODE: Discovering the Deterministic Path</h1>


The reverse SDE provides a complete picture for stochastic generation, but what about the deterministic path discovered by DDIM? This is where the second part of the unified theory comes into play.

For any given SDE that describes the evolution of individual random trajectories, there exists a corresponding **Ordinary Differential Equation (ODE)** that describes the evolution of the *probability density* of those trajectories. This is known as the **Probability Flow (PF) ODE**. While the SDE traces a jagged, random path, the PF-ODE traces a smooth, deterministic "flow" of probability from the noise distribution to the data distribution.

The PF-ODE corresponding to the reverse SDE is:

$$
dx_t = \left[ f(x_t, t) - \frac{g(t)^2}{2} s_{\theta}(x_t, t) \right] dt 
$$

Notice the two critical differences from the SDE: 
1. The diffusion term is gone. There is no more stochastic noise injection. The process is entirely **deterministic**.
2. The score term $s_{\theta}$ is scaled by $\frac{1}{2}$. This factor arises directly from the mathematical conversion (via the Fokker-Planck equation) from the SDE to its deterministic counterpart.

This ODE is the definitive answer to the puzzle of DDIM. The deterministic path ($\eta = 0$) that DDIM so effectively approximates is, in fact, a trajectory of this very Probability Flow ODE. The DDIM update rule is a specific (and quite effective) numerical solver for this equation.



---

<h1 id="section6.3" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">6.3 A New Paradigm for Sampling</h1>



The SDE/ODE framework marks a moment of theoretical completion and a paradigm shift. We now have a single, coherent view that elegantly encompasses all previous methods:




This new perspective immediately begs the next question. The world of numerical analysis has spent over a century developing sophisticated methods for solving ODEs‚Äîfrom simple Euler methods to high-order Runge-Kutta schemes. Can we simply apply these off-the-shelf, classical solvers to the PF-ODE and achieve even greater speed and accuracy?

As we will see in the next chapter, the answer is surprisingly complex. The unique properties of the diffusion PF-ODE present significant challenges to standard numerical methods, necessitating the development of a new class of specialized, "diffusion-aware" solvers.





---

<h1 id="section7" style="color: #1E40AF; font-size: 26px; font-weight: bold; text-decoration: underline;">7. High-Order PF-ODE Solver in Diffusion Models</h1>


The transition from stochastic sampling (via SDEs) to deterministic sampling (via the Probability Flow ODE) transformed diffusion generation into a **numerical integration problem**. Once the evolution of data can be written as an ODE,

$$
\frac{d\mathbf{x}}{dt} = \mathbf{f}(t,\mathbf{x}) - \frac{g^2(t)}{2}\, s_\theta(\mathbf{x}, t),
$$

the task of generating a sample becomes equivalent to **integrating this equation backward in time**, from random noise at $t = T$ to the data manifold at $t = 0$.

At first glance, this seems to open the door to the rich ecosystem of traditional numerical solvers, such as Euler, Heun, Runge‚ÄìKutta, Adams‚ÄìBashforth, and so on. However, diffusion sampling presents unique challenges that make these classical algorithms inadequate without adaptation.



---

<h1 id="section7.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">7.1 Why Classical ODE Solvers Fall Short</h1>


The PF-ODE is not a conventional physics or engineering system but a learned dynamical system defined by a neural network. Classical solvers assume smooth, analytic derivatives and prioritize **path fidelity**‚Äîensuring each intermediate step closely matches the true trajectory. Diffusion sampling, by contrast, only requires **endpoint fidelity**: as long as the final state $x_0$ lies on the data manifold, large intermediate deviations are acceptable.

Moreover, evaluating the derivative $\mathbf{f}(t,\mathbf{x})$ involves a full forward pass of a deep neural network (often a U-Net), making every function evaluation costly. High-order solvers that need multiple evaluations per step rapidly become inefficient.

Compounding this, diffusion dynamics are **stiff**‚Äîdifferent dimensions evolve at dramatically different rates‚Äîand the learned score field $s_\theta$ is only an approximation, introducing noise and bias that can destabilize high-order methods. These characteristics make standard numerical integration theory ill-suited to the diffusion context.


---

<h1 id="section7.2" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">7.2 The Rise of Specialized Solvers</h1>

These difficulties led to a new family of **diffusion-aware numerical solvers**, designed specifically to balance accuracy, stability, and computational budget. Instead of preserving local precision at every point, these methods explicitly optimize for **global fidelity of the terminal sample** under limited NFEs (number of function evaluations).

This insight underpins the design of solvers such as **DPM-Solver**, **DPM-Solver++**, **DEIS**, **UniPC**, and many others, which adapt principles from exponential integrators, predictor‚Äìcorrector frameworks, and adaptive step-size control to the peculiarities of PF-ODE dynamics. Collectively, they represent a crucial bridge between differential-equation theory and practical, efficient diffusion sampling.





Because this topic encompasses substantial theoretical and algorithmic depth‚Äîincluding solver derivations, stability analyses, and empirical comparisons‚Äîit is discussed separately in a dedicated article. For the complete treatment of high-order PF-ODE solvers, please refer to the following 


- [High-Order PF-ODE Solver in Diffusion Models](https://innovation-cat.github.io/posts/2025/06/ODE-Solver/).

<br>


---

<h1 id="sectionPartIII" style="color: #1E3A8A; font-size: 30px; font-weight: bold; text-decoration: underline;">Part III ‚Äî Distillation for Fast Sampling</h1>



Although higher-order ODE solvers (e.g., DPM-Solver++, UniPC) can reduce the number of denoising steps from hundreds to fewer than twenty, the computational cost remains substantial for high-resolution image synthesis and real-time applications. The ultimate goal of sampling acceleration is to achieve **few-step** or even **single-step** generation without sacrificing fidelity or diversity.

Distillation-based approaches address this challenge by **compressing the multi-step diffusion process into a student model** that can approximate the teacher‚Äôs distribution within only one or a few evaluations. Unlike numerical solvers, distillation learns a direct functional mapping from noise to data, thereby converting iterative denoising into an amortized process.




---

<h1 id="section8" style="color: #1E40AF; font-size: 26px; font-weight: bold; text-decoration: underline;">8. Trajectory-Based Distillation</h1>

**Trajectory distillation** is built on the principle **distill how the teacher moves**. Instead of matching only final samples, it compresses the teacher‚Äôs multi-step sampler into fewer, larger steps by teaching the student to reproduce the teacher‚Äôs **transition operator** $$x_{t_i}!\mapsto x_{t_{j}}$$ (or an entire short segment of the trajectory). 

The design philosophy is solver-centric: sampling is an algorithm, and distillation is a way to learn an **approximate integrator / flow map** that preserves the teacher‚Äôs path properties (stability, guidance behavior, error cancellation) under reduced NFE. Practically, this appears as step-halving curricula, multi-step-to-single-step regression targets, or direct learning of coarse-grained transitions. 

---


<h1 id="section8.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">8.1 Progressive Distillation</h1>



**Progressive Distillation (PD)** [^pd] compresses a pretrained "teacher" diffusion sampler that runs in $T$ steps into a "student" sampler that runs in $T/2$ steps, then recursively repeats the procedure $$(T/2 \rightarrow T/4 \rightarrow \dots)$$, typically reaching **4‚Äì8 steps** with minimal quality loss.

Let $$\{t_n\}_{n=0}^{N}$$ be a decreasing time/$\sigma$ grid with $t_0=0$ and $t_N=T$. Denote the teacher‚Äôs one-step transition (DDIM/ODE or DDPM/SDE) by

$$
x_{t_{n-1}}^{\text{T}}=\Phi_\theta(x_{t_n};\,t_n \to t_{n-1}),
$$

where $\Phi_\theta$ is an ODE Solver. Its **two-step composite**

$$
x_{t_{n-2}}^{\text{T}} = \Phi_\theta^{(2)}(x_{t_n};\,t_n \to t_{n-2})
=
\Phi_\theta\big(\,\Phi_\theta(x_{t_n};\, t_n \to t_{n-1});\,t_{n-1}\to t_{n-2}\big).
$$

A student $f$ with parameters $\phi$ is trained to **replace two teacher steps with one**:

$$
x_{t_{n-1}}^{\text{S}} =
f_\phi(x_{t_n};\,t_n \to t_{n-1})
\approx
\Phi_\theta^{(2)}(x_{t_n};\,t_n \to t_{n-2})
$$

A visualization of progressive distillation algorithm is shown as follows.

<div style="display: block; margin: 0 auto; text-align: center;">
  <img src="/assets/images/2025-10-01-blog-post/pd.jpg" alt="outline" style="display: block; margin: 0 auto;" width="900" height="700">
  <div style="margin-top: 8px; font-size: 16px; color: #666; font-style: Times New Roman;">
     Figure 4: Progressive Distillation Algorithm
  </div>
</div>

<br>
 
---


<h1 id="section8.1.1" style="color: #1D4ED8; font-size: 20px; font-weight: bold; text-decoration: underline;">8.1.1 Target Construction and Loss Design</h1>

Training pairs are **self-supervised** by the teacher: Sample $x_{t_n}$ either from $\mathcal N(0,I)$ or by forward noising data $$x_0\sim p_{\text{data}}$$ through $$q(x_{t_n} \mid x_0)$$. Then, run **two** teacher steps to get the target state
 
$$
   x_{t_{n-2}}^{\text{T}}=\Phi_\theta^{(2)}(x_{t_n};\,t_n\to t_{n-2}) = x_{t_n}
+
\underbrace{\Delta^{(2)}_\theta(x_{t_n};\,t_n \to t_{n-2})}_{\text{Integral part from} \  t_{n} \text{ to } t_{n-2}}.
$$

PD can utilize different targets that we expect the student model to predict in one step from $x_t$, each corresponding to a distinct loss function. $x_{t_{n-2}}^{\text{T}$ is  a simple and straightforward target that we expect the student model to predict, with this as the goal, we introduce the PD regression loss.
 


$$
\mathcal L_{\text{state}} = \mathbb E_{x_{t_n}} \big\| f_\phi(x_{t_n};\,t_n\to t_{n-1}) - x_{t_{n-2}}^{\text{T}}
\big\|_2^2
$$



---


<h1 id="section8.1.2" style="color: #1D4ED8; font-size: 20px; font-weight: bold; text-decoration: underline;">8.1.2 Strengths and Limitations</h1>

Progressive Distillation offers an elegant and empirically robust pathway toward accelerating diffusion sampling. Its main strengths lie in its stability, modularity, and data efficiency.
- **Data-free** (or data-light): supervision comes from the teacher sampler itself.
- **Stable and modular**: each stage is a local two-step merge problem.
- **Excellent trade-off** at **4‚Äì8 steps**: large speedups with negligible FID/IS degradation.
- **Architecture-agnostic**: applies to UNet/DiT backbones and both pixel/latent spaces.

However, PD also faces fundamental limitations.

- **Produce blurry and over-smoothed output**: The MSE loss minimizes the squared difference between the prediction and the target, which mathematically drives the optimal solution toward the conditional mean of all possible teacher outputs.

- **One-step limit is hard**: local two-step regression accumulates errors when composed; texture/high-freq details degrade.
- **Teacher-binding**: student quality inherits teacher biases (guidance scale, scheduler, failure modes).
- **Compute overhead**: multiple PD stages plus teacher inference can approach original pretraining cost (though still typically cheaper).

<br>

<h1 id="section8.2" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">8.2 Transitive Closure Time-Distillation</h1>

TRACT (Transitive Closure Time-Distillation) is a **trajectory distillation** method that generalizes **binary** (2-step) distillation to **S-step groups**, so that a teacher sampler with $T$ steps can be reduced to $T/S$ steps in *one phase*, and only a **small number of phases** (typically 2‚Äì3) are needed to reach very small step counts. [^Tract]


---

<h1 id="section8.2.1" style="color: #1D4ED8; font-size: 20px; font-weight: bold; text-decoration: underline;">8.2.1 Core Idea: From Binary Merge to Transitive Closure</h1>



In a single distillation phase, the teacher schedule $$\{t_0, t_1, \dots, t_T\}$$ is partitioned into **contiguous groups** of size $S$. Within a group, TRACT asks the student to ‚Äújump‚Äù from **any** time in the group directly to the **group end**. 

<div style="display: block; margin: 0 auto; text-align: center;">
  <img src="/assets/images/2025-10-01-blog-post/tract.jpg" alt="outline" style="display: block; margin: 0 auto;" width="900" height="700">
  <div style="margin-top: 8px; font-size: 16px; color: #666; font-style: Times New Roman;">
     Figure 5: Transitive Closure Time-Distillation
  </div>
</div>

<br>

Concretely, pick a group boundary $(t_{k}, t_{k-S})$ (sampling runs backward), and consider any intermediate $t_{k-i}$ with $i \in \{0,\dots,S-1\}$. We define:

- Teacher **one-step** transition (e.g., DDIM / ODE step): 

  $$
  x_{t_{k-i-1}}^{\mathrm T} = f_{\theta}(x_{t_{k-i}};\,t_{k-i}\to t_{k-i-1}).
  $$

- Student **long jump** transition (same network, but larger time stride):  
  
  $$
  x_{t_{k-S}}^{\mathrm S} = g_{\varphi}(x_{t_{k-i}};\,t_{k-i}\to t_{k-S}).
  $$


If we naively enforced the ‚Äújump-from-anywhere‚Äù constraint, training would require **multiple student calls** per example (one for each possible $t_{k-i}$), which is expensive. 

$$
g_\varphi(x_{t_{k-i}};\,t_{k-i}\to t_{k-S})\;=\;f_{\theta}(\ldots f_{\theta}(x_{t_{k-i}};\,t_{k-i}\to t_{k-i-1})).
$$

TRACT resolves this by introducing a **self-teacher** $g_{\bar\varphi}$, an EMA of the student, and enforces **transitive closure** via a **recurrence**: the ‚Äúlong jump‚Äù from $t_{k-i}$ to $t_{k-S}$ is defined by composing

- **One teacher step**: one teacher step from $t_{k-i}$ to $t_{k-i-1}$ and 
- **Bootstrapping with EMA-student**: one EMA-student jump from $t_{k-i-1}$ to $t_{k-S}$. 

A simple way to write the training target state is:

- If $i=S-1$ (already adjacent to the group end), the teacher can provide the endpoint directly:

  $$
  x_{t_{k-S}}^{\mathrm T}=f_{\theta}(x_{t_{k-S+1}};\,t_{k-S+1}\to t_{k-S}).
  $$

- Otherwise (general case), use the self-teacher recurrence:
  
  $$
  x_{t_{k-S}}^{\mathrm T}
  =
  g_{\bar\varphi}\Big(
      f_{\theta}(x_{t_{k-i}};\,t_{k-i}\to t_{k-i-1});
      \,t_{k-i-1}\to t_{k-S}\Big).
  $$




---

<h1 id="section8.2.2" style="color: #1D4ED8; font-size: 20px; font-weight: bold; text-decoration: underline;">8.2.2 Target Construction and Objective</h1>


Like PD/BTD, TRACT can be implemented with a **signal-prediction** (i.e., $\hat x_0$) network. The student network itself still takes the standard diffusion inputs $(x_t, t, c)$ and predicts $\hat x_0$; the ‚Äúdestination‚Äù $t_{k-S}$ is used only in the **deterministic update** (it selects the coefficients). [^Tract]

Let $x_t=\sqrt{\alpha_t}x_0+\sqrt{1-\alpha_t}\,\epsilon$ (VP parameterization), so during training we can compute the *exact* noise:

$$
\epsilon = \frac{x_t-\sqrt{\alpha_t}\,x_0}{\sqrt{1-\alpha_t}}.
$$

Given the target endpoint state $x_{t_{k-S}}^{\mathrm T}$ (constructed above), we choose the **target clean signal** $x_0^{\star}$ so that a DDIM-style deterministic jump from $t$ to $t_{k-S}$ using the *same* $\epsilon$ reproduces $x_{t_{k-S}}^{\mathrm T}$:

$$
x_0^{\star}
=
\frac{x_{t_{k-S}}^{\mathrm T} - \sqrt{1-\alpha_{t_{k-S}}}\,\epsilon}{\sqrt{\alpha_{t_{k-S}}}}.
$$

Then the student is trained by a standard regression (optionally with the usual DDIM/VP weighting):

$$
\mathcal L_{\text{TRACT}}
=
\mathbb E\Big[ \lambda(t)\,\big\| \hat x_{0,\phi}(x_t,t,c) - x_0^{\star}\big\|_2^2 \Big].
$$

This view also clarifies a common confusion: **TRACT does not require the model to explicitly take an endpoint $s$ as input**. The network remains a standard $\hat x_0(x_t,t,c)$ predictor; the chosen stride (e.g., ‚Äújump to group end‚Äù) is realized by plugging $\hat x_0$ into a deterministic solver update with the corresponding $(\alpha_t,\alpha_s)$. [^Tract]


<div class="modern-deep-card">
  <p class="card-content">
    <span class="label-text">Important: why the target construction enforces $x_t$ and $x_s$ lie on the same DDIM trajectory</span>
  </p>
  <p class="card-content"> 
    In TRACT, the supervision target is constructed under the <span class="hl-blue">DDIM (<span class="math">Œ∑ = 0</span>)</span> trajectory model, where a trajectory is characterized by a <span class="hl-blue">shared latent noise</span> <span class="math">Œµ</span> (and a shared clean signal <span class="math">x‚ÇÄ</span>) across time. Concretely, DDIM assumes that along a deterministic path we can write, for any two times <span class="math">t &gt; s</span>,

    $$
    x_t = \sqrt{\gamma_t}\,x_0 + \sqrt{1-\gamma_t}\,\epsilon,\qquad
    x_s = \sqrt{\gamma_s}\,x_0 + \sqrt{1-\gamma_s}\,\epsilon,
    $$

 
    with the <span class="hl-blue">same</span> <span class="math">Œµ</span>. TRACT then <span class="hl-blue">chooses</span> the target <span class="math">xÃÇ</span> (an <span class="math">x‚ÇÄ</span>-prediction target) by eliminating <span class="math">Œµ</span> and solving for <span class="math">x‚ÇÄ</span>, yielding a closed-form <span class="math">xÃÇ</span> that makes $(x_t, x_s)$ <span class="hl-blue">consistent with one DDIM trajectory</span>. 
    
    $$
\hat x = 

\frac{
x_s\sqrt{1-\gamma_t}-x_t\sqrt{1-\gamma_s}
}{
\sqrt{\gamma_s}\sqrt{1-\gamma_t}-\sqrt{\gamma_t}\sqrt{1-\gamma_s}
},
$$
    
    
    The regression loss pushes the student $g_œÜ(x_t,t)$ toward this <span class="math">xÃÇ</span>; equivalently, it enforces that the inferred noise

    $$
\hat\epsilon=\frac{x_t-\sqrt{\gamma_t}\,g_\phi(x_t,t)}{\sqrt{1-\gamma_t}},
$$

    is the same noise that reproduces <span class="math">x_s</span> under DDIM reconstruction. Therefore, the target construction does not merely fit <span class="math">x‚ÇÄ</span> in isolation‚Äîit explicitly enforces that <span class="math">x_t</span> and <span class="math">x_s</span> share the <span class="hl-blue">same latent noise</span> and hence lie on the <span class="hl-blue">same deterministic DDIM path</span>.
  </p>
</div>


---


<h1 id="section8.2.3" style="color: #1D4ED8; font-size: 20px; font-weight: bold; text-decoration: underline;">8.2.3 TRACT vs. Progressive Distillation (Key Differences)</h1>

- **Stride:** PD enforces a *fixed* compression ratio of 2 per phase; TRACT uses an *arbitrary* group size $S$, enabling much larger stride per phase. [^Tract]  
- **Constraint form:** PD matches a *two-step teacher composition* with a one-step student; TRACT enforces a **transitive-closure recurrence** over a group, so that jumps from interior points are consistent with jumps from later points. [^Tract]  
- **Teacher quality:** PD‚Äôs ‚Äúteacher of the next phase‚Äù is the previous student; TRACT‚Äôs recurrence uses an **EMA self-teacher** inside a phase to stabilize targets while avoiding many generations. [^Tract]  
- **Compute:** With the recurrence, each training sample needs only **one teacher step + one self-teacher jump**, rather than multiple student evaluations across group positions. [^Tract]



---

<h1 id="section8.3" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">8.3 Guided Distillation</h1>

So far, our discussion implicitly assumes an **unguided** diffusion model, where each sampling step requires one network evaluation. However, modern high-quality text-to-image systems heavily rely on **classifier-free guidance (CFG)**, which doubles per-step cost by evaluating: one for a conditional model ($$\epsilon_{c}$$, with prompt / class), and another for an unconditional model ($$\epsilon_{\phi}$$, null prompt), then mixing them with a guidance scale.

$$
\epsilon_{total} = \epsilon_{\phi} + g\cdot(\epsilon_{c}  - \epsilon_{\phi} )
$$

This creates a second bottleneck *orthogonal* to ‚Äúnumber of steps‚Äù: even if you distill $$N\!\to\!4$$ steps, CFG still costs roughly **$2\times 4$ forward passes**. Guided distillation [^gddm] tackles both axes:

- distill ‚Äútwo-model guidance‚Äù into **one** student (remove the 2√ó factor),

- then apply progressive / trajectory distillation to reduce steps further.

---


<h1 id="section8.3.1" style="color: #1D4ED8; font-size: 20px; font-weight: bold; text-decoration: underline;">8.3.1 Stage-One: Distill the Guidance Operator</h1>

Let the teacher provide two predictions at time $t$: a conditional one $$\hat x^{c}_{\theta}(z_t)$$ and an unconditional one $$\hat x_{\theta}(z_t)$$. CFG combines them into a guided prediction at guidance strength $w$:

$$
\hat x^{w}_{\theta}(z_t) = (1+w)\,\hat x^{c}_{\theta}(z_t) - w\,\hat x_{\theta}(z_t).
$$

The key idea is to train a **single** student $$\hat x_{\eta_1}(z_t, w)$$ (also conditioned on context $c$) to directly regress this combined target for **a range of $w$ values**:

$$
\min_{\eta_1}\ 
\mathbb E_{w\sim\mathcal U[w_{\min},w_{\max}],\ t\sim\mathcal U[0,1],\ x\sim p_{\text{data}}}
\Big[
\omega(t)\ \big\|\hat x_{\eta_1}(z_t,w) - \hat x^{w}_{\theta}(z_t)\big\|_2^2
\Big].
$$

This removes the ‚Äútwo-network per step‚Äù overhead while retaining the quality‚Äìdiversity knob via $w$-conditioning. [^gddm]



---


<h1 id="section8.3.2" style="color: #1D4ED8; font-size: 20px; font-weight: bold; text-decoration: underline;">8.3.2 Stage-Two: Distill Sampling Steps (Progressive / Binary)</h1>

After stage-one, the student already matches the **guided** teacher prediction at each time, but still requires many steps if we sample with the original schedule. Stage-two applies standard **step distillation**: train a student to match a **two-step** DDIM trajectory segment of the (guided) teacher in **one** step, then repeat to halve the number of steps again (initializing each new student from its teacher). [^gddm]

Conceptually, guided distillation is therefore ‚Äútrajectory distillation‚Äù in two layers:

1. **Operator distillation (guidance):** distill the **combination rule** $$(\hat x^c,\hat x)\mapsto \hat x^w$$ into a single network. 

2. **Time distillation (steps):** distill multi-step solver trajectories into fewer steps.

In practice, the biggest win often comes from stage-one (saving ~2√ó compute per step), while stage-two provides the additional ‚Äúfew-step‚Äù acceleration. [^gddm]




---

<h1 id="section9" style="color: #1E40AF; font-size: 26px; font-weight: bold; text-decoration: underline;">9. Adversarial-Based Distillation</h1>

**Adversarial distillation** follows the principle ‚Äúdistill **fast realism** by adding a critic.‚Äù The central belief is that ultra-few-step (especially one-step) generators suffer from perceptual artifacts that are not fully penalized by teacher-matching losses alone, so a discriminator provides a complementary signal that sharply enforces high-frequency realism and data-manifold alignment. 

The philosophy is hybrid: keep the teacher as a **semantic / distributional supervisor** while using adversarial training as a *perceptual regularizer* that compensates for the limited iterative correction budget. In practice, adversarial objectives are rarely used alone‚Äîthey are layered on top of score/disttribution/trajectory losses to stabilize training and improve texture fidelity.

---


<h1 id="section9.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">9.1 Adversarial Diffusion Distillation</h1>


While Progressive Distillation (Sec. [8](#section8)) compresses multi-step sampling through supervised teacher-student regression, it still depends on explicit trajectory matching: the student must imitate the teacher‚Äôs denoising transitions.

**Adversarial Diffusion Distillation (ADD)** [^add] proposes a fundamentally different perspective: rather than minimizing a reconstruction distance between teacher and student trajectories, the student is trained **adversarially** to generate outputs that are **indistinguishable from real data**, while remaining consistent with the diffusion process implied by the teacher.


---


<h1 id="section9.1.1" style="color: #1D4ED8; font-size: 20px; font-weight: bold; text-decoration: underline;">9.1.1 Motivation: From Trajectory Matching to Perceptual Realism</h1>

Trajectory-based distillation implicitly assumes the teacher path is a good proxy for perceptual quality. But under aggressive compression (e.g., 1‚Äì4 steps), supervised matching tends to reproduce the teacher‚Äôs **mean-like** behavior, leading to blurred edges and weak high-frequency details. ADD injects a **critic** that explicitly penalizes ‚Äúoff-manifold‚Äù artifacts, while the teacher still anchors the student to the diffusion prior and the condition (text/image guidance).

---


<h1 id="section9.1.2" style="color: #1D4ED8; font-size: 20px; font-weight: bold; text-decoration: underline;">9.1.2 Principle and Formulation</h1>



Let $x_T\sim\mathcal N(0,I)$ denote the initial noise and $\Phi_\text{teacher}$ the full diffusion sampler of teacher model (DDIM, ODE, or PF-ODE). ADD seeks to train a **student generator** $G_\phi(x_K;\,K\to 0)$ that approximates the teacher‚Äôs final output $\Phi_\text{teacher}(x_T;\,T\to0)$ in **$K$ step** ($K$ is small, $$K \ll T$$), while simultaneously fooling an **adversarial discriminator** $D_\psi(x)$. Its loss function consists of two parts.



- **1. Teacher Consistency Loss.** To maintain semantic alignment with the diffusion manifold, ADD retains a lightweight *teacher consistency term*:

  $$
  \mathcal L_{\text{distill}} = \mathbb E_{x_T} \left[\|G_\phi(x_K;\,K\to 0) - \Phi_\text{teacher}(x_T;\,T\to 0)\|_2^2 \right].
  $$

  This term ensures that the student‚Äôs outputs lie near the teacher‚Äôs sample space, preserving the diffusion prior.

- **2. Adversarial Loss.** A discriminator $D_\psi$ is trained to differentiate **real data** $x_0 \sim p_\text{data}$ from **generated samples** $G_\phi(x_K;\,K\to 0)$. The standard non-saturating (or hinge) GAN loss is adopted:

  $$
  \mathcal L_{\text{adv}}^G =
  -\mathbb E_{x_T\sim \mathcal N(0,I)} [\log D_\psi(G_\phi(x_K;\,K\to 0))].
  $$

  By optimizing $\mathcal L_{\text{adv}}^G$ jointly with $\mathcal L_{\text{distill}}$, the student learns to generate visually realistic samples that also align with the teacher‚Äôs denoising manifold.


The overall training objective combines both components with a weighting $\lambda_{\text{adv}}$:

$$
\mathcal L_{\text{ADD}} = 

\mathcal L_{\text{distill}}
+\lambda_{\text{adv}}\,
\mathcal L_{\text{adv}}^G.
$$

During training, the discriminator $D_\psi$ is updated alternately to maximize $\mathcal L_D$.

$$
\mathcal L_D = -\,\mathbb E_{x_0\sim p_\text{data}} [\log D_\psi(x_0)]
-\,\mathbb E_{x_T\sim \mathcal N(0,I)} [\log(1 - D_\psi(G_\phi(x_K;\,K\to 0)))]
$$



---


<h1 id="section9.1.3" style="color: #1D4ED8; font-size: 20px; font-weight: bold; text-decoration: underline;">9.1.3 Implementation Mechanism</h1>

The training procedure can be express in the following figure.

<div style="display: block; margin: 0 auto; text-align: center;">
  <img src="/assets/images/2025-10-01-blog-post/add.jpg" alt="outline" style="display: block; margin: 0 auto;" width="900" height="700">
  <div style="margin-top: 8px; font-size: 16px; color: #666; font-style: Times New Roman;">
     Figure 5: Adversarial Diffusion Distillation
  </div>
</div>

<br>


During training, let $x_0 \sim p_{\text{data}}$.


- **Step 1:** utilize forward diffusion process to add noise to clean image $x_0$.

  $$
  x_s = \alpha(s)x_0 + \sigma(s)\epsilon ,\qquad \epsilon   \sim \mathcal N(0,I).
  $$

  where $0 < s \leq K $.
  
- **step 2:** The student first performs a s-step denoising:
   
   $$
   x_0^{S}=G_\phi(x_s; s\to 0)
   $$
   
   This represents the image the student would produce during inference.

- **step 3:** Instead of drawing real data $x_0$ again from the dataset, ADD reuses the student‚Äôs own output $x_0^{S}$ and applies forward diffusion:
   
   $$
   x_t = \alpha(t) x_0^{S} + \sigma(t)\epsilon^{'} , \qquad \epsilon^{'}   \sim \mathcal N(0,I).
   $$
   
   where $0 < t \leq T $. This design is crucial‚Äî**the teacher is conditioned on the student‚Äôs current distribution** rather than on real data. By doing so, the teacher supervises exactly the domain that the student will visit at inference time, forming a **closed-loop, on-policy distillation** process.

- **step 4:** The teacher diffusion model, typically a pretrained DDIM or PF-ODE sampler, now starts from $x_t$ and performs full multi-step denoising:
   
   $$
   x_0^{T}=\Phi_{\text{teacher}}(x_t;\,t\to 0),
   $$
   
   yielding a high-quality reconstruction of what that noisy sample *should* look like under the teacher‚Äôs diffusion dynamics.

- **step 5:**  The pair $(x_0^{S},,x_0^{T})$ defines the distillation target, while an adversarial discriminator evaluates realism relative to real data $x_0 \sim p_{\text{data}}$.


At inference, only the student is used: one forward pass from Gaussian noise $x_T$ directly yields $x_0^S$. The teacher is entirely discarded.

---

<h1 id="section9.2" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">9.2 Progressive Adversarial Diffusion Distillation (PADD)</h1>

ADD demonstrates that adversarial feedback can ‚Äúrepair‚Äù the perceptual weakness of extreme step reduction, but a practical tension remains: **single-shot** students often lose **tooling compatibility** (e.g., guidance variants, LoRA/Control-style conditioning, or anything that expects a multi-step trajectory). **Progressive Adversarial Diffusion Distillation (PADD)** [^padd] (popularized by **SDXL-Lightning**) resolves this by combining three ideas into a single recipe:

1. **Progressive curriculum (coarse-to-fine compression).**  
   Do not jump directly to 1-step. Train a reasonably accurate multi-step student first (e.g., tens of steps), then progressively halve the step count. This reduces optimization shock and mitigates error accumulation in a controlled way.

2. **Trajectory-preserving parameterization (ODE-flow compatibility).**  
   Instead of only learning an endpoint map $$x_T\mapsto x_0$$, PADD emphasizes learning updates that remain consistent with the underlying diffusion/ODE flow. This preserves the ‚Äúiterability‚Äù of the model‚Äîso downstream mechanisms that rely on multi-step behavior remain usable.

3. **Adversarial sharpening at each stage.**  
   MSE-style distillation at low step counts tends to blur. PADD injects adversarial training (in practice in **latent space**) to enforce crisp details and suppress ‚Äústudent smoothing.‚Äù

A notable engineering choice in PADD is the **latent discriminator design**: rather than decoding to pixel space and relying on an external DINO, the discriminator can reuse parts of a **pretrained diffusion U-Net encoder** as a strong backbone, yielding an efficient adversarial signal while staying aligned with the diffusion model‚Äôs internal representation space.

Overall, PADD can be summarized as: **progressively distill the solver** (stability) + **constrain the learned dynamics** (compatibility) + **use adversarial feedback** (sharpness).

---

<h1 id="section9.3" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">9.3 Latent Adversarial Diffusion Distillation (LADD)</h1>

ADD‚Äôs projected discriminator is effective, but for latent diffusion models it introduces a major inefficiency: to compute an image-space adversarial loss (e.g., on DINO features), one must repeatedly **decode latents into pixels**, which is expensive and memory-heavy. **Latent Adversarial Diffusion Distillation (LADD)** [^ladd] addresses this by replacing ‚Äúdiscriminative projected features‚Äù with **generative features from the teacher diffusion model itself**, enabling training **purely in latent space**.

The core philosophy shift is:

- **ADD:** adversarial loss on top of *external* discriminative features (often image-space), plus a diffusion-style distillation term.  
- **LADD:** adversarial loss on top of the *teacher‚Äôs internal generative features* (noise-level aware), optionally augmented by synthetic data so that an extra distillation loss can become unnecessary.

Concretely, LADD attaches multiple discriminator heads to **token sequences / hidden states** extracted from the teacher at various depths (e.g., after attention blocks). Each discriminator is conditioned on:
- the **noise level** (so it can judge global structure at high noise and textures at low noise),
- pooled **text embeddings** (so realism is evaluated under the prompt).

Two practical insights make LADD especially scalable:

1. **Noise-distribution as a knob for discriminator behavior.**  
   By biasing the sampled noise levels (e.g., via a logit-normal), one can trade off global coherence vs fine textures‚Äîeffectively a *direct handle* on what the adversarial feedback emphasizes.

2. **Synthetic data to improve text alignment.**  
   Natural datasets may be weakly aligned to prompts compared to teacher-generated pairs. LADD therefore leverages **teacher-synthesized data** at a fixed CFG setting; empirically, with synthetic data the adversarial objective alone can be sufficient, and the auxiliary distillation term becomes less critical.

In summary, LADD keeps the adversarial ‚Äúsharpening‚Äù spirit of ADD, but redesigns the discriminator pipeline to be **latent-native, noise-aware, and scalable**‚Äîwhich is particularly important for modern large diffusion transformers and high-resolution generation.


---

<h1 id="section10" style="color: #1E40AF; font-size: 26px; font-weight: bold; text-decoration: underline;">10. Distribution-Based Distillation</h1>

**Distribution distillation** is driven by the principle ‚Äúdistill **what distribution the teacher samples from**.‚Äù Here the target is not a particular trajectory or per-step function, but the **terminal (conditional) data distribution** induced by the teacher sampler; the student is trained so its generated samples match the teacher‚Äôs distribution under a chosen divergence (KL or more general $f$-divergences). 

The philosophy is generator-centric: treat the teacher diffusion model as an implicit sampler defining $$p_T(x\mid c)$$, and train a fast student (often one-step) whose distribution $$q_\theta(x\mid c)$$ aligns with it. A common design pattern is to use score-based identities to obtain usable gradients of distribution divergences (e.g., via score differences), enabling stable training without explicit likelihoods. 

---

<h1 id="section10.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">10.1 Distribution Matching Distillation (DMD)</h1>



**Distribution Matching Distillation (DMD)** [^dmd] introduces a more *principled and stable* formulation.
Rather than relying on an explicit discriminator, DMD enforces that the **distribution of samples produced by a student one-step generator** matches the **distribution implicitly represented by the teacher diffusion model**.
This approach reframes diffusion distillation as a **distribution-level alignment problem**, bridging the gap between likelihood-based diffusion modeling and sample-based generative matching.

---

<h1 id="section10.1.1" style="color: #1D4ED8; font-size: 20px; font-weight: bold; text-decoration: underline;">10.1.1 Motivation ‚Äì Beyond Adversarial Alignment</h1>


Instead of forcing a student to mimic the teacher‚Äôs **noise‚Üíimage** mapping point-by-point (which is expensive and brittle), **Distribution Matching Distillation (DMD)** trains a **one-step generator** so that its **distribution of outputs** becomes indistinguishable from the data distribution learned by the base diffusion model. We do not require the output of the student model to be exactly the same as that of the teacher model for each sample; instead, we are more concerned that the output distribution of the student model should be as consistent as possible with that of the teacher model. 

<div style="display: block; margin: 0 auto; text-align: center;">
  <img src="/assets/images/2025-10-01-blog-post/dmd.jpg" alt="outline" style="display: block; margin: 0 auto;" width="900" height="700">
  <div style="margin-top: 8px; font-size: 16px; color: #666; font-style: Times New Roman;">
     Figure 7: Distribution Matching Distillation
  </div>
</div>

<br>


To make it feasible, the DMD design consists of three network structures.

- **Base (real) denoiser** $\mu_{\text{base}}(x_t,t)$: a pretrained diffusion model (EDM or Stable Diffusion) used as a **frozen** score estimator for the diffused real distribution.  

- **One-step generator** $G_\theta(z)$: same architecture as the denoiser but **no time input**; initialized from the base model‚Äôs weights (mean-prediction form in the paper, works identically with $\epsilon$-prediction).
  
- **Fake (dynamic) denoiser** $\mu_{\phi}^{\text{fake}}(x_t,t)$: **trainable** denoiser that continuously fits the **current student** distribution (used to compute the fake score). 




---

<h1 id="section10.1.2" style="color: #1D4ED8; font-size: 20px; font-weight: bold; text-decoration: underline;">10.1.2 Principle and Formulation</h1>


Two ingredients make DMD work, The final generator loss consists of two components

- **Distribution-Matching Loss.** which is used to minimize the student output distribution ($p_{\text{fake}}$) and the teacher output distribution ($p_{\text{real}}$).

  $$
  D_{\mathrm{KL}}\big(p_{\text{fake}}\ |\ p_{\text{real}}\big)\;=\;\mathbb{E}_{x\sim p_{\text{fake}}}\big[\log p_{\text{fake}}(x)-\log p_{\text{real}}(x)\big],
  $$

- **Regression Loss.** The KL gradient above is well-behaved at **moderate‚Äìhigh noise** but can get unreliable at very **low noise** (real density nearly zero off-manifold). Also, scores are invariant to scaling of the density, which can invite **mode collapse/dropping**. DMD therefore adds a tiny amount of **paired supervision**: precompute a small set of **(noise, multi-step output)** pairs $(z,y)$ from the base model via a deterministic ODE sampler, and minimize

  $$
  \mathcal L_{\text{reg}}=\mathbb{E}_{(z,y)}\,\text{LPIPS}\big(G_\theta(z),\,y\big).
  $$
  
  where $\text{LPIPS}$ [^LPIPS] is an abbreviation for Learned Perceptual Image Patch Similarity, a learned deep feature‚Äìbased metric that measures perceptual similarity between two images, capturing human visual judgment more accurately than pixel-wise distances or conventional perceptual losses.
  
The final generator loss is

$$
\mathcal L_G=D_{\mathrm{KL}}+\lambda_{\text{reg}},\mathcal L_{\text{reg}}\quad (\lambda_{\text{reg}}=0.25 \text{ by default}).
$$


The following figure shows optimizing various objectives starting from the initial state leads to different outcomes.


<div style="display: block; margin: 0 auto; text-align: center;">
  <img src="/assets/images/2025-10-01-blog-post/dmd2.jpg" alt="outline" style="display: block; margin: 0 auto;" width="900" height="700">
  <div style="margin-top: 8px; font-size: 16px; color: #666; font-style: Times New Roman;">
     Figure 2: Distribution Matching Distillation 2
  </div>
</div>

<br>


---

<h1 id="section10.1.3" style="color: #1D4ED8; font-size: 20px; font-weight: bold; text-decoration: underline;">10.1.3 The core objective: KL Divergence</h1>


DMD minimizes

$$
D_{\mathrm{KL}}\big(p_{\text{fake}}\ |\ p_{\text{real}}\big)\;=\;\mathbb{E}_{x\sim p_{\text{fake}}}\big[\log p_{\text{fake}}(x)-\log p_{\text{real}}(x)\big],
$$

with $x=G_\theta(z),\ z\sim\mathcal N(0,I)$. Differentiating w.r.t. $\theta$ using reparameterization gives

$$
\nabla_\theta D_{\mathrm{KL}}\;=\;
\mathbb{E}_{z}\Big[\big(s_{\text{fake}}(x)-s_{\text{real}}(x)\big)\,\frac{\partial G_\theta}{\partial\theta}\Big],
$$

where $s(\cdot)=\nabla_x \log p(\cdot)$ is the **score**. Intuition: $s_{\text{real}}$ pushes samples toward real modes; $-s_{\text{fake}}$ "spreads" samples away from spurious fake modes. The generator update thus follows **(fake ‚àí real)**.  

However, two issues arise in the image space: 

- Scores may be undefined where the other distribution has zero density, that's because at the very beginning, there is almost no overlap between $p_{\text{fake}}$ and $p_{\text{real}}$, according to the score definition 

  $$
  s_{\text{real}} = \nabla_x \log p_{\text{real}}(x) = \frac{\nabla_x p_{\text{real}}(x)}{p_{\text{real}}(x)}
  $$
  
  since $x\sim p_{\text{fake}}$, makes $p_{\text{real}}(x) = 1$ for most area.

- We don't know the exact distribution of $p_{\text{real}}$ and $p_{\text{fake}}$.

Diffusion models estimate scores **of diffused distributions**, not the raw data distribution. The fix is to work **in diffusion space**: perturb $x$ with Gaussian noise to obtain $$x_t\sim q_t(x_t \mid x)=\mathcal N(\alpha_t x,\sigma_t^2 I)$$, where supports overlap and denoisers approximate the corresponding scores. 




Using mean-prediction form [^Song_2019], the scores at time $t$ are

$$
s_{\text{real}}(x_t,t)= -\frac{x_t-\alpha_t\,\mu_{\text{base}}(x_t,t)}{\sigma_t^2},\qquad
s_{\text{fake}}(x_t,t)= -\frac{x_t-\alpha_t\,\mu_{\phi}^{\text{fake}}(x_t,t)}{\sigma_t^2}.
$$

Then the **distribution-matching update** becomes

$$
\nabla_\theta D_{\mathrm{KL}}
\;\approx\;
\mathbb{E}_{z,t,x,x_t}\left[
w_t\,\alpha_t\,\big(s_{\text{fake}}(x_t,t)-s_{\text{real}}(x_t,t)\big)\,\frac{\mathrm d G}{\mathrm d\theta}
\right],
$$

with a carefully chosen **time weight** $w_t$ to normalize gradient magnitudes across noise levels. 

---

<h1 id="section10.1.4" style="color: #1D4ED8; font-size: 20px; font-weight: bold; text-decoration: underline;">10.1.4 How the "fake denoiser" is trained</h1>

Because the student‚Äôs output distribution keeps evolving, DMD trains a **dynamic fake denoiser** $$\mu_{\phi}^{\text{fake}}$$ to track the **current** fake distribution. Training uses a standard diffusion **denoising loss**

$$
\mathcal L_{\text{denoise}}(\phi)
= \big\|\mu_{\phi}^{\text{fake}}(x_t,t)-x_0\big\|_2^2,
$$


where $x_0$ is the (stop-grad) student output that was just diffused to form $x_t$. This keeps $\mu_{\phi}^{\text{fake}}$ on-support for the fake distribution, so its score is numerically stable and informative for the generator update. 

<div class="modern-deep-card">
  <p class="card-content">
    <span class="label-text">Interpretation:</span> Functionally, $\mu_{\phi}^{\text{fake}}$ serves as a <span class="hl-blue">pseudo-teacher</span> in diffusion space for the student‚Äôs current distribution, while the frozen base $\mu_{\text{base}}$ provides the real score. The generator‚Äôs gradient is driven by the difference of the two. Data-flow wise, though, $G_\theta$ supplies the data to train $\mu_{\phi}^{\text{fake}}$, creating a bootstrapped self-distillation loop.
  </p>
</div>




---


---

<h1 id="section10.2" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">10.2 Improved Distribution Matching Distillation (DMD2)</h1>


<br>

---

<h1 id="section11" style="color: #1E40AF; font-size: 26px; font-weight: bold; text-decoration: underline;">11. Score-Based Distillation</h1>






<br>

---

<h1 id="section12" style="color: #1E40AF; font-size: 26px; font-weight: bold; text-decoration: underline;">12. Consistency-Based Distillation</h1>

Consistency distillation is a *path-invariance* distillation paradigm: it compresses a long-horizon iterative generation (or inference) procedure into a one-step or few-step mapping by enforcing that predictions made at different time/noise levels‚Äî**as long as they lie on the same underlying trajectory**‚Äîmust agree in a canonical prediction space. 

While ‚ÄúConsistency Models‚Äù popularized a particular instantiation, the principle is much broader and applies to diffusion sampling, score/SDE solvers, flow-matching ODEs, annealed MCMC/Langevin dynamics, and more generally any process that evolves a state along a continuous (or discretized) notion of time, noise level, or temperature.


---

<h1 id="section12.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">12.1 Core Ingredients of Consistency Distillation</h1>

A generic consistency-distillation system can be decomposed into four ingredients.

- <strong style="color: blue;">A canonical prediction space ($\mathcal{Y}$).</strong>  Choose a representation that encodes the ‚Äúdenoised meaning‚Äù of a noisy state. 

- <strong style="color: blue;">A known re-noising / transport operator ($$\mathcal R_{t\to s}(x_t;\hat x_0)$$).</strong>  Given a canonical prediction (e.g., $\hat x_0$) we can deterministically ‚Äúmove‚Äù along time by reparameterization.

  For example, VP defines the deterministic *re-noise* map (DDIM-style, $\eta=0$):

  $$
  \mathcal R_{t\to s}(x_t;\hat x_0) = \sqrt{\alpha_s}\,\hat x_0 + \sqrt{1-\alpha_s}\,\hat\epsilon, \quad \hat\epsilon =
  \frac{x_t-\sqrt{\alpha_t}\,\hat x_0}{\sqrt{1-\alpha_t}}.
  $$

  This operator is the **bridge** that allows comparing predictions at different noise levels.


- <strong style="color: blue;">A teacher signal (explicit or implicit).</strong>  Consistency distillation needs a reference to avoid collapse:

  - **Explicit teacher**: a pretrained diffusion model / sampler provides $\hat x_0^{\mathrm T}(x_t,t,c)$ or one-step transitions.
  - **Implicit teacher**: an EMA copy of the student acts as a ‚Äúslow-moving target network‚Äù (bootstrapping).

- <strong style="color: blue;">An anti-collapse anchor (boundary condition).</strong> Pure self-consistency admits the trivial constant solution. A boundary/identity condition fixes this:

  $$
  f_\theta(x_\epsilon, t_\epsilon, c) \approx x_\epsilon, \qquad t_\epsilon \to 0,
  $$

  or more generally: the prediction must match ground-truth (when available) or a trusted teacher target near the endpoint.

---

<h1 id="section12.2" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">12.2  Canonical Prediction Space and the Consistency Constraint</h1>


Let $x_t$ denote the state at time/noise level $t$. Consistency distillation trains a student predictor

$$
f_\theta:\ (x,t,c)\mapsto y \in \mathcal{Y},
$$

where $$\mathcal{Y}$$ is a **canonical prediction space** chosen so that all points on the same trajectory should map to the same canonical output. Common choices of $$\mathcal{Y}$$ include:

* <strong style="color: blue;">Clean sample space:</strong> $$y=\hat{x}_0$$, most common in diffusion-style settings.
* <strong style="color: blue;">Noise / score / velocity spaces:</strong> $$y=\hat\epsilon$$, or $$y={\nabla_x \log p_t(x)}$$, or $$y=\hat v$$.
* <strong style="color: blue;">Jump/flow-map space:</strong> $$y=\hat x_s$$, a direct prediction of a later state.
* <strong style="color: blue;">Latent canonical representations:</strong> any ‚Äúequivalence-class representative‚Äù that remains stable along the path

The fundamental consistency condition is: if $x_t$ and $x_s$ lie on the **same trajectory** (with shared randomness coupling), then

$$
f_\theta(x_t,t,c)\ \approx\ f_\theta(x_s,s,c),\qquad t>s.
$$

Intuitively, the student is trained to output a time-invariant ‚Äúanswer‚Äù for every point along a trajectory‚Äîso that denoising at different noise levels becomes mutually consistent after accounting for the process dynamics.

---

<h1 id="section12.3" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">12.3 The Generic Objective: Pairwise Time-Consistency</h1>

Let $$t > s$$ be two noise levels (or continuous times). The central constraint is: The canonical prediction produced at time $t$ should match the canonical prediction produced at time $s$ after transporting the state from $t$ to $s$ using the known dynamics.

A widely used template is a **teacher-student / stop-grad** form:

$$
\mathcal L_{\text{cons}}
=
\mathbb E_{x_t,\,t>s}\Big[
d\Big(
f_\theta(x_t,t,c),\;
\underbrace{f_{\theta^-}(x_s,s,c)}_{\text{stop-grad target}}
\Big)
\Big],
$$

where $d(\cdot,\cdot)$ is an L2/L1/LPIPS-like metric, and

$$
x_s = \mathcal R_{t\to s}\!\big(x_t;\; \hat x_0^{\mathrm{ref}}\big).
$$

The only remaining design choice is: **what is the reference $\hat x_0^{\mathrm{ref}}$ used to build $x_s$?**  
Two dominant patterns cover most practical systems:

---

<h1 id="section12.3.1" style="color: #1D4ED8; font-size: 20px; font-weight: bold; text-decoration: underline;">12.3.1 Teacher-Driven Consistency (Distill a Strong Base Model)</h1>

Use a strong pretrained teacher to define the reference denoising meaning:

$$
\hat x_0^{\mathrm{ref}} = \hat x_0^{\mathrm T}(x_t,t,c).
$$

Then construct the transported state:
$$
x_s
=
\mathcal R_{t\to s}\!\big(x_t;\hat x_0^{\mathrm T}(x_t,t,c)\big).
$$

The student is trained so that its prediction at $(x_t,t)$ agrees with a target prediction at $(x_s,s)$ (often from EMA-student or teacher):

$$
\mathcal L_{\text{cons}}
=
\mathbb E\Big[
d\Big(
f_\theta(x_t,t,c),\;
f_{\theta^-}(x_s,s,c)
\Big)
\Big].
$$


---

<h1 id="section12.3.2" style="color: #1D4ED8; font-size: 20px; font-weight: bold; text-decoration: underline;">12.3.2 Self-Teacher Consistency (EMA Bootstrapping)</h1>

When an explicit teacher is unavailable or too expensive, use an EMA target network to define the reference:

$$
\hat x_0^{\mathrm{ref}} = f_{\theta^-}(x_t,t,c),
\qquad
x_s = \mathcal R_{t\to s}(x_t;\hat x_0^{\mathrm{ref}}).
$$

Then enforce

$$
f_\theta(x_t,t,c) \approx f_{\theta^-}(x_s,s,c).
$$

This turns consistency distillation into a **bootstrapped contraction-learning** problem: the EMA provides stability, while the boundary condition prevents collapse.


---

<h1 id="section12.4" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">12.4 Boundary and Anchor Conditions</h1>

A pure invariance constraint admits degenerate solutions (e.g., constant outputs). In practice, consistency distillation is paired with **boundary/anchor constraints** that pin the canonical space to a meaningful target. Typical anchors include:

* **Data boundary:** enforce $$f_\theta(x_0,0)\approx x_0$$ (or an equivalent small-noise condition)
* **Teacher boundary:** align the student‚Äôs canonical prediction with a teacher‚Äôs high-quality estimate at some reference time
* **Reconstruction/likelihood-related constraints:** depending on the process, add auxiliary losses that prevent collapse and preserve fidelity

Conceptually, the anchor defines *what* the invariant quantity should be, while the consistency loss enforces that this quantity remains stable along the path. The totol consistency loss:


$$
\mathcal L
=
\mathcal L_{\text{cons}} + \lambda\,L_{\text{bnd}} 
$$

where 

$$
\mathcal L_{\text{bnd}}
=
\mathbb E\Big[
d\Big(
f_\theta(x_\epsilon,\epsilon,c),\;x_\epsilon
\Big)
\Big].
$$



---

<h1 id="section12.5" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">12.5 A Canonical Training Algorithm (Template)</h1>

Below is a minimal, implementation-oriented template that captures the *core algorithmic skeleton* independent of specific papers.

```python
# Consistency Distillation (generic template)

# inputs:
#   teacher: optional pretrained diffusion model (can be None)
#   student: f_phi(x_t, t, c) -> x0_hat  (or other canonical prediction)
#   target : EMA(student) = f_phi_bar (stop-grad)
#   schedule: alpha_t, sigma_t (or equivalent)
#   metric d(.,.)

for each training step:
    # 1) sample data and times
    x0 ~ pdata
    eps ~ N(0, I)
    pick t > s  # random pair, or sample s = t - Œî, etc.

    # 2) form noisy input at time t
    x_t = sqrt(alpha_t) * x0 + sqrt(1 - alpha_t) * eps

    # 3) choose reference denoising meaning for transport
    if teacher is not None:
        x0_ref = teacher.predict_x0(x_t, t, c)       # stop-grad
    else:
        x0_ref = target(x_t, t, c)                   # stop-grad (EMA)

    # 4) transport / re-noise from t to s (DDIM-style deterministic)
    eps_ref = (x_t - sqrt(alpha_t) * x0_ref) / sqrt(1 - alpha_t)
    x_s     = sqrt(alpha_s) * x0_ref + sqrt(1 - alpha_s) * eps_ref

    # 5) compute student prediction at t, target prediction at s
    pred_t = student(x_t, t, c)
    with no_grad:
        pred_s = target(x_s, s, c)

    # 6) pairwise consistency loss
    L_cons = d(pred_t, pred_s)

    # 7) boundary / identity anchor (anti-collapse)
    # choose tiny t_eps (or s==0), enforce prediction ~ clean
    L_bnd = d(student(x_eps, t_eps, c), x_eps)  # x_eps close to x0

    # 8) optimize
    L = L_cons + Œª * L_bnd
    update(student)
    update_ema(target, student)

```

At inference, start from a high-noise prior $$x_T$$ and evaluate $$f_\theta(x_T,T)$$ (one-step) or perform a small number of large jumps.

This template emphasizes the central abstraction: **learn a projection-like operator** that maps any point on the trajectory to the same canonical representative.

<br>

---
<h1 id="sectionPartIV" style="color: #1E3A8A; font-size: 30px; font-weight: bold; text-decoration: underline;">Part IV ‚Äî Flow Matching: Looking for Straight Trajectory</h1>


The first two families of acceleration methods reviewed earlier‚Äî**high-order ODE solvers** ([Part II](#sectionPartII)) and **distillation-based acceleration** ([Part III](#sectionPartIII))‚Äîprimarily address **how to make an already-trained diffusion model generate faster**.

They either 1). design more accurate **numerical integrators** to approximate the same diffusion ODE in fewer steps, or 2). learn a **student network** that imitates the teacher‚Äôs multi-step trajectory within one or a few denoising passes.

By contrast, **Flow Matching** does **not** rely on an existing teacher model or on post-hoc solver improvements. Instead, it redefines the **training objective itself**‚Äîlearning a **continuous vector field** that transports samples from noise to data **along the most efficient and smooth path**. In other words, rather than speeding up sampling **after training**, FM builds the notion of efficiency **into the training dynamics**.


---


<h1 id="section13" style="color: #1E40AF; font-size: 26px; font-weight: bold; text-decoration: underline;">13. Flow Matching: A New Paradigm for Fast Sampling</h1>


At its heart, flow matching treats generation as learning an **ordinary differential equation**

$$
\frac{\mathrm d x_t}{\mathrm d t} = v_\theta(x_t, t),
$$

whose trajectories deterministically transform the noise distribution into the data distribution.
Unlike diffusion training, which learns a **stochastic denoising score field** and relies on thousands of discretization steps, flow matching directly learns the **deterministic transport velocity** between the two distributions.

The essential insight is that **the geometry of this learned flow field determines sampling speed**: if the field forms a **straight** and **low-curvature** path from noise to data, the ODE can be integrated in far fewer steps without deviating from the target manifold. Thus, FM aims to find the **shortest and flattest path**‚Äîa "geodesic" in distribution space‚Äîrather than merely replicating the stochastic diffusion trajectory.

A detailed and technical analysis of flow matching will be presented in a separate articles

- [From Diffusion to Flow: A New Genrative Paradigm](https://innovation-cat.github.io/posts/2025/08/flow-matching/)
- [Fast Generation with Flow Matching](https://innovation-cat.github.io/posts/2025/09/fast-flow-matching/)



<br>

---

<h1 id="sectionPartV" style="color: #1E3A8A; font-size: 30px; font-weight: bold; text-decoration: underline;">Part V ‚Äî From Trajectory to Operator</h1>



Up to this point, fast diffusion sampling has been framed as ‚Äúhow to traverse a trajectory faster‚Äù:
- better solvers reduce discretization error with fewer NFEs ([Part II](#sectionPartII)),
- distillation compresses many denoising steps into one/few evaluations ([Part III](#sectionPartIII)),
- flow matching learns a straighter underlying vector field so that integration becomes cheaper ([Part IV](#sectionPartIV)).


In modern pipelines, however, generation is rarely a single monotone pass. We increasingly need to <b>compose</b> large jumps
(e.g., iterative editing loops, refine-and-correct schedules, back-and-forth guidance, multi-stage control).
This motivates a higher-level abstraction: instead of learning (or approximating) a <i>trajectory</i>, learn an <b>operator</b> that directly maps between any two times.




---


<h1 id="section14" style="color: #1E40AF; font-size: 26px; font-weight: bold; text-decoration: underline;">14. Core Concept and General discussion</h1>



The Flow Map Paradigm generalizes both diffusion-style integration and endpoint-anchored consistency:
learn a two-time neural operator that directly approximates the finite-time solution map:


$$
f_\theta(x_t,t,s)\ \approx\ \Phi_{t\to s}(x_t),\qquad s\le t.
$$


This buys three practical benefits (developed in detail in the companion post):

- <strong style="color: blue;">Speed:</strong> a large jump $$t \to s$$ becomes a single network evaluation, not a solver loop.
- <strong style="color: blue;">Control:</strong> the intended jump $$(t,s)$$ is an explicit input, enabling flexible schedules (especially for editing).
- <strong style="color: blue;">Composability:</strong> the ‚Äúcorrect‚Äù operators should satisfy a semigroup law:

    $$\Phi_{t\to s}\approx \Phi_{u\to s}\circ \Phi_{t\to u}.$$
    
    Violations of this law provide a concrete diagnostic (‚Äúsemigroup violation‚Äù) for drift in multi-hop pipelines.



In short: if Part II‚ÄìIV are about **making trajectories cheaper**, Part V is about **learning the jump operator itself**.
Please refer to the following post for the full derivations (Eulerian vs. Lagrangian distillation) and the inpainting experiments that motivate this operator-centric shift.

- [From Trajectories to Operators: A Unified Flow Map Perspective on Generative Modeling](https://innovation-cat.github.io/posts/2026/01/flow-map/)

<br>



---


<h1 id="section15" style="color: #1E3A8A; font-size: 26px; font-weight: bold; text-decoration: underline;">15. References</h1>



[^Parisi]: Parisi G. Correlation functions and computer simulations[J]. Nuclear Physics B, 1981, 180(3): 378-384.

[^Grenander]: Grenander U, Miller M I. Representations of knowledge in complex systems[J]. Journal of the Royal Statistical Society: Series B (Methodological), 1994, 56(4): 549-581.

[^Aapo]: Aapo Hyv√§rinen, ‚ÄúEstimation of non-normalized statistical models by score matching‚Äù, JMLR, 2005.

[^Song_2019]: Yang Song and Stefano Ermon. "Generative Modeling by Estimating Gradients of the Data Distribution". NeurIPS 2019.

[^ddpm]: Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[J]. Advances in neural information processing systems, 2020, 33: 6840-6851.

[^ddim]: Song J, Meng C, Ermon S. Denoising diffusion implicit models[J]. arXiv preprint arXiv:2010.02502, 2020.

[^Vincent]: Vincent P. A connection between score matching and denoising autoencoders[J]. Neural computation, 2011, 23(7): 1661-1674.

[^ctm]: Kim D, Lai C H, Liao W H, et al. Consistency trajectory models: Learning probability flow ode trajectory of diffusion[J]. arXiv preprint arXiv:2310.02279, 2023.

[^cm]: Song Y, Dhariwal P, Chen M, et al. Consistency models[J]. 2023.

[^pd]: Salimans T, Ho J. Progressive distillation for fast sampling of diffusion models[J]. arXiv preprint arXiv:2202.00512, 2022.

[^add]: Sauer A, Lorenz D, Blattmann A, et al. Adversarial diffusion distillation[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024: 87-103.

[^dmd]: Yin T, Gharbi M, Zhang R, et al. One-step diffusion with distribution matching distillation[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2024: 6613-6623.

[^dmd2]: Yin T, Gharbi M, Park T, et al. Improved distribution matching distillation for fast image synthesis[J]. Advances in neural information processing systems, 2024, 37: 47455-47487.


[^LPIPS]: Zhang R, Isola P, Efros A A, et al. The unreasonable effectiveness of deep features as a perceptual metric[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 586-595.

[^sds]: Poole B, Jain A, Barron J T, et al. Dreamfusion: Text-to-3d using 2d diffusion[J]. arXiv preprint arXiv:2209.14988, 2022.

[^sdi]: Lukoianov A, S√°ez de Oc√°riz Borde H, Greenewald K, et al. Score distillation via reparametrized ddim[J]. Advances in Neural Information Processing Systems, 2024, 37: 26011-26044.

[^Tract]: Berthelot D, Autef A, Lin J, et al. Tract: Denoising diffusion models with transitive closure time-distillation[J]. arXiv preprint arXiv:2303.04248, 2023.

[^gddm]: Meng C, Rombach R, Gao R, et al. On Distillation of Guided Diffusion Models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2023.
[^padd]: Lin J, Chen J, Wang Y, et al. SDXL-Lightning: Progressive Adversarial Diffusion Distillation for Efficient Text-to-Image Synthesis[J]. arXiv preprint arXiv:2402.13929, 2024.
[^ladd]: Sauer A, Karras T, Laine S, et al. Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion Distillation[J]. arXiv preprint arXiv:2403.12015, 2024.
