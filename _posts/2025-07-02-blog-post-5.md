---
title: 'A General Discussion of Flow Matching'
date: 2025-07-02
permalink: /posts/2025/07/flow-matching-1/
tags:
  - Flow Matching
  - AIGC
  - category2
---

Flow matching [^fm_2022][^fm_2024] is a continuous-time generative framework in which you learn a time-dependent vector field 
$v_{\theta}$, whose flow transports samples from a simple prior distribution ( usually a standard gaussian distribution) at $t=0$ to your target data distribution at $t=1$. 

![An Overview of Flow Matching](/images/posts/post_5/1.gif)


## preliminaries

In this section, we first summarize the key terms and terminology needed for learning flow matching.

**Vector Field**: vector field is a function that assigns to each spatial point $x_t \in \mathbb R^d$ and time $t \in [0, 1]$ an instantaneous velocity $v_{\theta}(t, x_t)$:

$$
v_{\theta}:\ \mathbb R^d \times [0,1] \to \mathbb R^d
$$

**ODE**: ODE (Ordinary Differential Equation) is the dynamical law you impose using that vector field:

$$
\frac{dx_t}{dt}=v_{\theta}(t, x_t)
$$

Solving this ODE from $t=0$ to $t=1$ is equivalent to sampling, whose goal is to transport an initial point $x_0$ to a target $x_1$ through space according to the learned velocities.

**Trajectory**: A trajectory $(x_0, \dots, x_{t}, \dots,x_1)$, is simply the solution of the above ODE for a given start point $x_0$.  It’s the continuous path that the “particle” traces out under the influence of the vector field:

$$
x(t)=x(0) + \int_0^tv_{\theta}(s, x(s))ds
$$

Or using the Euler method to  solve in a discrete time step:

$$
x_t=x_0+h*v_{\theta}(t, x_t)
$$

**Flow**: a flow is essentially a collection of trajectories that follows the ODE, that means by solving the above ODE we gather a lot of solutions for different initial points

---


## Probability path

The first step of FM is to define a probability path, who specifies a gradual interpolation from initial distribution $p_{init}$ to target distribution $p_{data}$.

**Conditional probability path:**  given an end point $z \sim p_{data}$, a conditional probability path is the distribution of an intermediate sample conditioned on $z$, denoted as $p_t(x_t\|z)$ such that

$$
p_0(\cdot\|z)=p_{init},\ \ \ p_0(\cdot\|z)=\delta_z \ \ \ \ {\rm for\ all}\ z \in \mathbb R^d
$$

**Marginal probability path:** marginal probability path defined as the distribution that we obtain by first sampling a data point $z ∼ p_{data}$ from the data distribution and then sampling from $p_t(x_t \|z)$, we can formalize it as:

$$
p_t(x_t)=\int p_t(x_t|z) p_{data}(z)dz
$$

$p_t(x_t)$ satisfy $p_0(\cdot)=p_{init}$ and $p_1(\cdot)=p_{data}$. The difference between the marginal probability path and the conditional probability path can be illustrated as follows.

![Conditional probability path and Marginal probability path](/images/posts/post_5/2.gif)

---

## References

[^fm_2022]: Lipman Y, Chen R T Q, Ben-Hamu H, et al. Flow matching for generative modeling[J]. arXiv preprint arXiv:2210.02747, 2022.

[^fm_2024]: Lipman Y, Havasi M, Holderrieth P, et al. Flow matching guide and code[J]. arXiv preprint arXiv:2412.06264, 2024.
