---
title: 'Flow Map Learning: A Unified Framework for Fast Generation'
date: 2025-10-20
permalink: /posts/2025/10/flow-map-learning/
tags:
  - Flow Matching
  - Diffusion Model
  - Consistency Models
  - Trajectory
  - Distillation
---


A "flow map" [^ayf] [^flow_map] [^cmt] typically denotes a neural network (or parametric model) 

$$
f_{\theta}(x_t, t, s) ,\qquad 0\!\le\! s\!\le\! t\!\le\! 1.
$$

that maps a state at time $t$ directly to a state at a time $s < t$ (often earlier), following the underlying probability-flow ODE or other transport dynamics. 

In classical diffusion and flow-matching generative frameworks, the state at time $s$ can be obationed by numerical integration: starting from $x_t$, and perform multiple ODE solver steps to reach $x_s$.

$$
x_s = x_t + \int_{t}^{s} v(x_t,t) dt ,\qquad 0\!\le\! s\!\le\! t\!\le\! 1.
$$

where $v$ is a vector field. We use $$Phi_{t\to s}(x_t)=x_s$$ to represent the exact solution operator.



The **Flow Map paradigm** rethinks this process from a functional perspective. Instead of iteratively **integrating** a differential equation, it seeks to **learn the entire solution operator** of that equation, that is to sayï¼Œ a flow map network  $f_{\theta}$ aims to approximate this operator family:

$$
f_\theta(x_t,t,s)\,\approx\,\Phi_{t\!\to\! s}(x_t)  ,\qquad 0\!\le\! s\!\le\! t\!\le\! 1.
$$

Hence, Flow Map learning transforms diffusion sampling from an **integration problem** into a **function-approximation problem**: learn the end-to-end mapping between noise levels, and then generate in one or a few evaluations.

![Flow Map Learning](/images/posts/2025-10-20-blog-post/fm.jpg)
  
---

# <a id="section8">8. References</a>

[^Reflow]: Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022.

[^nvp]: Dinh L, Sohl-Dickstein J, Bengio S. Density estimation using real nvp[J]. arXiv preprint arXiv:1605.08803, 2016.

[^Glow]: Kingma D P, Dhariwal P. Glow: Generative flow with invertible 1x1 convolutions[J]. Advances in neural information processing systems, 2018, 31.

[^Neural_ODE]: Chen R T Q, Rubanova Y, Bettencourt J, et al. Neural ordinary differential equations[J]. Advances in neural information processing systems, 2018, 31.

[^Ffjord]: Grathwohl W, Chen R T Q, Bettencourt J, et al. Ffjord: Free-form continuous dynamics for scalable reversible generative models[J]. arXiv preprint arXiv:1810.01367, 2018.

[^rectified_flow]: Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022.

[^FM]: Lipman Y, Chen R T Q, Ben-Hamu H, et al. Flow matching for generative modeling[J]. arXiv preprint arXiv:2210.02747, 2022.

[^SI]: Albergo M S, Boffi N M, Vanden-Eijnden E. Stochastic interpolants: A unifying framework for flows and diffusions[J]. arXiv preprint arXiv:2303.08797, 2023.

[^SI_1]: Albergo M S, Vanden-Eijnden E. Building normalizing flows with stochastic interpolants[J]. arXiv preprint arXiv:2209.15571, 2022.

[^SI_2]: Albergo M S, Goldstein M, Boffi N M, et al. Stochastic interpolants with data-dependent couplings[J]. arXiv preprint arXiv:2310.03725, 2023.

[^improve_rf]: Lee S, Lin Z, Fanti G. Improving the training of rectified flows[J]. Advances in neural information processing systems, 2024, 37: 63082-63109.

[^instaflow]: Liu X, Zhang X, Ma J, et al. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation[C]//The Twelfth International Conference on Learning Representations. 2023.

[^meanflow]: Geng Z, Deng M, Bai X, et al. Mean flows for one-step generative modeling[J]. arXiv preprint arXiv:2505.13447, 2025.

[^flow_map]: Boffi N M, Albergo M S, Vanden-Eijnden E. Flow map matching with stochastic interpolants: A mathematical framework for consistency models[J]. Transactions on Machine Learning Research, 2025.

[^ayf]: Sabour A, Fidler S, Kreis K. Align Your Flow: Scaling Continuous-Time Flow Map Distillation[J]. arXiv preprint arXiv:2506.14603, 2025.

[^cmt]: Hu Z, Lai C H, Mitsufuji Y, et al. CMT: Mid-Training for Efficient Learning of Consistency, Mean Flow, and Flow Map Models[J]. arXiv preprint arXiv:2509.24526, 2025.