---
title: 'Inverse Problems with Generative Priors'
date: 2025-11-10
permalink: /posts/2025/11/inverse-problems/
tags:
  - Flow Matching
  - Diffusion Model
  - Consistency Models
  - Trajectory
  - Distillation
---


Inverse problems, which aim to recover a signal of interest from indirect and often corrupted measurements, are a cornerstone of computational science and engineering. These problems are typically ill-posed, necessitating the use of prior knowledge to regularize the solution space and ensure a unique and stable reconstruction. This paper provides a structured exposition of the evolution of priors in solving inverse problems, from classical formulations to the modern paradigm of deep generative models. We begin by formalizing the inverse problem from a probabilistic perspective, deriving the Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP) frameworks, which correspond to solutions without and with priors, respectively. We then review classical, non-generative regularization techniques, focusing on the influential Plug-and-Play (PnP) and Regularization by Denoising (RED) frameworks, which leverage off-the-shelf denoisers as implicit priors. The core of this paper is dedicated to the contemporary approach of employing deep generative models—including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Normalizing Flows, and Diffusion Models—as powerful, explicit priors. We detail how the unique properties of each model class can be integrated into the inverse problem objective, enabling state-of-the-art performance by constraining solutions to a learned data manifold. This work aims to bridge the conceptual gap between classical and modern techniques, offering a unified view of the role of priors in solving generative inverse problems.

---

<h1 id="section1" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">1. The Probabilistic Formulation of Inverse Problems</h1>



An inverse problem seeks to recover an unknown signal $\mathbf{x} \in \mathbb{R}^n$ from a set of observed measurements $\mathbf{y} \in \mathbb{R}^m$. This process is typically modeled by a linear forward operator $\mathbf{A}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ and corrupted by additive noise $\mathbf{n} \in \mathbb{R}^m$:

$$
\mathbf{y} = \mathbf{A}\mathbf{x} + \mathbf{n}
$$

The operator $\mathbf{A}$ models the physics of the measurement process, such as a blurring kernel in deblurring, a Radon transform in computed tomography (CT), or a subsampling mask in magnetic resonance imaging (MRI). The problem is deemed "inverse" because we seek to reverse the effect of $\mathbf{A}$. It is often **ill-posed**, meaning that a solution may not exist, may not be unique, or may not depend continuously on the data. This ill-posedness arises when $\mathbf{A}$ is rank-deficient or ill-conditioned ($m < n$ or singular values decay rapidly), making the recovery of $\mathbf{x}$ from $\mathbf{y}$ an ambiguous task.


---

<h1 id="section1.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">1.1 Inverse Problems without Priors: Maximum Likelihood Estimation</h1> 


In the absence of any prior knowledge about the signal $\mathbf{x}$, our estimation relies solely on the data formation model. A common and mathematically convenient assumption is that the noise $\mathbf{n}$ is independent and identically distributed (i.i.d.) Gaussian with zero mean and variance $\sigma^2$, i.e., 

$$\mathbf{n} \sim \mathcal{N}(\mathbf{0}, \sigma^2\mathbf{I})$$

This assumption allows us to formulate the problem within the framework of Maximum Likelihood Estimation (MLE). The likelihood function $p(\mathbf{y}\mid \mathbf{x})$ describes the probability of observing the measurements $\mathbf{y}$ given a specific signal $\mathbf{x}$. Under the Gaussian noise assumption, the relationship $\mathbf{y} = \mathbf{A}\mathbf{x} + \mathbf{n}$ implies that $\mathbf{y}$ is also Gaussian-distributed, centered at $\mathbf{A}\mathbf{x}$:

$$
p(\mathbf{y}\mid \mathbf{x}) = \mathcal{N}(\mathbf{y}; \mathbf{A}\mathbf{x}, \sigma^2\mathbf{I})
$$

The probability density function for this multivariate Gaussian distribution is given by:

$$
p(\mathbf{y}\mid \mathbf{x}) = \frac{1}{(2\pi\sigma^2)^{m/2}} \exp \left(\,-\frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2\, \right)
$$

The MLE principle seeks the estimate $\hat{\mathbf{x}}_{\text{MLE}}$ that maximizes this likelihood. For computational stability and simplicity, it is standard practice to maximize the log-likelihood instead:

$$
\hat{\mathbf{x}}_{\text{MLE}} = \arg\max_{\mathbf{x}} \log p(\mathbf{y}\mid \mathbf{x}) = \arg\max_{\mathbf{x}} \left[ -\frac{m}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 \right]
$$

Since the first term is a constant with respect to $\mathbf{x}$, and maximizing the negative of a function is equivalent to minimizing the function itself, the MLE objective simplifies to a least-squares problem:

$$
\hat{\mathbf{x}}_{\text{MLE}} = \arg\min_{\mathbf{x}} \underbrace{\,\|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2\, }_{\text{data consistency}}
$$

This objective is also known as the Mean Squared Error (MSE) data fidelity term. While principled, it is insufficient for ill-posed problems, as many different signals $\mathbf{x}$ can yield a similarly small residual $\|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2$, often resulting in solutions dominated by noise.


---

<h1 id="section1.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">1.1 Inverse Problems with Priors: Maximum A Posteriori Estimation</h1> 

To overcome the limitations of MLE, we introduce prior knowledge about the expected properties of the signal $\mathbf{x}$. This is formalized using a Bayesian approach. We define a prior distribution $p(\mathbf{x})$ that assigns higher probability to signals that are plausible (e.g., natural images with smooth regions and sharp edges) and lower probability to others (e.g., pure noise).

Using Bayes' theorem, we can combine the likelihood $p(\mathbf{y}\mid \mathbf{x})$ with the prior $p(\mathbf{x})$ to obtain the posterior distribution $p(\mathbf{x}\mid \mathbf{y})$:

$$
p(\mathbf{x}\mid \mathbf{y}) = \frac{p(\mathbf{y}\mid \mathbf{x})p(\mathbf{x})}{p(\mathbf{y})}
$$

The posterior represents our updated belief about $\mathbf{x}$ after observing the data $\mathbf{y}$. The goal of Maximum A Posteriori (MAP) estimation is to find the signal $\hat{\mathbf{x}}_{\text{MAP}}$ that maximizes this posterior probability:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\max_{\mathbf{x}} p(\mathbf{x}\mid \mathbf{y}) = \arg\max_{\mathbf{x}} \frac{p(\mathbf{y}\mid \mathbf{x})p(\mathbf{x})}{p(\mathbf{y})}
$$

Since the evidence $p(\mathbf{y})$ is constant with respect to $\mathbf{x}$, the MAP objective becomes:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\max_{\mathbf{x}} p(\mathbf{y}\mid \mathbf{x})p(\mathbf{x})
$$

Again, we work with the log-posterior for convenience:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\max_{\mathbf{x}} \left[ \log p(\mathbf{y}|\mathbf{x}) + \log p(\mathbf{x}) \right]
$$

Substituting the log-likelihood term from the Gaussian noise model, we get:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\max_{\mathbf{x}} \left[ -\frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 + \log p(\mathbf{x}) \right]
$$

This is equivalent to minimizing the negative log-posterior:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\min_{\mathbf{x}} \left[ \frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 - \log p(\mathbf{x}) \right]
$$

This formulation elegantly connects Bayesian inference to regularized optimization. If we define a regularization function $R(\mathbf{x}) = -\log p(\mathbf{x})$ and a regularization parameter $\lambda = 2\sigma^2$, the MAP estimation is precisely equivalent to solving the following optimization problem:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\min_{\mathbf{x}} \left[ \underbrace{\|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2}_{\text{data consistency}} + \underbrace{\lambda R(\mathbf{x})}_{\text{Prior / Regularization}} \right]
$$

Here, the data fidelity term ensures consistency with the measurements, while the regularization term $R(\mathbf{x})$ enforces the prior. The choice of $R(\mathbf{x})$ is paramount and has been the subject of extensive research.

---

<h1 id="section2" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">2. Classical Regularization via Implicit Priors</h1>

Before the deep learning era, the regularization term $R(\mathbf{x})$ was typically handcrafted based on assumed signal properties, such as sparsity (L1-norm) or smoothness (Total Variation). The advent of Plug-and-Play (PnP) and Regularization by Denoising (RED) marked a significant shift, proposing to use powerful image denoisers to define the prior implicitly.


---

<h1 id="section2.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.1 Plug-and-Play (PnP) Priors</h1> 


The PnP framework leverages optimization algorithms that split the MAP objective into separate steps, such as the Alternating Direction Method of Multipliers (ADMM). The key insight is that the proximal operator associated with many regularization terms, $\text{prox}_{\lambda R}(\mathbf{v}) = \arg\min_{\mathbf{x}} (\frac{1}{2}\|\mathbf{x}-\mathbf{v}\|_2^2 + \lambda R(\mathbf{x}))$, is mathematically equivalent to a denoising operation.

PnP proposes to replace this proximal step with a call to a powerful, off-the-shelf denoiser $\mathcal{D}(\cdot)$. The optimization iterates between a data-fidelity update and a denoising (regularization) step. For example, using ADMM, the updates would look like:

1.  **Data-Fidelity Step:** Solve for $\mathbf{x}_{k+1}$ using the data term.
2.  **Regularization Step:** Apply the denoiser $\mathbf{z}_{k+1} = \mathcal{D}(\mathbf{v}_k)$, where $\mathbf{v}_k$ is an intermediate variable from the ADMM formulation.

The denoiser $\mathcal{D}$, often a pre-trained neural network like BM3D or a DnCNN, implicitly defines the prior $p(\mathbf{x})$. It projects its input onto the manifold of "clean" images it was trained on, thus enforcing the prior that the solution should be denoisable.

#### 2.2 Regularization by Denoising (RED)

While PnP is empirically successful, its convergence guarantees can be elusive as it replaces an operator within an algorithm. RED provides a more formal approach by constructing an explicit regularizer $R(\mathbf{x})$ from a denoiser $\mathcal{D}(\mathbf{x})$. RED is based on the assumption that for a sufficiently well-behaved denoiser, the regularizer's gradient can be defined as:

$$
\nabla R(\mathbf{x}) = \mathbf{x} - \mathcal{D}(\mathbf{x})
$$

This is motivated by the observation that the residual $(\mathbf{x} - \mathcal{D}(\mathbf{x}))$ points away from the clean image manifold. With this explicit gradient for the prior term, one can solve the MAP objective using standard gradient-based methods. For instance, a gradient descent update becomes:

$$
\mathbf{x}_{k+1} = \mathbf{x}_k - \eta \left( \nabla_{\mathbf{x}} \|\mathbf{y} - \mathbf{A}\mathbf{x}_k\|_2^2 + \lambda (\mathbf{x}_k - \mathcal{D}(\mathbf{x}_k)) \right)
$$
$$
\mathbf{x}_{k+1} = \mathbf{x}_k - \eta \left( 2\mathbf{A}^T(\mathbf{A}\mathbf{x}_k - \mathbf{y}) + \lambda (\mathbf{x}_k - \mathcal{D}(\mathbf{x}_k)) \right)
$$

RED provides a more solid theoretical foundation by explicitly defining the objective function being minimized, allowing for more flexible integration with various optimization algorithms.


  

---

<h1 id="section3" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">3. References</h1>


[^Reflow]: Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022.

[^nvp]: Dinh L, Sohl-Dickstein J, Bengio S. Density estimation using real nvp[J]. arXiv preprint arXiv:1605.08803, 2016.

[^Glow]: Kingma D P, Dhariwal P. Glow: Generative flow with invertible 1x1 convolutions[J]. Advances in neural information processing systems, 2018, 31.

[^Neural_ODE]: Chen R T Q, Rubanova Y, Bettencourt J, et al. Neural ordinary differential equations[J]. Advances in neural information processing systems, 2018, 31.

[^Ffjord]: Grathwohl W, Chen R T Q, Bettencourt J, et al. Ffjord: Free-form continuous dynamics for scalable reversible generative models[J]. arXiv preprint arXiv:1810.01367, 2018.

[^rectified_flow]: Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022.

[^FM]: Lipman Y, Chen R T Q, Ben-Hamu H, et al. Flow matching for generative modeling[J]. arXiv preprint arXiv:2210.02747, 2022.

[^SI]: Albergo M S, Boffi N M, Vanden-Eijnden E. Stochastic interpolants: A unifying framework for flows and diffusions[J]. arXiv preprint arXiv:2303.08797, 2023.

[^SI_1]: Albergo M S, Vanden-Eijnden E. Building normalizing flows with stochastic interpolants[J]. arXiv preprint arXiv:2209.15571, 2022.

[^SI_2]: Albergo M S, Goldstein M, Boffi N M, et al. Stochastic interpolants with data-dependent couplings[J]. arXiv preprint arXiv:2310.03725, 2023.

[^improve_rf]: Lee S, Lin Z, Fanti G. Improving the training of rectified flows[J]. Advances in neural information processing systems, 2024, 37: 63082-63109.

[^instaflow]: Liu X, Zhang X, Ma J, et al. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation[C]//The Twelfth International Conference on Learning Representations. 2023.

[^meanflow]: Geng Z, Deng M, Bai X, et al. Mean flows for one-step generative modeling[J]. arXiv preprint arXiv:2505.13447, 2025.

[^flow_map]: Boffi N M, Albergo M S, Vanden-Eijnden E. Flow map matching with stochastic interpolants: A mathematical framework for consistency models[J]. Transactions on Machine Learning Research, 2025.

[^ayf]: Sabour A, Fidler S, Kreis K. Align Your Flow: Scaling Continuous-Time Flow Map Distillation[J]. arXiv preprint arXiv:2506.14603, 2025.

[^cmt]: Hu Z, Lai C H, Mitsufuji Y, et al. CMT: Mid-Training for Efficient Learning of Consistency, Mean Flow, and Flow Map Models[J]. arXiv preprint arXiv:2509.24526, 2025.

[^cm]: Song Y, Dhariwal P, Chen M, et al. Consistency models[J]. 2023.

[^ctm]: Kim D, Lai C H, Liao W H, et al. Consistency trajectory models: Learning probability flow ode trajectory of diffusion[J]. arXiv preprint arXiv:2310.02279, 2023.