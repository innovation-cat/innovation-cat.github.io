---
title: 'Inverse Problems with Generative Priors'
date: 2025-11-10
permalink: /posts/2025/11/inverse-problems/
tags:
  - Flow Matching
  - Diffusion Model
  - Consistency Models
  - Trajectory
  - Distillation
---



<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>üìö Table of Contents</strong></span>
  </summary>
  <ul>
    <li><a href="#section1">1. The Probabilistic Formulation of Inverse Problems</a>
		<ul>
		  <li><a href="#section1.1">1.1 Inverse Problems without Priors: Maximum Likelihood Estimation</a></li>
		  <li><a href="#section1.2">1.2 Inverse Problems with Priors: Maximum A Posteriori Estimation
</a></li>
		</ul>
	</li>
	<li><a href="#section2">2. Classical Regularization with Non-Generative Priors</a>
		<ul>
		  <li><a href="#section2.1">2.1 Smoothness Priors (Handcrafted Explicit Priors)</a></li>
		  <li><a href="#section2.2">2.2 Sparsity Priors in Transform Domains</a></li>
		  <li><a href="#section2.3">2.3 Self-Similarity and Low-Rank Priors</a></li>
		  <li><a href="#section2.4">2.4 Learning-Based Implicit Priors</a></li>
		  <li><a href="#section2.5">2.5 Summary of Classical Regularization Priors</a></li>
		</ul>
	</li>
	<li><a href="#section3">3. Classical Generative Models as Explicit Priors</a>
		<ul>
		  <li><a href="#section3.1">3.1 Generative Adversarial Networks: Manifold Priors</a></li>
		  <li><a href="#section3.2">3.2 Variational Autoencoders: Latent-Variable Priors</a></li>
		  <li><a href="#section3.3">3.3 Normalizing Flows: Exact Log-Density Priors</a></li>
		  <li><a href="#section3.4">3.4 Energy-Based Models: Explicit Energy Priors</a></li>
		</ul>
	</li>
	<li><a href="#section4">4. Diffusion-Based Models as Generative Priors</a>
		<ul>
		  <li><a href="#section4.1">4.1 From Explicit Manifolds to Implicit Stochastic Trajectories</a></li>
		  <li><a href="#section4.2">4.2 Sampling view: Posterior Sampling via Reverse Dynamics</a></li>
		</ul>
	</li>
	<li><a href="#section5">5. Posterior-Guided Sampling for Inverse Problems</a>
		<ul>
		  <li><a href="#section5.1">5.1 Why Inverse Problems Can Be Solved by Sampling: A Fokker‚ÄìPlanck View</a></li>
		  <li><a href="#section5.2">5.2 Constructing Posterior-Guided Reverse Dynamics</a></li>
		  <li><a href="#section5.3">5.3 Approximating the Likelihood Score via the Clean Space</a></li>
		  <li><a href="#section5.4">5.4 Operator Splitting: Balancing Prior and Likelihood in Practice</a></li>
		</ul>
	</li>
	<li><a href="#section11">11. References</a></li>
  </ul>
</details>




Inverse problems, which aim to recover a signal of interest from indirect and often corrupted measurements, are a cornerstone of computational science and engineering. These problems are typically ill-posed, necessitating the use of prior knowledge to regularize the solution space and ensure a unique and stable reconstruction. This post provides a structured exposition of the evolution of priors in solving inverse problems, from classical formulations to the modern paradigm of deep generative models. This work aims to bridge the conceptual gap between classical and modern techniques, offering a unified view of the role of priors in solving generative inverse problems.

---

<h1 id="section1" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">1. The Probabilistic Formulation of Inverse Problems</h1>



An inverse problem seeks to recover an unknown signal $\mathbf{x} \in \mathbb{R}^n$ from a set of observed measurements $\mathbf{y} \in \mathbb{R}^m$. This process is typically modeled by a linear forward operator $\mathbf{A}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ and corrupted by additive noise $\mathbf{n} \in \mathbb{R}^m$:

$$
\mathbf{y} = \mathbf{A}\mathbf{x} + \mathbf{n}, \qquad \mathbf{n} \sim \mathcal{N}(0, \sigma^2 \,I)
$$

The operator $\mathbf{A}$ encodes the physics or geometry of the measurement process. For example:

- In **image deblurring**, $\mathbf{A}$ is a convolution with a blur kernel.  
- In **super-resolution** or **compressed sensing**, $\mathbf{A}$ is a subsampling or projection operator.  
- In **computed tomography (CT)**, $\mathbf{A}$ is a (discretized) Radon transform.  
- In **magnetic resonance imaging (MRI)**, $\mathbf{A}$ corresponds to a subsampled Fourier transform.  

Beyond imaging, similar formulations appear in **audio denoising and deconvolution**, **geophysical exploration**, **astronomical imaging**, and more generally in **parameter estimation** for scientific systems.

The problem is deemed "inverse" because we seek to reverse the effect of $\mathbf{A}$. It is often **ill-posed**, meaning that a solution may not exist, may not be unique, or may not depend continuously on the data. Depending on whether **prior knowledge** is provided, we divide the solution of inverse problems into two major categories for discussion.




---

<h1 id="section1.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">1.1 Inverse Problems without Priors: Maximum Likelihood Estimation</h1> 


In the absence of any prior knowledge about the signal $\mathbf{x}$, our estimation relies solely on the data formation model. A common and mathematically convenient assumption is that the noise $\mathbf{n}$ is independent and identically distributed (i.i.d.) Gaussian with zero mean and variance $\sigma^2$, i.e., 

$$\mathbf{n} \sim \mathcal{N}(\mathbf{0}, \sigma^2\mathbf{I})$$

This assumption allows us to formulate the problem within the framework of **Maximum Likelihood Estimation (MLE)**. The likelihood function $p(\mathbf{y}\mid \mathbf{x})$ describes the probability of observing the measurements $\mathbf{y}$ given a specific signal $\mathbf{x}$. Under the Gaussian noise assumption, the relationship $\mathbf{y} = \mathbf{A}\mathbf{x} + \mathbf{n}$ implies that $\mathbf{y}$ is also Gaussian-distributed, centered at $\mathbf{A}\mathbf{x}$:

$$
p(\mathbf{y}\mid \mathbf{x}) = \mathcal{N}(\mathbf{y}; \mathbf{A}\mathbf{x}, \sigma^2\mathbf{I})
$$

The probability density function for this multivariate Gaussian distribution is given by:

$$
p(\mathbf{y}\mid \mathbf{x}) = \frac{1}{(2\pi\sigma^2)^{m/2}} \exp \left(\,-\frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2\, \right)
$$

The MLE principle seeks the estimate $\hat{\mathbf{x}}_{\text{MLE}}$ that maximizes this likelihood. For computational stability and simplicity, it is standard practice to maximize the log-likelihood instead:

$$\small
\hat{\mathbf{x}}_{\text{MLE}} = \arg\max_{\mathbf{x}} \log p(\mathbf{y}\mid \mathbf{x}) = \arg\max_{\mathbf{x}} \left[ -\frac{m}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 \right]
$$

Since the first term is a constant with respect to $\mathbf{x}$, and maximizing the negative of a function is equivalent to minimizing the function itself, the MLE objective simplifies to a least-squares problem:

$$
\hat{\mathbf{x}}_{\text{MLE}} = \arg\min_{\mathbf{x}} \left[ \underbrace{\,\frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2\, }_{\text{data consistency}} \right]
$$

This objective is also known as the Mean Squared Error (MSE) data fidelity term. While principled, it is insufficient for ill-posed problems, as many different signals $\mathbf{x}$ can yield a similarly small residual $$\|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2$$, often resulting in solutions dominated by noise [^tikhonov] [^tarantola] [^kaipio].


---

<h1 id="section1.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">1.2 Inverse Problems with Priors: Maximum A Posteriori Estimation</h1> 

To overcome the limitations of MLE, we introduce prior knowledge about the expected properties of the signal $\mathbf{x}$. This is formalized using a Bayesian approach. We define a prior distribution $p(\mathbf{x})$ that assigns higher probability to signals that are plausible (e.g., natural images with smooth regions and sharp edges) and lower probability to others (e.g., pure noise).

Using Bayes' theorem, we can combine the likelihood $p(\mathbf{y}\mid \mathbf{x})$ with the prior $p(\mathbf{x})$ to obtain the posterior distribution $p(\mathbf{x}\mid \mathbf{y})$:

$$
p(\mathbf{x}\mid \mathbf{y}) = \frac{p(\mathbf{y}\mid \mathbf{x})\,p(\mathbf{x})}{p(\mathbf{y})}
$$

The posterior represents our updated belief about $\mathbf{x}$ after observing the data $\mathbf{y}$. The goal of **Maximum A Posteriori (MAP)** estimation is to find the signal $\hat{\mathbf{x}}_{\text{MAP}}$ that maximizes this posterior probability [^tarantola] [^kaipio]:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\max_{\mathbf{x}} p(\mathbf{x}\mid \mathbf{y}) = \arg\max_{\mathbf{x}} \frac{p(\mathbf{y}\mid \mathbf{x})\,p(\mathbf{x})}{p(\mathbf{y})}
$$

Since the denominator $p(\mathbf{y})$ is constant with respect to $\mathbf{x}$, the MAP objective becomes:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\max_{\mathbf{x}} p(\mathbf{y}\mid \mathbf{x})\,p(\mathbf{x})
$$

Again, we work with the log-posterior for convenience:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\max_{\mathbf{x}} \left[ \log p(\mathbf{y}|\mathbf{x}) + \log p(\mathbf{x}) \right]
$$

Substituting the log-likelihood term from the Gaussian noise model, we get:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\max_{\mathbf{x}} \left[ -\frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 + \log p(\mathbf{x}) \right]
$$

This is equivalent to minimizing the negative log-posterior:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\min_{\mathbf{x}} \left[ \frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 - \log p(\mathbf{x}) \right]\label{eq:map}
$$

This formulation elegantly connects Bayesian inference to regularized optimization. If we define a regularization function $R(\mathbf{x}) = -\log p(\mathbf{x})$ and absorbing all constants into a free regularization weight $\lambda > 0$, the MAP estimation is precisely equivalent to solving the following optimization problem:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\min_{\mathbf{x}} \left[ \underbrace{\frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2}_{\text{data consistency}}\quad + \underbrace{\lambda R(\mathbf{x})}_{\text{Prior (Regularization)}} \right]
$$

Here, the data fidelity term ensures consistency with the measurements, while the regularization term $R(\mathbf{x})$ enforces the prior. The choice of $R(\mathbf{x})$ is paramount and has been the subject of extensive research.

---

<h1 id="section2" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">2. Classical Regularization with Non-Generative Priors</h1>



Before the widespread adoption of deep generative models, the regularization term $R(\mathbf{x})$ in the MAP objective was primarily designed based on handcrafted statistical assumptions about the signal. 

These priors do **not** attempt to model the full high-dimensional distribution $p(\mathbf{x})$; instead, they encode specific structural properties‚Äîsuch as smoothness, sparsity, or non-local self-similarity‚Äîthat are believed to hold for natural signals.


---

<h1 id="section2.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.1 Smoothness Priors (Handcrafted Explicit Priors)</h1> 


<strong>Core assumption:</strong> Natural signals, such as images, are predominantly smooth or piecewise-constant. High-frequency variations are often attributed to noise rather than the underlying signal structure. Representative Regularizers including:

1. **Tikhonov Regularization [^tikhonov]:** This is one of the earliest and most fundamental regularizers, penalizing the L2-norm of the signal's derivatives. From a Bayesian perspective, this corresponds to assuming a **zero-mean Gaussian prior** on the signal's derivative, i.e.,

   $$
   p(\mathbf{L}\mathbf{x}) \propto \exp \left(- \| \mathbf{L}\mathbf{x} \|_2^2 / 2\tau^2\right)\,\Longrightarrow\,R(\mathbf{x}) = -\log p(\mathbf{L}\mathbf{x}) = \|\mathbf{L}\mathbf{x}\|_2^2
   $$

   
   where $\mathbf{L}$ is a linear operator, typically a finite-difference approximation of the gradient or Laplacian. 

2. **Total Variation (TV) [^tv]:** A significant advancement over Tikhonov, the TV regularizer is defined as the L1-norm of the signal's gradient magnitude, this is equivalent to imposing a **Laplacian prior** on the gradient magnitudes, i.e.,

   $$
   p(\nabla \mathbf{x}) \propto \exp \left(- \|\nabla \mathbf{x}\|_1 / \tau\right)\,\Longrightarrow\,R(\mathbf{x}) =  -\log p(\nabla \mathbf{x}) = \|\nabla \mathbf{x}\|_1
   $$

   Unlike the L2-norm, which penalizes large gradients heavily, the L1-norm promotes solutions where gradients are sparse, effectively preserving sharp edges while enforcing smoothness in flat regions.

   
   
   


---

<h1 id="section2.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.2 Sparsity Priors in Transform Domains</h1> 




<strong>Core assumption:</strong> While signals may not be sparse in their native domain (e.g., pixel space), they often admit a sparse representation in a suitable transform domain.

Let $\mathbf{\Psi}$ denote a transform (orthonormal basis, frame, or learned dictionary), and let
$$
\boldsymbol{\alpha} = \mathbf{\Psi}\mathbf{x}
$$
be the transform coefficients. Sparsity priors posit that most entries of $\boldsymbol{\alpha}$ are zero or near-zero. This choice can be viewed as imposing a **Laplace prior** on the coefficients, which is well known to promote sparsity [^cs-donoho] [^cs-candes], i.e.,

$$
p(\mathbf{\Psi}\mathbf{x}) \propto \exp \left(- \|\mathbf{\Psi}\mathbf{x}\|_1 / \tau\right) \,\Longrightarrow\, R(\mathbf{x}) = \|\mathbf{\Psi}\mathbf{x}\|_1.
$$

Depending on the application, $\mathbf{\Psi}$ may be: a fixed analytical transform (e.g., wavelets for piecewise-smooth images, DCT for photographic content); or a data-driven dictionary learned from example patches.  

Sparsity priors play a central role in **Compressed Sensing**, where one proves that sparse signals can be recovered from undersampled measurements under appropriate conditions on $\mathbf{A}$ and $\mathbf{\Psi}$.

While sparsity priors are powerful for signals with a known sparsifying transform and enjoy strong theoretical guarantees, they depend critically on choosing a suitable transform. They can also bias the magnitude of coefficients (shrinkage), potentially oversuppressing subtle but important details.





---

<h1 id="section2.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.3 Self-Similarity and Low-Rank Priors</h1> 


<strong>Core assumption:</strong> Natural images exhibit rich **non-local self-similarity**: small patches tend to repeat across the image, either exactly or with mild variations. A common way to exploit this is:

1. Extract overlapping patches from $\mathbf{x}$.  
2. Group together patches that are similar (e.g., by Euclidean distance in patch space).  
3. Stack each group of similar patches as columns of a matrix $\mathbf{P}_{\mathbf{x}}$.  

Within each group, the columns of $$\mathbf{P}_{\mathbf{x}}$$ are highly correlated, so $$\mathbf{P}_{\mathbf{x}}$$ is approximately low-rank. This suggests imposing a **low-rank prior** via the **nuclear norm** (sum of singular values) [^nuclear], which convexly relaxes the rank:

$$
R(\mathbf{x}) = \|\mathbf{P}_{\mathbf{x}}\|_*.
$$

Prominent algorithms such as Non-Local Means (NL-Means) [^nlm] and BM3D [^bm3d] operationalize this idea by combining patch grouping, collaborative filtering, and transform-domain shrinkage. These methods captured state-of-the-art performance in image denoising and related tasks for many years.

Self-similarity and low-rank priors are particularly effective for images with repetitive patterns and structured textures. However, they require non-trivial patch grouping and can be computationally demanding. Moreover, their assumptions may break down for objects or regions that are unique within the image, leading to over-smoothing or ghosting artifacts.



---

<h1 id="section2.4" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.4 Learning-Based Implicit Priors</h1> 



The priors discussed so far are defined by explicit analytical forms of $R(\mathbf{x})$. A natural evolution is to let data drive the prior, but still within the traditional optimization framework. **Learning-based implicit priors** achieve this by replacing explicit regularizers with powerful, pre-trained **denoisers**.

<strong>Core assumption:</strong> The manifold of natural signals can be characterized implicitly by the action of a high-quality denoiser $\mathcal{D}(\cdot)$. Clean signals are (approximate) fixed points, $\mathcal{D}(\mathbf{x}) \approx \mathbf{x}$, whereas noisy inputs are pushed towards the signal manifold. Two influential frameworks are:

1. <strong>Plug-and-Play (PnP) Priors [^pnp].</strong>  Many classical algorithms for solving the MAP problem
   
   $$
   \arg\min_{\mathbf{x}} \frac{1}{2\sigma^2}\|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 + \lambda R(\mathbf{x})
   $$
   
   (e.g., ADMM, Half-Quadratic Splitting) alternate between a **data-fidelity step** and a **proximal step** of the form
   
   $$
   \text{prox}_{\lambda R}(\mathbf{z}) 
   = \arg\min_{\mathbf{x}} 
   \left[ 
     \frac{1}{2}\|\mathbf{x} - \mathbf{z}\|_2^2 + \lambda R(\mathbf{x})
   \right].
   $$
   
   For certain choices of $R(\mathbf{x})$ and under a Gaussian noise model, this proximal operator can be interpreted as a denoising operation.

   PnP methods take the **reverse perspective**: they start from a strong, possibly black-box denoiser $\mathcal{D}(\cdot)$ (e.g., BM3D or a deep CNN) and simply **replace** $\text{prox}_{\lambda R}(\cdot)$ with $\mathcal{D}(\cdot)$ inside the optimization algorithm. In doing so, they define an **implicit prior** via the behavior of the denoiser, without ever writing down $R(\mathbf{x})$ explicitly.

2. <strong>Regularization by Denoising (RED) [^red].</strong>  RED goes a step further by attempting to construct an **explicit energy** from a denoiser. Given a denoising operator $\mathcal{D}(\mathbf{x})$, RED defines a vector field
   
   $$
   \nabla R(\mathbf{x}) = \mathbf{x} - \mathcal{D}(\mathbf{x}),
   $$
   
   and uses it as the gradient of a regularization function $R(\mathbf{x})$. Under mild conditions on $\mathcal{D}$ (e.g., local homogeneity and a symmetric Jacobian), this vector field is conservative, so there exists a scalar functional $R(\mathbf{x})$ whose gradient matches the denoiser residual. This enables the use of standard gradient-based methods to minimize
   
   $$
   \arg\min_{\mathbf{x}} 
   \left[
     \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 + \lambda R(\mathbf{x})
   \right],
   $$
   
   where the gradient of $R$ is implemented via $\mathbf{x} - \mathcal{D}(\mathbf{x})$.

Learning-based implicit priors form a bridge between handcrafted regularization and fully generative models: they leverage powerful learned representations (denoisers) while retaining the structure of classical optimization algorithms. However, they still do not provide an explicit, normalized model of $p(\mathbf{x})$, and questions about the exact form or even existence of the underlying prior can be subtle.





  
---

<h1 id="section2.5" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.5 Summary of Classical Regularization Priors</h1> 

We summarize and compare the classic implicit priors listed above using the following table



| Prior Category | Core Assumption | Representative Regularizers / Methods | Advantages | Disadvantages |
| :---: | :--- | :--- | :--- | :--- |
| **Smoothness Priors** | Signals are predominantly smooth or piecewise-constant. High-frequency variations are noise. | ‚Ä¢ Tikhonov: $\|\mathbf{L}\mathbf{x}\|_2^2$ <br> ‚Ä¢ Total Variation (TV): $\|\nabla \mathbf{x}\|_1$ | ‚Ä¢ Simple, convex, and computationally efficient. <br> ‚Ä¢ Effective for signals that are inherently smooth or blocky. | ‚Ä¢ **Tikhonov**: Oversmooths, blurring sharp edges. <br> ‚Ä¢ **TV**: Can create "staircasing" artifacts. <br> ‚Ä¢ Both are too simple for complex textures. |
| **Sparsity Priors** | Signals have a sparse representation in a suitable transform domain (e.g., Wavelets, DCT). | ‚Ä¢ L1-norm of transform coefficients: $\|\mathbf{\Psi}\mathbf{x}\|_1$ <br> ‚Ä¢ Foundational to Compressed Sensing. | ‚Ä¢ Strong theoretical guarantees for recovery under certain conditions. <br> ‚Ä¢ Excellent for signals with a known sparsifying basis. | ‚Ä¢ Relies on finding a suitable transform $\mathbf{\Psi}$. <br> ‚Ä¢ Not effective for signals without a sparse representation. <br> ‚Ä¢ Can introduce bias in coefficient amplitudes. |
| **Self-Similarity & Low-Rank Priors** | Natural signals contain repetitive patterns. Grouped similar patches form a low-rank matrix. | ‚Ä¢ **Methods**: NL-Means, BM3D <br> ‚Ä¢ **Regularizer**: Nuclear Norm of patch matrix, $\|\mathbf{P}_{\mathbf{x}}\|_*$ | ‚Ä¢ Captures complex non-local structures and textures effectively. <br> ‚Ä¢ Achieved state-of-the-art results for many years. | ‚Ä¢ Computationally very expensive due to patch searching and grouping. <br> ‚Ä¢ Can introduce patch-based artifacts. <br> ‚Ä¢ Difficult to formulate as a simple, explicit regularizer. |
| **Learning-Based Implicit Priors** | The manifold of natural signals can be implicitly defined by the action of a powerful denoiser. | ‚Ä¢ **Plug-and-Play (PnP)**: Replaces proximal operator with a denoiser $\mathcal{D}(\cdot)$. <br> ‚Ä¢ **Regularization by Denoising (RED)**: Defines prior gradient as $\nabla R(\mathbf{x}) = \mathbf{x} - \mathcal{D}(\mathbf{x})$. | ‚Ä¢ Highly flexible and modular; leverages SOTA denoisers (incl. deep nets). <br> ‚Ä¢ Excellent empirical performance without needing to train a full generative model. | ‚Ä¢ A "black box" prior. <br> ‚Ä¢ Convergence guarantees can be weak, especially for PnP with complex denoisers. <br> ‚Ä¢ Performance is entirely dependent on the quality and domain of the pre-trained denoiser. |  
  

While powerful, these classical and implicit methods do not model the full data distribution $p(\mathbf{x})$. This limits their ability to, for instance, quantify uncertainty or generate diverse solutions consistent with the measurements. This limitation motivates the paradigm shift towards using explicit deep generative models as priors, as we will explore in the next section.
  
  
---

<h1 id="section3" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">3. Classical Generative Models as Explicit Priors</h1>  
  
  

PnP and RED use denoisers as powerful but implicit priors. The latest paradigm shift involves using deep generative models that explicitly learn the entire data distribution $p(\mathbf{x})$ from a training set. This allows for a more direct and expressive enforcement of the prior. The core idea is to **constrain the solution $\mathbf{x}$ to lie on the low-dimensional manifold learned by the model.**


---

<h1 id="section3.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.1 Generative Adversarial Networks: Manifold Priors</h1> 

A standard GAN [^gan], consisting of a generator $\mathcal{G}(\mathbf{z})$ and a discriminator $\mathcal{D}$. However, both of them are not provide a tractable method for evaluating $p(\mathbf{x})$ for an arbitrary signal $\mathbf{x}$. 

Therefore, applying a GAN prior does not involve formulating an explicit regularizer. Instead, the prior is enforced through a **reparameterization of the solution space**. It acts as a powerful **manifold constraint**, fundamentally changing the nature of the optimization problem itself [^gan-cs].

Suppose a pre-trained GAN generator

$$
\mathcal{G}: \mathcal{Z}  \rightarrow \mathbb{R}^n\,\qquad \text{where}\, \mathcal{Z} \in \mathbb{R}^d\quad \text{and}\quad d \ll n
$$ 

learns a mapping from a simple, low-dimensional latent space $\mathcal{Z}$ to the high-dimensional signal space $\mathbb{R}^n$. The core assumption when using a GAN as a prior is that all plausible signals (e.g., natural images) lie on or very close to the low-dimensional manifold defined by the **range of the generator**. We denote this manifold as $\mathcal{M}$:

$$
\mathcal{M} = \{ \mathcal{G}(\mathbf{z}) \mid \mathbf{z} \in \mathcal{Z} \}
$$

if we write the MAP objective directly in the signal space, imposing that the reconstruction must lie on this manifold, we obtain the constrained problem

$$
\hat{\mathbf{x}}
= \arg\min_{\mathbf{x}\in\mathbb{R}^n}
\frac{1}{2\sigma^2}\| \mathbf{y}-A\mathbf{x}\|_2^2
\quad\text{s.t.}\quad
\mathbf{x}\in\mathcal{M}.
$$

Using the parameterization $$\mathbf{x} = \mathcal{G}(\mathbf{z})$$ and assuming that $\mathcal{M}$ is (approximately) covered by the generator, this constrained problem is **exactly equivalent** to the unconstrained optimization in latent space, Conceptually, we can encode this idea by defining a "regularizer" that enforces a **hard constraint** in the signal space:


$$
R_{\text{GAN}}(\mathbf{x}) = \begin{cases} -\log p(z) & \text{if } \mathbf{x}=\mathcal{G}(\mathbf{z}) \in \mathcal{M} \\ \\
+\infty & \text{if } \mathbf{x} \notin \mathcal{M} \end{cases}
$$

If we were to plug this into the MAP objective, we would get:

$$
\begin{align}
\hat{\mathbf{x}} & = \arg\min_{\mathbf{x} \in \mathbb{R}^n} \left( \frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 + \lambda R_{\text{GAN}}(\mathbf{x}) \right) \\[10pt]
\Longrightarrow \hat{\mathbf{z}} & = \arg\min_{\mathbf{z}} \left( \frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}(\mathcal{G}(\mathbf{z}))\|_2^2 - \lambda \log p(z) \right)
\end{align}
$$
    
This objective forces the solution $\mathbf{x}$ to be strictly within the manifold $\mathcal{M}$ (to avoid the infinite penalty), and among all possible solutions on the manifold, it finds the one that best fits the measurements $\mathbf{y}$.

$$
-\log p(\mathbf{z}) = -\log \left( \frac{1}{(2\pi)^{d/2}} \exp\left(-\frac{1}{2}\|\mathbf{z}\|_2^2\right) \right) = \frac{1}{2}\|\mathbf{z}\|_2^2 + \text{const.}
$$

This leads to the most common and practical objective function for GAN-based inverse problems:

$$
\hat{\mathbf{z}} = \arg\min_{\mathbf{z} \in \mathcal{Z}} \left( \frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}(\mathcal{G}(\mathbf{z}))\|_2^2 + \lambda \|\mathbf{z}\|_2^2 \right)
$$

Minimal python-like pseudocode summaries and  detailed inline comments:

```python
# GAN prior: optimize latent code z so that generated image G(z) fits measurements.
# This is the canonical "latent-MAP" for implicit-manifold priors (no tractable p(x)).
# Inputs:
#   A, y, G (generator), sigma2 (noise variance), lambda_z (latent prior weight)
# Hyperparams: eta (step size), K (iterations)

# Define Gaussian negative log-likelihood gradient for x (pixel space)
def fidelity_grad_x(x):
    # For Gaussian noise: grad_x (1/(2œÉ^2) ||Ax-y||^2) = (1/œÉ^2) A^T(Ax - y)
    return (1.0 / sigma2) * A.T( A(x) - y )

# Initialize z
z = z_init  # e.g., random normal, or encoder warm-start if available

for k in range(K):
    # Forward: map latent to pixels
    x = G(z)

    # Data-fidelity gradient (w.r.t. x) then push to z via vector-Jacobian product
    g_x = fidelity_grad_x(x)
    # Use autodiff VJP: g_z_data = (dG/dz)^T * g_x
    g_z_data = vjp(G, z, g_x)  # pseudo-call: compute vector-Jacobian product

    # Prior gradient in latent space: Gaussian prior p(z)=N(0,I) -> -‚àá log p(z) = z
    g_z_prior = z  # gradient of (1/2)||z||^2

    # Total gradient in z
    g_z = g_z_data + lambda_z * g_z_prior

    # Gradient step (could use Adam instead)
    z = z - eta * g_z

# Return reconstructed image
x_hat = G(z)
```

From an optimization viewpoint, the latent-MAP formulation above is nothing more than a reparameterization of a **hard-constrained problem** in pixel space. 

---

<h1 id="section3.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.2 Variational Autoencoders: Latent-Variable Priors</h1> 

VAE [^vae] define a latent-variable generative model with an decoder (generator):

$$
x\sim p_\theta(x\mid z), \qquad \text{where}\,z\sim p(z)=\mathcal{N}(0,I)
$$

and an encoder $q_\phi(z\mid x)$. This allows the marginal likelihood $\log p_\theta(x)$ to be lower-bounded by the well-known Evidence Lower Bound (ELBO):

$$
\log p_\theta(x)\;\ge\; \underbrace{\mathbb{E}_{q_\phi(z\mid x)}\!\left[\log p_\theta(x\mid z)\right]-
D_{\mathrm{KL}}\!\left(q_\phi(z\mid x)\,\|\, p(z)\right)}_{\mathrm{ELBO}(x;\theta,\phi)}
$$

At first glance, the ELBO appears to offer a tractable surrogate for $-\log p(x)$, 

$$
\hat{\mathbf{x}} = \arg\min_{\mathbf{x}} \left( \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 - \lambda \cdot \text{ELBO}(\mathbf{x}) \right)
$$

However, this approach is problematic for several reasons:

1. **Problem 1:** When we try to minimize the objective with respect to $\mathbf{x}$, the gradient of the regularization term
   
   $$\nabla_{\mathbf{x}} \text{ELBO}(\mathbf{x})$$
   
   must be computed. This involves backpropagating through the entire encoder network $q_{\phi}$, the KL divergence calculation, and the expectation term. This creates an extremely complex, high-dimensional, and likely non-convex optimization landscape.

2. **Problem 2:** ELBO is Only a Lower Bound, the inequality 

   $$\log p(\mathbf{x}) \ge \text{ELBO}(\mathbf{x})$$ 
   
   is central. This gap can be non-zero and highly variable.


Due to the above limitations, VAE-based inverse problem solvers almost universally adopt a **latent-space MAP formulation**. Since the VAE defines a simple Gaussian prior over (z), one instead parameterizes the reconstructed signal as

$$
x = G_\theta(z),
$$

where $G_\theta$ is the VAE decoder (the conditional mean of $p_\theta(x\mid z)$). The prior reduces to

$$
-\log p(z) = \tfrac{1}{2}\|z\|_2^2
$$

yielding the latent-variable MAP objective:

$$
z^\star = \arg\min_{z}
\|A(G_\theta(z)) - y\|^2\;+\;\lambda \|z\|_2^2 ,\qquad x^\star = G_\theta(z^\star).
$$

This approach eliminates the encoder entirely and transforms the inverse problem into a low-dimensional optimization over (z), yielding stable and efficient solvers.

Minimal python-like pseudocode summaries and  detailed inline comments:

```python
# VAE prior: optimize z under decoder x = G_theta(z).
# Although VAE provides ELBO, in practice we solve latent-MAP:
#   min_z  ||A(G(z)) - y||^2 + lambda_z * ||z||^2
# Inputs and hyperparams similar to GAN case.

def fidelity_grad_x(x):
    return (1.0 / sigma2) * A.T( A(x) - y )

z = z_init  # sample or encoder warm-start

for k in range(K):
    x = G(z)  # decoder forward

    # Data gradient w.r.t. x then to z
    g_x = fidelity_grad_x(x)
    g_z_data = vjp(G, z, g_x)

    # Latent prior gradient for Gaussian z
    g_z_prior = z

    g_z = g_z_data + lambda_z * g_z_prior
    z = z - eta * g_z

x_hat = G(z)
```

From a probabilistic viewpoint, VAE-based inverse solvers are built on an **explicit** generative model

$$
p_\theta(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I}), 
\qquad
p_\theta(\mathbf{x}\mid\mathbf{z}) = \mathcal{N}\big(\mathbf{x};\,\mu_\theta(\mathbf{z}), \Sigma_\theta(\mathbf{z})\big),
$$

which in principle defines a **soft prior** over the entire ambient space: as long as $$\Sigma_\theta(\mathbf{z})$$ is full-rank, $$p_\theta(\mathbf{x})>0$$ for all $$\mathbf{x}\in\mathbb{R}^n$$. 

In practice, however, the way VAE priors are used in inverse problems makes them behave like a **parameterized hard constraint** for three reasons:

1. **Deterministic decoder usage.** Instead of sampling $$\mathbf{x}\sim p_\theta(\mathbf{x}\mid\mathbf{z})$$, we almost always set
   
   $$
   \mathbf{x} = \mu_\theta(\mathbf{z}),
   $$
   
   effectively sending the decoder covariance $$\Sigma_\theta(\mathbf{z}) \to \mathbf{0}$$ and discarding the generative noise.

2. **Restricted reconstruction family.** By insisting that every reconstruction has the form $$\mathbf{x} = \mu_\theta(\mathbf{z})$$, we implicitly restrict the feasible set to the **decoder manifold**
   
   $$
   \mathcal{M}_\text{VAE} = \{\mu_\theta(\mathbf{z}) \mid \mathbf{z}\in\mathcal{Z}\},
   $$
   
   and off-manifold points are simply not representable by the solver.

3. **Latent-only optimization.** The inverse problem is solved by optimizing over $$\mathbf{z}$$ alone (latent MAP), without introducing additional degrees of freedom in $$\mathbf{x}$$ or explicitly modelling the decoder noise.

As a result, the VAE-based method, when implemented as latent-space optimization with a deterministic decoder, effectively realizes a **parameterized hard manifold constraint** $$\mathbf{x}\in\mathcal{M}_\text{VAE}$$, regularized by the Gaussian latent prior on $$\mathbf{z}$$. 

---

<h1 id="section3.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.3 Normalizing Flows: Exact Log-Density Priors</h1> 


Normalizing flows (NFs) [^realnvp] [^glow] can precisely obtain the probability density $p(x)$, of variables  which enables us to directly apply Eq. \ref{eq:map} as the objective funtion. Specifically, NFs define an invertible, differentiable map 

$$
f_\theta:\mathbb{R}^d\!\to\!\mathbb{R}^d
$$ 

with 

$$
x=f_\theta(z),\qquad z\sim p(z) = \mathcal N(0,I)
$$

The change-of-variables formula yields an **exact** log-density for any $x$:

$$
\log p_\theta(x) = \log p(z)\;-\;\log\!\left|\det J_{f_\theta}(z)\right|, \qquad z=f_\theta^{-1}(x),
$$

so the MAP objective is

$$
\hat{\mathbf{x}} = \arg\min_{\mathbf{x}} \left( \|Ax-y\|^2\;-\;\log p_\theta(x) \right)
$$

Crucially, because $f_\theta$ is **bijective**, we can optimize either **in pixel space** ($x$-space) or **in latent space** ($z$-space). The two are mathematically equivalent but have distinct computational trade-offs.

- **Pixel-Space MAP (optimize over $x$).** The Objective is .
  
  $$
  \begin{align}
  \hat{\mathbf{x}} & = \arg\min_{\mathbf{x}} \left( \|Ax-y\|^2\;-\;\log p_\theta(x) \right) \\[10pt]
  & = \arg\min_{\mathbf{x}} \left( \|Ax-y\|^2 - \log p\!\big(f_\theta^{-1}(x)\big)\;+\;\log\!\left|\det J_{f_\theta}\!\big(f_\theta^{-1}(x)\big)\right| \right)
  \end{align}
  $$
  
  The gradient with respect to $x$ can be computed with chain of rule. 

  $$
  \nabla_x\!\left[-\log p_\theta(x)\right] = -\Big(\nabla_z \log p(z) - \nabla_z \log |\det J_{f_\theta}(z)|\Big)\;J_{f^{-1}}(x).
  $$
  
  where 
  
  $$z=f_\theta^{-1}(x) \quad \text{and} \quad J_{f^{-1}}(x)=\partial z/\partial x$$
  
  Thus each step requires evaluating the inverse flow and backpropagating through the log-det term. This is direct but can be heavier in practice (especially when (f_\theta^{-1}) is costly).
  
  ```python
  # Flow prior in x-space
  # x ‚Üî z via invertible f_theta, with log p(x) exact
  x = x_init  # e.g., A^T y
  for k in range(K):
    # data gradient in x
    g_data = (1.0 / sigma2) * A.T( A(x) - y )

    # invert to z
    z = f_inv(x)

    # compute prior gradient in z space
    g_z_prior = z
    g_z_jac = grad_logdet_J_f(z)
    g_z = g_z_prior - g_z_jac

    # push g_z back to x with J_f^{-T} (use JVP/VJP)
    Jinv = jvp(f_inv, x, g_z)  # or equivalent autodiff op
    g_prior_x = Jinv

    # total gradient
    g_total = g_data + lambda_z * g_prior_x
    x = x - eta * g_total

  x_hat = x
  ```
  
- **Latent-Space MAP (optimize over $z$)**. Reparameterize $x=f_\theta(z)$ and minimize over $z$:

  $$
  \hat{\mathbf{z}} = \arg\min_{\mathbf{z}} \left( \|A(f_\theta(z))-y\|^2\;-\;\log p(z)\;+\;\log\!\left|\det J_{f_\theta}(z)\right| \right)
  $$
  
  This avoids the inverse pass and often yields smoother optimization. For 
  
  $$p(z)=\mathcal N(0,I) \quad \Longrightarrow \quad -\log p(z)=\frac{1}{2}|z|^2$$ 

  Compared to optimization in the $x$-space, optimization in the latent space $z$ is a more commonly used approach because it is **simpler**, more **stable**, and does not require backpropagating to compute the Jacobian at each step.


  ```python
  # Flow prior in z-space
  # Optimize in latent space z and then map to x = f(z)
  # Objective: min_z ||A f(z) - y||^2 - log p(z) + log|det J_f(z)|

  z = z_init  # e.g., zero or sample
  for k in range(K):
    x = f(z)  # forward flow

    # data fidelity gradient w.r.t x
    g_x = (1.0 / sigma2) * A.T( A(x) - y )

    # push fidelity gradient to z via VJP
    g_z_data = vjp(f, z, g_x)

    # prior term: -‚àá_z log p(z) ; for N(0,I) this is z
    g_z_prior = z

    # log-det Jacobian gradient: ‚àá_z log|det J_f(z)|
    g_z_jac = grad_logdet_J_f(z)  # domain-specific: sum of layer log-dets

    g_z = g_z_data + lambda_z *( g_z_prior - g_z_jac )

    z = z - eta * g_z

  x_hat = f(z)
  ```


---

<h1 id="section3.4" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.4 Energy-Based Models: Explicit Energy Priors</h1> 



Energy-Based Models (EBMs) [^ebm] provide a particularly direct way to define generative priors for inverse problems. Instead of representing $p_\theta(x)$ through a normalized likelihood or a decoder, EBMs define a scalar **energy function** $E_\theta(x)$ that assigns lower energy to more likely samples:

$$
p_\theta(x) \;=\; \frac{1}{Z_\theta} \, \exp\!\big(-E_\theta(x)\big)
$$

where 

$$Z_\theta=\int \exp(-E_\theta(x)) dx$$

is the (typically intractable) partition function. The logarithm of the prior is then:

$$
\log p_\theta(x) = -E_\theta(x) - \log Z_\theta\quad \Longrightarrow \quad  \nabla_x \log p_\theta(x) = -\nabla_x E_\theta(x).
$$


Because the gradient of the log-density depends only on the local energy derivative, **EBMs provide explicit access to the prior gradient** without requiring a normalized likelihood or latent variable, making them ideal for gradient-based inverse solvers.

Minimal python-like pseudocode summaries and  detailed inline comments:

```python
# EBM prior: p(x) ‚àù exp(-E(x)), so MAP solves:
#   min_x ||A x - y||^2 + Œª E(x)
# Direct gradient-based optimization is natural.

x = x_init  # e.g., A^T y
for k in range(K):
    # fidelity gradient (Gaussian case)
    g_data = (1.0 / sigma2) * A.T( A(x) - y )

    # prior gradient is gradient of energy: ‚àá_x E(x)
    g_prior = grad_E(x)  # requires E model to be differentiable

    # combined update (can use Adam)
    x = x - eta * ( g_data + lambda_e * g_prior )

# Optionally, run Langevin refinement to sample posterior:
# x <- x - eta*(g_data + lambda_e * g_prior) + sqrt(2*eta)*Normal(0,I)
x_hat = x
```  

---

<h1 id="section4" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">4. Diffusion-Based Models as Generative Priors</h1>  

The preceding section showed that in classical generative models ‚Äî GANs, VAEs, EBMs, and Normalizing Flows ‚Äî enters the MAP objective as an **explicit regularizer** or as an **implicit manifold constraint** expressed through a low-dimensional parameterization $x = G(z)$.

Despite architectural differences, these models share a common geometric essence: **the inverse problem is solved by searching for a point on the generator‚Äôs manifold that best agrees with the observations**.

When moving to diffusion-family models [^ddpm], this geometric principle remains intact, but the **representation of the manifold and the means of enforcing the constraint** change fundamentally.
  

---

<h1 id="section4.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">4.1 From Explicit Manifolds to Implicit Stochastic Trajectories</h1> 
 


Diffusion models, score-based models [^sde-score], and their deterministic counterparts (probability-flow ODEs, rectified/mean/consistency flows) do not represent the data manifold via an explicit mapping $$G:\!z\mapsto x$$.

Instead, they define it **implicitly** through a **stochastic differential equation (SDE)** or its deterministic limit:

$$
d\mathbf{x}_t = f(\mathbf{x}_t,t)\,dt + g(t)\,d\mathbf{w}_t ,\qquad
\mathbf{x}_0 \!\sim\! p_{\text{data}}.
$$

The training objective is to learn the **score field**

$$s_\theta(\mathbf{x}_t,t) \approx \nabla_{\mathbf{x}_t}\!\log p_t(\mathbf{x}_t)$$

that characterizes the instantaneous geometry of the evolving density $$p_t(\mathbf{x}_t)$$. The reverse process,

$$
d\mathbf{x}_t = [\,f(\mathbf{x}_t,t) - g(t)^2 s_\theta(\mathbf{x}_t,t)\,]\,dt + g(t)\,d\bar{\mathbf{w}}_t
$$

generates samples by integrating backward from pure noise $$\mathbf{x}_T\!\sim\!\mathcal{N}(0,I)$$ to a clean sample $$\mathbf{x}_0$$. Every valid trajectory of this reverse dynamics terminates on the learned **data manifold**‚Äîthus the manifold is not an explicit surface but the **reachable set** of the learned flow field $s_\theta$.





---

<h1 id="section4.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">4.2 Sampling view: Posterior Sampling via Reverse Dynamics</h1> 




Given a measurement model $$\mathbf{y}=A\mathbf{x_0}+\mathbf{n}$$, Bayes‚Äô rule yields the posterior

$$
p(\mathbf{x_0}\mid \mathbf{y})\,  \propto\, p(\mathbf{y}\mid\mathbf{x_0})\,p(\mathbf{x_0}) 
$$


this yields the diffusion-prior inverse problem as wanting to solve the "Bayesian gold standard":

$$
\hat{\mathbf{x}}_{\text{MAP}}
= \arg\max_{\mathbf{x}_0}
\big[
\log p(\mathbf{y}\mid \mathbf{x}_0) + \log p(\mathbf{x}_0)
\big]
$$

This is conceptually straightforward. In practice, however, with a diffusion prior it becomes hard to solve.

The **core issue** is that the inverse problem is formulated in the **clean image space**, seeking an optimal $$\mathbf{x}_0$$, while the diffusion model's knowledge (the score function) is defined in the noisy image space, operating on $$\mathbf{x}_t$$. We cannot directly apply $$s_{\theta}(\mathbf{x}_t, t)$$ to the prior term $$\log p(\mathbf{x}_0)$$ because they operate in different domains.
 
$$\underbrace{s_{\theta}(\mathbf{x}_t, t) = \nabla_{\mathbf{x}_t} \log p(\mathbf{x}_t)}_{t>0}\qquad {\xrightarrow{\quad {\textbf{?}} \quad }} \qquad  \underbrace{\log p(\mathbf{x}_0)}_{t=0}$$ 


We will explore several solutions for solving inverse problems with diffusion priors in the subsequent chapters.




---

<h1 id="section5" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">5. Posterior-Guided Sampling for Inverse Problems</h1>  



In this section, we focus on **posterior-guided sampling** [^sbi-ip] [^dps] [^mcg], a paradigm for solving inverse problems from the perspective of sampling. In this paradigm, we embed the inverse problem into the reverse-time dynamics of the diffusion model by replacing the prior score with a **posterior score**:

$$
\nabla_{\mathbf{x}_t}\log p(\mathbf{x}_t \mid \mathbf{y})
=
\underbrace{\nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t)}_{\text{prior score}}\qquad
+
\underbrace{\nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t)}_{\text{likelihood / data-consistency score}}
$$

and then simulate a modified reverse SDE/ODE whose stationary distribution at $$t\!=\!0$$ coincides with the desired posterior $$p(\mathbf{x}_0\mid \mathbf{y})$$.






---

<h1 id="section5.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">5.1 Why Inverse Problems Can Be Solved by Sampling: A Fokker‚ÄìPlanck View</h1>

We begin with the **unconditional** case. Let $$\{ \mathbf{x}_t \}_{t\in[0,T]}$$ be the forward noising process of a diffusion model,

$$
d\mathbf{x}_t = f(\mathbf{x}_t,t)\,dt + g(t)\,d\mathbf{w}_t,\qquad \mathbf{x}_0 \sim p_{\text{data}}
$$

with $$\mathbf{w}_t$$ a standard Wiener process and $$p_t(\mathbf{x}_t)$$ the marginal density at time $$t$$. The evolution of $$p_t$$ is governed by the **Fokker‚ÄìPlanck equation** [^sde-score] [^sbi-ip]

$$
\partial_t p_t(\mathbf{x}_t) = -\nabla_{\mathbf{x}_t}\!\cdot\!\big(f(\mathbf{x}_t,t)\,p_t(\mathbf{x}_t)\big) + \frac{1}{2}\nabla_{\mathbf{x}_t}^2\!\big(g^2(t)\, p_t(\mathbf{x}_t)\big).
$$

Score-based diffusion models rely on the fact that there exists a **reverse-time SDE**

$$
d\mathbf{x}_t = \Big(f(\mathbf{x}_t,t) - g^2(t) \nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t)\Big)\,dt + g(t)\,d\bar{\mathbf{w}}_t,
$$

whose marginals also satisfy the same Fokker‚ÄìPlanck equation but evolve **backward in time** from $$t=T$$ to $$t=0$$. If we start from $$\mathbf{x}_T \sim p_T \approx \mathcal{N}(0,I)$$ and integrate the reverse SDE/ODE with the **true** score $$\nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t)$$, then the marginals of $$\mathbf{x}_t$$ are exactly $$\{p_t\}_{t\in[0,T]}$$, and in particular $$\mathbf{x}_0\sim p_{\text{data}}$$.

Now consider an inverse problem with measurements

$$
\mathbf{y} = A\mathbf{x}_0 + \mathbf{n},\qquad \mathbf{n}\sim\mathcal{N}(0,\sigma^2 I).
$$

Conditioning on $$\mathbf{y}$$ lifts to the full diffusion trajectory: we obtain a **posterior process** $${\mathbf{x}_t\mid \mathbf{y}}$$ with marginals $$p(\mathbf{x}_t\mid \mathbf{y})$$. The corresponding Fokker‚ÄìPlanck equation for the posterior densities is

$$
\partial_t p(\mathbf{x}_t\mid \mathbf{y}) = 

-\nabla_{\mathbf{x}_t}\!\cdot\!\big(f(\mathbf{x}_t,t)\,p(\mathbf{x}_t\mid \mathbf{y})\big)

+ \frac{1}{2}\nabla_{\mathbf{x}_t}^2\!\big(g^2(t) p(\mathbf{x}_t\mid \mathbf{y})\big).
$$

By the same reasoning as in the unconditional case, there exists a **posterior reverse SDE**

$$
d\mathbf{x}_t =
\Big(
f(\mathbf{x}_t,t)

- g^2(t) \nabla_{\mathbf{x}_t}\log p(\mathbf{x}_t\mid \mathbf{y})
  \Big)\,dt

+ g(t)\,d\bar{\mathbf{w}}_t,
$$

such that, if we initialize at $$t = T$$ with $$\mathbf{x}_T\sim p(\mathbf{x}_T\mid \mathbf{y})$$ (which is typically close to the unconditional $$p_T$$), then the marginals of $$\mathbf{x}_t$$ follow $${ p(\mathbf{x}_t\mid \mathbf{y}) }_{t\in[0,T]}$$, and finally 
 
$$
  \mathbf{x}_0 \sim p(\mathbf{x}_0\mid \mathbf{y}).
$$

This gives the fundamental justification: If we can approximate the posterior score $$\nabla_{\mathbf{x}_t}\log p(\mathbf{x}_t\mid \mathbf{y})$$ and integrate the corresponding reverse-time dynamics, then sampling the inverse problem reduces to simulating a posterior-guided diffusion process whose terminal distribution at $$t\to 0$$ is exactly the desired posterior $$p(\mathbf{x}_0\mid \mathbf{y})$$.

In other words, **inverse problems can be solved by sampling** because the Fokker‚ÄìPlanck equation guarantees that the reverse-time SDE with the posterior score has the posterior as its stationary marginal at $t=0$. The remaining question is how to approximate this posterior score in practice.








---

<h1 id="section5.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">5.2 Constructing Posterior-Guided Reverse Dynamics</h1>  

By Bayes‚Äô rule,

$$
\log p(\mathbf{x}_t\mid \mathbf{y})
= \log p_t(\mathbf{x}_t) + \log p(\mathbf{y}\mid \mathbf{x}_t) + \text{const.},
$$

so the posterior score decomposes as

$$

\nabla_{\mathbf{x}_t}\log p(\mathbf{x}_t\mid \mathbf{y}) = 

\underbrace{\nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t)}_{\text{prior score}}
+
\underbrace{\nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t)}_{\text{likelihood score}}.
$$

Diffusion training gives us a **learned prior score**

$$
s_\theta(\mathbf{x}_t,t) \approx \nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t),
$$

so the main missing piece is the **likelihood score**. Assuming we can approximate it (we discuss in section¬†[5.3](#section5.3)), the **posterior-guided reverse ODE** becomes

$$
\frac{d\mathbf{x}_t}{dt} =

f(\mathbf{x}_t,t)
-\frac{1}{2}g(t)^2
\Big(
s_\theta(\mathbf{x}_t,t)
+
\nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t)
\Big).
$$

In practice, this continuous-time dynamics is discretized at a sequence of times

$$
T = t_K > t_{K-1} > \dots > t_1 > t_0 = 0.
$$




Return $$\mathbf{x}_0$$ at the end of the trajectory as a sample from an approximate posterior over clean images.








---

<h1 id="section5.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">5.3 Approximating the Likelihood Score via the Clean Space</h1>  



We now address the core technical difficulty: how to approximate the **likelihood score**

$$
\nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t).
$$


Under the standard linear-Gaussian measurement model, the likelihood is defined in the **clean space**:

$$
\log p(\mathbf{y}\mid \mathbf{x}_0)
= -\frac{1}{2\sigma^2}\|\mathbf{y}-A\mathbf{x}_0\|_2^2 + \text{const},
$$

with gradient

$$
\nabla_{\mathbf{x}_0}\log p(\mathbf{y}\mid \mathbf{x}_0)
= \frac{1}{\sigma^2} A^\top(\mathbf{y}-A\mathbf{x}_0).
$$

However, the diffusion model operates in the **noisy space** $$\mathbf{x}_t$$, related to $$\mathbf{x}_0$$ via the forward noising process, e.g.

$$
\mathbf{x}_t = s(t)\mathbf{x}_0 + \sigma(t)\boldsymbol{\epsilon},
\qquad \boldsymbol{\epsilon}\sim\mathcal{N}(0,I).
$$

The noisy-space likelihood is obtained by marginalizing over $$\mathbf{x}_0$$:

$$
p(\mathbf{y}\mid \mathbf{x}_t) = \int p(\mathbf{y}\mid \mathbf{x}_0)\,p(\mathbf{x}_0\mid \mathbf{x}_t)\,d\mathbf{x}_0.
$$

This integral is intractable in high dimensions, and differentiating $$\log p(\mathbf{y}\mid \mathbf{x}_t)$$ w.r.t. $$\mathbf{x}_t$$ would require full access to the conditional distribution $$p(\mathbf{x}_0\mid \mathbf{x}_t)$$, which we do not have explicitly.

Posterior-guided methods therefore rely on a **concentration assumption**: for a given $$(\mathbf{x}_t, t)$$, the conditional distribution $$p(\mathbf{x}_0\mid \mathbf{x}_t)$$ is sharply peaked around a denoised estimate

$$
\hat{\mathbf{x}}_0(\mathbf{x}_t,t)
\approx
\mathbb{E}[\mathbf{x}_0\mid \mathbf{x}_t] = \frac{\mathbf{x}_t - \sigma(t)\,\epsilon_{\theta}(x_t, t)}{s(t)}
$$

Approximating $$p(\mathbf{x}_0\mid \mathbf{x}_t)$$ by a delta measure at $$\hat{\mathbf{x}}_0$$ (a Laplace / delta approximation) yields

$$
p(\mathbf{y}\mid \mathbf{x}_t)
\approx
p(\mathbf{y}\mid \hat{\mathbf{x}}_0(\mathbf{x}_t,t))\quad \Longrightarrow
\quad
\log p(\mathbf{y}\mid \mathbf{x}_t)
\approx
\log p(\mathbf{y}\mid \hat{\mathbf{x}}_0(\mathbf{x}_t,t)).
$$

This is the key approximation, we differentiate this approximate expression w.r.t. $$\mathbf{x}_t$$, we obtain

$$\small
\nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t)
\;\approx\; \nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \hat{\mathbf{x}}_0(\mathbf{x}_t,t)) = 

\bigg(\frac{\partial \hat{\mathbf{x}}_0}{\partial \mathbf{x}_t}\bigg)^\top
\nabla_{\mathbf{x}_0}\log p(\mathbf{y}\mid \mathbf{x}_0)
\Big|_{\mathbf{x}_0=\hat{\mathbf{x}}_0(\mathbf{x}_t,t)}.\label{eq:lik}
$$




---

<h1 id="section5.4" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">5.4 Geometric interpretation: the Jacobian as a projection onto the tangent space</h1>

Likelihood score described above is not merely algebra. The crucial insight is geometric: under standard manifold assumptions, this Jacobian behaves like a projection onto the tangent space of the data manifold.


Assume that the clean data $$\mathbf{x}_0$$ lie (locally) on a smooth low-dimensional manifold $$\mathcal{M}\subset\mathbb{R}^d$$, and that the denoiser $$\hat{\mathbf{x}}_0(\mathbf{x}_t,t)$$ acts as a **projection back to the manifold**:

$$
\hat{\mathbf{x}}_0(\mathbf{x}_t,t) \in \mathcal{M}, \quad
\hat{\mathbf{x}}_0(\mathbf{x}_t,t) \approx \arg\min_{\mathbf{z}\in\mathcal{M}} \|\mathbf{z} - \mathbf{x}_t\|_2^2.
$$

Intuitively, given a noisy point $$\mathbf{x}_t$$ near $$\mathcal{M}$$, the denoiser snaps it back to the nearest point $$\hat{\mathbf{x}}_0\in\mathcal{M}$$. Now consider a small perturbation $$\delta \mathbf{x}_t$$ around $$\mathbf{x}_t$$. We can decompose it as

$$
\delta \mathbf{x}_t = 

\delta \mathbf{x}_{\text{tan}} + \delta \mathbf{x}_{\text{nor}},
\quad
\delta \mathbf{x}_{\text{tan}}\in T_{\hat{\mathbf{x}}_0}\mathcal{M},\quad
\delta \mathbf{x}_{\text{nor}}\perp T_{\hat{\mathbf{x}}_0}\mathcal{M},
$$

where $$T_{\hat{\mathbf{x}}_0}\mathcal{M}$$ is the tangent space of the manifold at $$\hat{\mathbf{x}}_0$$.

- If we move $$\mathbf{x}_t$$ **along the tangent direction** $$\delta \mathbf{x}_{\text{tan}}$$, the projected point $$\hat{\mathbf{x}}_0(\mathbf{x}_t,t)$$ will move along the manifold in essentially the same direction.

- If we move $$\mathbf{x}_t$$ **along the normal direction** $$\delta \mathbf{x}_{\text{nor}}$$, the nearest point on the manifold changes very little‚Äîprojection ‚Äúdrops‚Äù the normal component.

Linearizing this behavior, we obtain

$$
\hat{\mathbf{x}}_0(\mathbf{x}_t + \delta \mathbf{x}_t,t)
\;\approx\;
\hat{\mathbf{x}}_0(\mathbf{x}_t,t)

+ J_{\hat{\mathbf{x}}_0}(\mathbf{x}_t),\delta \mathbf{x}_t
  \;\approx\;
  \hat{\mathbf{x}}_0(\mathbf{x}_t,t)
+ \delta \mathbf{x}_{\text{tan}}.
$$

This shows that, to first order,

$$
J_{\hat{\mathbf{x}}_0}(\mathbf{x}_t),\delta \mathbf{x}_t
= \delta \mathbf{x}_{\text{tan}},
$$

i.e. the Jacobian of the denoiser "keeps" the tangent component and discards the normal component. In other words, $$J_{\hat{\mathbf{x}}_0}(\mathbf{x}_t)$$ acts as a **projection operator from the ambient space $$\mathbb{R}^d$$ onto the tangent space $$T_{\hat{\mathbf{x}}_0}\mathcal{M}$$**.


Putting this back into the likelihood score expression, looking at the Eq. \ref{eq:lik} above, we can easily see that the likelihood score actually consists of two parts.

$$
\nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t)
\;\approx\;
\underbrace{J_{\hat{\mathbf{x}}_0}^\top(\mathbf{x}_t)}_{\text{Jacobian projection}}\,
\underbrace{\nabla_{\mathbf{x}_0}\log p(\mathbf{y}\mid \mathbf{x}_0)\big|_{\mathbf{x}_0=\hat{\mathbf{x}}_0(\mathbf{x}_t,t)}}_{ \text{clean-space DC gradient} }.
$$



Geometrically, this means:

- First, we compute the **clean-space measurement gradient**:

  $$
  \mathbf{g}_{x_0}
  = \frac{1}{\sigma^2} A^\top(\mathbf{y}-A\hat{\mathbf{x}}_0),
  $$
  
  which tells us how to move $$\hat{\mathbf{x}}_0$$ to better satisfy $$A\mathbf{x}_0\approx\mathbf{y}$$.

- Then, we apply $$J_{\hat{\mathbf{x}}_0}^\top$$ (which behaves like a projection onto $$T_{\hat{\mathbf{x}}_0}\mathcal{M})$$ to this gradient, **removing its normal component** and keeping only the **tangent component** along the data manifold.

The result is a **likelihood score in noisy space** that moves $$\mathbf{x}_t$$ in a direction that: improves data consistency (because it originates from $$\mathbf{g}_{x_0}$$), and remains compatible with the learned data manifold (because it is projected to the tangent space).




---

<h1 id="section5.5" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">5.5 Operator Splitting: Balancing Prior and Likelihood in Practice</h1>

Finally, we discuss a key **algorithmic design pattern** in posterior-guided sampling: **operator splitting** between prior and likelihood contributions. Instead of integrating the full posterior vector field in one monolithic update, we decompose each step into:

1. A **prior step** driven solely by the diffusion score $$s_\theta(\mathbf{x}_t,t)$$;
2. A **data-consistency (DC) gradient step** driven by an approximate likelihood score $$\hat{\nabla}_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t)$$.


Mathematically, this corresponds to splitting the posterior vector field

$$
v_{\text{post}}(\mathbf{x}_t,t) =

v_{\text{prior}}(\mathbf{x}_t,t)
+
v_{\text{Likelihood}}(\mathbf{x}_t,t),
$$

and discretizing one time step as

$$
\mathbf{x}_{t_k}
\;\xrightarrow{\;\text{prior step}\;}\;
\tilde{\mathbf{x}}_{t_k}
\;\xrightarrow{\;\text{DC step}\;}\;
\mathbf{x}_{t_{k-1}},
$$

where

$$

\tilde{\mathbf{x}}_{t_k}\, \xleftarrow{\,\text{Euler step}\,}\, \mathbf{x}_{t_k} - \Delta_t\, v_{\text{prior}}(\mathbf{x}_t,t) \\[10pt]
\mathbf{x}_{t_{k-1}}\, \xleftarrow{\,\text{DC Gradient step}\,}\, \tilde{\mathbf{x}}_{t_k} + \eta_k\,v_{\text{lik}}(\tilde{\mathbf{x}}_{t_k},t_k).

$$


This splitting offers several practical advantages:



- **Flexible weighting and scheduling.** The scalar $\eta_k$ (and possibly additional scaling of the DC gradient) implicitly defines the **relative strength** of prior vs likelihood at each time step: larger $\eta_k$ refers to stronger enforcement of data consistency at time $t_k$; smaller $\eta_k$ implies that stronger trust in the generative prior at that time.

  This allows practitioners to design **time-dependent guidance schedules**‚Äîfor example, weak data-consistency at very noisy times (when measurements are not yet informative) and stronger guidance near (t=0) (when the denoised estimate is more reliable).

- **Modularity and reuse of samplers.** The prior step can be any existing sampler (DDIM, DPM-Solver, Heun, etc.) treated as a black box, requiring no modification to incorporate inverse problems. The DC step is an additive correction on top of this sampler, making the method simple to implement and easy to combine with new diffusion architectures.

- **Stability and interpretability.**
  Integrating prior and likelihood contributions in separate substeps makes it easier to:

  * Diagnose instabilities (e.g., if DC steps are too aggressive and cause divergence, or if prior steps over-dominate and ignore measurements);
  * Interpret the effect of hyperparameters: prior step size / solver order vs DC step size / iterations.

- **Compatibility with advanced DC solvers.** Operator splitting cleanly separates this subproblem from the generative prior, so one can deploy powerful physics-based solvers for the likelihood term without entangling them with the diffusion updates.

Overall, operator splitting realizes posterior-guided sampling as a **two-operator dynamical system** in which:

- The **prior operator** keeps samples on (or near) the learned data manifold and preserves the generative realism of the diffusion model.
- The **likelihood operator** pulls samples toward the measurement-consistent manifold, with tunable strength and potentially sophisticated numerical solvers.

This flexible decomposition is key to making posterior-guided sampling both **practical** and **robust** across a wide variety of inverse problems and forward operators (A).












---

<h1 id="section6" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">
6. Clean-Space Local-MAP Sampling with Diffusion Priors
</h1>

In Chapter 5, we characterized a broad family of generative inverse problem solvers as **posterior-guided samplers** operating in the noisy space $$\{\mathbf{x}_t\}_{t\in[0,T]}$$. In that view, methods such as DPS and MCG modify the reverse diffusion dynamics by injecting a likelihood score $$\nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t)$$ directly into the vector field, yielding a noisy-space approximation of the posterior score $$\nabla_{\mathbf{x}_t}\log p(\mathbf{x}_t\mid \mathbf{y})$$.

In this chapter we turn to a complementary paradigm, exemplified by **Decomposed Diffusion Sampling (DDS)**, in which the inverse problem is tackled directly in the **clean signal space** $\mathbf{x}_0$. Rather than constructing the posterior score in $\mathbf{x}_t$ and integrating a single posterior-guided SDE/ODE, DDS alternates between:

- a **generative prior step** that maps noisy states $\mathbf{x}_t$ to a denoised estimate $\hat{\mathbf{x}}_0$ and back; and  
- a **clean-space optimization step** that locally solves a (regularized) maximum likelihood or maximum a posteriori (MAP) problem in $\mathbf{x}_0$.

This yields what we will refer to as a **clean-space local-MAP sampling paradigm**. The outer loop still follows a diffusion-style trajectory in $t$, but each time step is mediated by an inner optimization in the clean space. This decouples the roles of the generative prior and the measurement model in a way that is conceptually close to classical inverse problem formulations, while retaining the expressive power of modern diffusion priors.

---

<h1 id="section6.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">6.1 From Global MAP to Local Clean-Space Refinement</h1>

We consider the standard linear inverse problem

$$
\mathbf{y} = A\mathbf{x}_0 + \mathbf{n}, \qquad 
\mathbf{n} \sim \mathcal{N}(\mathbf{0}, \sigma_y^2 I),
$$

and a diffusion prior over $\mathbf{x}_0$ implicitly defined by the forward noising process and its learned reverse dynamics. The global MAP problem in the clean space is

$$
\hat{\mathbf{x}}_0^{\text{MAP}} =
\arg\min_{\mathbf{x}_0} \Big[ -\log p(\mathbf{y}\mid \mathbf{x}_0) - \log p(\mathbf{x}_0) \Big]
$$

In principle, if we could evaluate (or differentiate) $$\log p(\mathbf{x}_0)$$ directly, we might attempt to solve this optimization problem by gradient-based methods in $$\mathbf{x}_0$$ (cf. Chapter 3). However, in diffusion models $p(\mathbf{x}_0)$ is given only implicitly through a time-dependent score oracle $$\nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t)$$ or equivalently through the reverse-time vector field. There is no tractable closed form for $$-\log p(\mathbf{x}_0)$$ itself.

Posterior-guided noisy-space samplers (Chapter 5) respond to this difficulty by working entirely in $\mathbf{x}_t$ space and approximating $$\nabla_{\mathbf{x}_t}\log p(\mathbf{x}_t\mid\mathbf{y})$$. DDS takes a different route:

- It **never attempts to explicitly evaluate $-\log p(\mathbf{x}_0)$**.  

- Instead, it uses the diffusion prior to provide, at each time $t$, a **local anchor** $\hat{\mathbf{x}}_0(\mathbf{x}_t,t)$ around which $-\log p(\mathbf{x}_0)$ can be approximated.  

- It then formulates a **local optimization problem** in $\mathbf{x}_0$ that blends:
  - a data term derived from $-\log p(\mathbf{y}\mid \mathbf{x}_0)$, and  
  - a local regularizer anchored at $\hat{\mathbf{x}}_0$, which acts as a quadratic approximation to $-\log p(\mathbf{x}_0)$ in a small neighborhood.

Thus, the global MAP objective is replaced by a sequence of **time-indexed local subproblems** that can be solved efficiently with classical optimization tools (e.g., conjugate gradients). The diffusion prior appears in two places:

1. It determines the **anchor point** $\hat{\mathbf{x}}_0(\mathbf{x}_t,t)$ and the **tangent subspace** where updates are allowed.  
2. It supplies the **re-noising mapping** that embeds the refined clean estimate back into the diffusion trajectory at the next time step.

---

<h1 id="section6.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">6.2 The Noiseless Setting: Local Maximum Likelihood in the Clean Space</h1>

We first consider the **noiseless measurement** case

$$
\mathbf{y} = A\mathbf{x}_0
$$

and assume the forward operator $A$ is known. In this setting, the (negative) log-likelihood reduces to a pure data-consistency term

$$
-\log p(\mathbf{y}\mid \mathbf{x}_0) \propto \tfrac{1}{2}\|\mathbf{y}-A\mathbf{x}_0\|_2^2.
$$

The core DDS update at time $t$ consists of three conceptual steps:

1. **Denoising / prior projection.**  From the current noisy state $\mathbf{x}_t$, we compute a clean estimate
   
   $$
   \hat{\mathbf{x}}_0 = \hat{\mathbf{x}}_0(\mathbf{x}_t,t)
   $$
   
   via an $x_0$-prediction network or Tweedie‚Äôs formula. Geometrically, $\hat{\mathbf{x}}_0$ can be interpreted as a local projection of $\mathbf{x}_t$ onto the data manifold.

2. **Local MLE refinement in the clean space.**  We use $\hat{\mathbf{x}}_0$ as an initialization and search for a refined estimate $\mathbf{x}_0^\star$ that better satisfies the measurement equation:
   
   $$
   \mathbf{x}_0^\star 
   \approx
   \arg\min_{\mathbf{x}_0}
   \frac{1}{2}\|\mathbf{y}-A\mathbf{x}_0\|_2^2
   \quad
   \text{subject to } \mathbf{x}_0 \in \hat{\mathbf{x}}_0 + \mathcal{T}_t.
   $$
   
   Here $$\mathcal{T}_t$$ is a subspace that approximates the **tangent space** of the data manifold at $\hat{\mathbf{x}}_0$. In practice, $\mathcal{T}_t$ is instantiated as a Krylov subspace generated by $A^\*A$ and the initial residual, which effectively constrains updates to directions that both:
   - lie in the local manifold tangent space; and  
   - are relevant for improving data consistency.

   Concretely, DDS performs a fixed number $M$ of conjugate gradient (CG) iterations on the normal equations
   
   $$
   A^* A \mathbf{x} = A^* \mathbf{y}
   $$
   
   with initial point $\hat{\mathbf{x}}_0$, implicitly restricting $\mathbf{x}$ to a low-dimensional Krylov subspace. The resulting $\mathbf{x}_0^\star$ can be viewed as a **local maximum likelihood** estimate under the constraint that updates remain approximately tangential to the generative manifold.

3. **Re-noising / embedding back into the trajectory.**  The refined clean estimate $\mathbf{x}_0^\star$ is then mapped back to a noisy state at the next time level $t-\Delta t$ via the forward noising rule (or its deterministic PF-ODE/DDIM counterpart):
   
   $$
   \mathbf{x}_{t-\Delta t} = \texttt{ReNoise}(\mathbf{x}_0^\star, t-\Delta t).
   $$

In the noiseless setting, the local clean-space problem contains **only the data term**. The prior acts indirectly via:

- the initial estimate $\hat{\mathbf{x}}_0(\mathbf{x}_t,t)$, which is already close to the data manifold; and  
- the choice of search subspace $\mathcal{T}_t$, which restricts updates to directions compatible with the learned manifold geometry.

This realizes a clean-space variant of the ‚Äúprior + data-consistency‚Äù decomposition: the prior is used to set up a well-posed low-dimensional ML problem, which is then solved using standard linear algebra.

---

<h1 id="section6.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">6.3 The Noisy Setting: Proximal Local-MAP Updates</h1>

When the measurements are **noisy**,

$$
\mathbf{y} = A\mathbf{x}_0 + \mathbf{n},
\quad \mathbf{n}\sim\mathcal{N}(0,\sigma_y^2 I),
$$

the pure MLE objective

$$
\frac{1}{2}\|\mathbf{y}-A\mathbf{x}_0\|_2^2
$$


is no longer sufficient: directly minimizing this term tends to overfit the noise and may severely amplify ill-posedness in $A$. In this regime, DDS extends the clean-space subproblem into a **local MAP or proximal update** of the form

$$
\mathbf{x}_0^\star
\approx
\arg\min_{\mathbf{x}_0}
\underbrace{\frac{\gamma}{2}\|\mathbf{y}-A\mathbf{x}_0\|_2^2}_{\text{data term}}
+
\underbrace{\frac{1}{2}\|\mathbf{x}_0-\hat{\mathbf{x}}_0\|_2^2}_{\text{local prior / trust-region term}}.
\tag{6.1}
$$

Here $\gamma>0$ is a scalar balancing the influence of the noisy data and the prior anchor $\hat{\mathbf{x}}_0$. This formulation admits several complementary interpretations.

---

<h2 id="section6.3.1" style="color: #1E40AF; font-size: 22px; font-weight: bold;">6.3.1 Local Gaussian Prior Approximation</h2>

Suppose that, in a neighborhood of $\hat{\mathbf{x}}_0$, the global prior $p(\mathbf{x}_0)$ can be approximated by a Gaussian

$$
p(\mathbf{x}_0) \approx \mathcal{N}(\hat{\mathbf{x}}_0, I),
$$

i.e.,

$$
-\log p(\mathbf{x}_0) \approx \text{const} + \frac{1}{2}\|\mathbf{x}_0-\hat{\mathbf{x}}_0\|_2^2.
$$

Then the negative log-posterior in this neighborhood is

$$
-\log p(\mathbf{x}_0\mid \mathbf{y})
\approx
\frac{1}{2\sigma_y^2}\|\mathbf{y}-A\mathbf{x}_0\|_2^2
+
\frac{1}{2}\|\mathbf{x}_0-\hat{\mathbf{x}}_0\|_2^2
+ \text{const},
$$


which is exactly of the form (6.1) (up to absorbing $\sigma_y^2$ into $\gamma$). Thus, the local subproblem in DDS can be viewed as a **local MAP problem** that replaces the intractable global prior $-\log p(\mathbf{x}_0)$ by a quadratic approximation around the denoised estimate $\hat{\mathbf{x}}_0$.

Under this interpretation:

- The diffusion prior still governs the global geometry and evolution of $\hat{\mathbf{x}}_0(\mathbf{x}_t,t)$ along the trajectory;  
- Within each time slice, the local term $\|\mathbf{x}_0-\hat{\mathbf{x}}_0\|^2$ instantiates a **local probabilistic surrogate** for $-\log p(\mathbf{x}_0)$ that is tractable and convex.

---

<h2 id="section6.3.2" style="color: #1E40AF; font-size: 22px; font-weight: bold;">6.3.2 Proximal and Trust-Region Viewpoints</h2>

Equation (6.1) can also be recognized as a **proximal subproblem**:

$$
\mathbf{x}_0^\star
=
\operatorname{prox}_{\gamma f}(\hat{\mathbf{x}}_0),
\quad
f(\mathbf{x}_0) = \tfrac{1}{2}\|\mathbf{y}-A\mathbf{x}_0\|_2^2,
$$

where

$$
\operatorname{prox}_{\gamma f}(\hat{\mathbf{x}}_0)
=
\arg\min_{\mathbf{x}_0}
\Big(
\gamma f(\mathbf{x}_0)
+
\tfrac12\|\mathbf{x}_0 - \hat{\mathbf{x}}_0\|^2
\Big).
$$

From this proximal perspective, the DDS clean-space step at time $t$ computes a **noise-aware refinement** of $\hat{\mathbf{x}}_0$ that:

- moves in the direction of decreasing data misfit;  
- but keeps $\mathbf{x}_0^\star$ within a **trust region** around $\hat{\mathbf{x}}_0$ whose size is implicitly controlled by $\gamma$.

This trust-region behavior is crucial in noisy or ill-posed settings:

- It prevents the local update from chasing noise in $\mathbf{y}$ too aggressively;  
- It improves the conditioning of the normal equations
  
  $$
  (\gamma A^\*A + I)\,\mathbf{x}_0 = \hat{\mathbf{x}}_0 + \gamma A^\*\mathbf{y},
  $$
  
  leading to more stable and faster CG convergence;  

- It allows the algorithm designer to explicitly trade off **data fidelity** (via $\gamma$) against **adherence to the prior trajectory** (via the proximity to $\hat{\mathbf{x}}_0$).

Thus, the local prior term serves a dual role: it is both a **local approximation to the generative prior** and a **numerical regularizer** that stabilizes the inner optimization in the presence of measurement noise.

---

<h1 id="section6.4" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">6.4 Algorithmic Structure: Decoupling Prior Trajectory and Clean-Space Optimization</h1>

A key structural feature of the clean-space local-MAP paradigm is the **explicit decoupling** between:

- the **prior trajectory in noisy space** (governed solely by the diffusion model); and  
- the **inverse-problem solve in clean space** (governed solely by $A$ and the data term).

A generic DDS-style iteration at time $t_k$ can be summarized as:

1. **Prior denoising:**
   
   $$
   \hat{\mathbf{x}}_0 = \hat{\mathbf{x}}_0(\mathbf{x}_{t_k}, t_k).
   $$

2. **Local clean-space refinement:** Noiseless case:
   $$
     \mathbf{x}_0^\star 
     \approx 
     \arg\min_{\mathbf{x}_0}
     \frac{1}{2}\|\mathbf{y}-A\mathbf{x}_0\|_2^2
     \quad
     \text{(solved by $M$ CG steps in a Krylov subspace)}.
   $$
   
   Whild for Noisy case:
   
   $$
     \mathbf{x}_0^\star 
     \approx 
     \arg\min_{\mathbf{x}_0}
     \frac{\gamma}{2}\|\mathbf{y}-A\mathbf{x}_0\|_2^2
     +
     \frac{1}{2}\|\mathbf{x}_0-\hat{\mathbf{x}}_0\|_2^2
     \quad
     \text{(again solved by CG)}.
   $$

3. **Re-noising and time stepping:**
   
   $$
   \mathbf{x}_{t_{k-1}} 
   = 
   \texttt{ReNoise}(\mathbf{x}_0^\star, t_{k-1}).
   $$

This structure contrasts sharply with noisy-space posterior-guided samplers, where the prior score and likelihood score are combined into a **single vector field** inside the ODE/SDE solver. Here, the **clean-space optimization is a standalone module** that:

- can be tuned independently (e.g., number of CG steps, choice of regularization, stopping criterion);  
- can exploit problem-specific structure of $A$ (e.g., FFTs for deblurring, NUFFTs for MRI, Radon transforms for CT);  
- does not require backpropagating through the diffusion network (no Jacobians of $\hat{\mathbf{x}}_0$ with respect to $\mathbf{x}_t$).

This decoupling yields a modular design: the diffusion model provides a powerful **learned prior denoiser**, while the local clean-space solver acts as a **classical inverse problem solver** that is plugged into every time step of the diffusion trajectory.

---

<h1 id="section6.5" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">6.5 Geometric Interpretation: Manifold Projection and Measurement Manifold Intersection</h1>

The clean-space local-MAP paradigm admits an intuitive geometric interpretation that parallels the discussion in Chapter 4.

Let $\mathcal{M}_0$ denote the data manifold of clean images. At a given time $t$, the noising process generates a ‚Äúblurred‚Äù manifold $\mathcal{M}_t$ consisting of points $\mathbf{x}_t$ lying in a tubular neighborhood of $\mathcal{M}_0$. A single DDS step at time $t$ can be viewed as:

1. **Manifold projection (prior step).**  
   The mapping $$\mathbf{x}_t \mapsto \hat{\mathbf{x}}_0(\mathbf{x}_t,t)$$ approximately projects $$\mathbf{x}_t$$ onto $$\mathcal{M}_0$$ (or a high-density region of $$p(\mathbf{x}_0)$$). In the small-noise regime, the Jacobian of this mapping approximates the orthogonal projector onto the tangent space $$T_{\hat{\mathbf{x}}_0}\mathcal{M}_0$$.

2. **Clean-space movement along the manifold.**  
   The local optimization in $\mathbf{x}_0$ moves $\hat{\mathbf{x}}_0$ along directions that simultaneously:

   - reduce the data misfit $$\|\mathbf{y}-A\mathbf{x}_0\|$$;  
   - remain within (or close to) the tangent space $$T_{\hat{\mathbf{x}}_0}\mathcal{M}_0$$;  
   - in the noisy case, stay within a trust region centered at $$\hat{\mathbf{x}}_0$$.

   This step therefore pushes the estimate toward the **intersection** of:
   - the generative manifold $$\mathcal{M}_0$$; and  
   - the measurement manifold $$\{\mathbf{x}_0 : A\mathbf{x}_0 \approx \mathbf{y}\}$$.

3. **Re-embedding into the noisy manifold.**  
   The re-noising map $$\mathbf{x}_0^\star \mapsto \mathbf{x}_{t-\Delta t}$$ transports the refined clean point back to the appropriate noisy level, from which the process repeats with a slightly smaller noise scale.

In this picture, the diffusion prior is responsible for **keeping the trajectory close to $\mathcal{M}_0$**, while the clean-space optimization is responsible for **enforcing measurement consistency**. DDS achieves this by alternating between:

- projection-like moves (prior denoising and re-noising), and  
- tangential moves (local ML/MAP in $\mathbf{x}_0$).

---

<h1 id="section6.6" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">6.6 Relation to Noisy-Space Guidance and Plug-and-Play Methods</h1>

From the unified perspective developed in Chapters 3‚Äì5, clean-space local-MAP sampling sits at the intersection of:

- **posterior-guided diffusion** (DPS/MCG-style methods), and  
- **plug-and-play / RED** approaches in classical inverse problems.

Compared to noisy-space posterior-guided sampling (DPS/MCG):

- Both paradigms seek to approximate the posterior $p(\mathbf{x}_0\mid \mathbf{y})$ by modifying a diffusion trajectory;  
- Noisy-space methods operate directly on $\mathbf{x}_t$ and require Jacobian‚Äìvector products through the denoiser network to transport the likelihood gradient into noisy space;  
- Clean-space methods operate on $\mathbf{x}_0$, solve a local ML/MAP problem using only $A$ and $A^\*$, and then re-noise to maintain compatibility with the diffusion trajectory.

Compared to plug-and-play and RED:

- PnP/RED treat a generic denoiser $D_\sigma$ as an implicit prior and insert it into iterative optimization schemes for inverse problems;  
- DDS can be viewed as a **time-continuous PnP framework**, where the denoiser is the diffusion model‚Äôs $x_0$-prediction network and the outer loop follows a decreasing noise schedule;  
- The local clean-space subproblem (with or without proximal regularization) plays the role of a **data-consistency module**, akin to the gradient step in PnP-ADMM or PnP-FISTA.

Thus, the clean-space local-MAP paradigm can be interpreted as an instance of **diffusion-based PnP** where:

- the prior is expressed as a time-dependent denoiser along the diffusion path;  
- the inverse problem is solved by alternating between **denoising** and **data-consistency** in the clean space, with diffusion re-noising ensuring global consistency across time.

---

<h1 id="section6.7" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">
6.7 Design Choices and Open Questions
</h1>

The clean-space local-MAP paradigm exposes several important design degrees of freedom:

- **Number of inner iterations.**  
  The number $M$ of CG steps per time level controls the strength of the data-consistency enforcement. Small $M$ yields light-touch corrections akin to gradient steps, while large $M$ approaches solving the local subproblem to high accuracy. The optimal trade-off between computational cost and reconstruction quality remains problem-dependent.

- **Scheduling of $\gamma_t$ and noise levels.**  
  In noisy settings, the parameter $\gamma_t$ modulates the relative weight of data and local prior in (6.1). It is natural to consider time-varying schedules where:
  - early (high-noise) stages rely more strongly on the prior (smaller $\gamma_t$), and  
  - later (low-noise) stages increasingly trust the data (larger $\gamma_t$).

- **Choice of search subspace.**  
  Beyond classical Krylov subspaces, one may consider learned or structure-aware subspaces that better capture the local geometry of $\mathcal{M}_0$ and the action of $A$.

- **Extensions beyond linear measurements.**  
  The current formulation assumes a linear $A$. Extending the clean-space local-MAP paradigm to nonlinear forward models raises new challenges: the inner problem may become non-convex, and efficient solvers must exploit both the structure of $A$ and the properties of the diffusion prior.

These open questions highlight that DDS-style clean-space local-MAP sampling is not merely an algorithmic variant of posterior-guided diffusion, but rather a distinct and flexible paradigm. It bridges global generative priors and classical inverse problem optimization by embedding a **sequence of local ML/MAP solves** into a diffusion trajectory, and thereby offers a principled route to leveraging powerful learned priors within well-understood optimization frameworks.




---

<h1 id="section3" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">3. References</h1>


[^tikhonov]: Tikhonov A N, Arsenin V Y. Solutions of ill-posed problems\[M]. Washington, DC: Winston, 1977.

[^tarantola]: Tarantola A. Inverse problem theory and methods for model parameter estimation\[M]. Philadelphia: SIAM, 2005.

[^kaipio]: Kaipio J, Somersalo E. Statistical and computational inverse problems\[M]. New York: Springer, 2005.

[^tv]: Rudin L I, Osher S, Fatemi E. Nonlinear total variation based noise removal algorithms\[J]. Physica D: Nonlinear Phenomena, 1992, 60(1‚Äì4): 259-268.

[^cs-donoho]: Donoho D L. Compressed sensing\[J]. IEEE Transactions on Information Theory, 2006, 52(4): 1289-1306.

[^cs-candes]: Cand√®s E J, Romberg J, Tao T. Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information\[J]. IEEE Transactions on Information Theory, 2006, 52(2): 489-509.

[^ista]: Daubechies I, Defrise M, De Mol C. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint\[J]. Communications on Pure and Applied Mathematics, 2004, 57(11): 1413-1457.

[^fista]: Beck A, Teboulle M. A fast iterative shrinkage-thresholding algorithm for linear inverse problems\[J]. SIAM Journal on Imaging Sciences, 2009, 2(1): 183-202.

[^admm]: Boyd S, Parikh N, Chu E, *et al*. Distributed optimization and statistical learning via the alternating direction method of multipliers\[J]. Foundations and Trends in Machine Learning, 2011, 3(1): 1-122.

[^nlm]: Buades A, Coll B, Morel J M. A non-local algorithm for image denoising\[C]//IEEE Computer Society Conference on Computer Vision and Pattern Recognition. San Diego: IEEE, 2005: 60-65.

[^bm3d]: Dabov K, Foi A, Katkovnik V, *et al*. Image denoising by sparse 3D transform-domain collaborative filtering\[J]. IEEE Transactions on Image Processing, 2007, 16(8): 2080-2095.

[^nuclear]: Cand√®s E J, Recht B. Exact matrix completion via convex optimization\[J]. Foundations of Computational Mathematics, 2009, 9(6): 717-772.

[^pnp]: Venkatakrishnan S V, Bouman C A, Wohlberg B. Plug-and-play priors for model based reconstruction\[C]//2013 IEEE Global Conference on Signal and Information Processing. Austin: IEEE, 2013: 945-948.

[^red]: Romano Y, Elad M, Milanfar P. The little engine that could: regularization by denoising (RED)\[J]. SIAM Journal on Imaging Sciences, 2017, 10(4): 1804-1844.

[^gan]: Goodfellow I J, Pouget-Abadie J, Mirza M, *et al*. Generative adversarial nets\[C]//Advances in Neural Information Processing Systems 27. Montreal: Curran Associates, 2014: 2672-2680.

[^gan-cs]: Bora A, Jalal A, Price E, *et al*. Compressed sensing using generative models\[C]//International Conference on Machine Learning. Sydney: PMLR, 2017: 537-546.

[^vae]: Kingma D P, Welling M. Auto-encoding variational Bayes\[C]//2nd International Conference on Learning Representations. Banff, 2014.

[^realnvp]: Dinh L, Sohl-Dickstein J, Bengio S. Density estimation using Real NVP\[C]//International Conference on Learning Representations. Toulon, 2017.

[^glow]: Kingma D P, Dhariwal P. Glow: Generative flow with invertible 1√ó1 convolutions\[C]//Advances in Neural Information Processing Systems 31. Montreal: Curran Associates, 2018: 10215-10224.

[^ebm]: LeCun Y, Chopra S, Hadsell R, *et al*. A tutorial on energy-based learning\[M]//Bakir G, Hofmann T, Sch√∂lkopf B, *et al*., eds. Predicting Structured Data. Cambridge, MA: MIT Press, 2006: 191-246.

[^sde-score]: Song Y, Sohl-Dickstein J, Kingma D P, *et al*. Score-based generative modeling through stochastic differential equations\[C]//International Conference on Learning Representations. 2021.

[^ddpm]: Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models\[C]//Advances in Neural Information Processing Systems 33. 2020: 6840-6851.

[^ddim]: Song J, Meng C, Ermon S. Denoising diffusion implicit models\[C]//International Conference on Learning Representations. 2021.

[^sbi-ip]: Song Y, Shen L, Xing L, *et al*. Solving inverse problems in medical imaging with score-based generative models\[C]//International Conference on Learning Representations. 2022.

[^dps]: Chung H, Ye J C. Diffusion posterior sampling for general noisy inverse problems\[J]. SIAM Journal on Imaging Sciences, 2023, 16(1): 1-33.

[^mcg]: Chung H, Ryu E, Ye J C. Improving diffusion models for inverse problems using manifold constraints\[J]. IEEE Journal of Selected Areas in Information Theory, 2022, 3(3): 529-544.

[^dds]: Chung H, Lee S, Ye J C. Decomposed diffusion sampler for accelerating large-scale inverse problems\[C]//Proceedings of the 12th International Conference on Learning Representations. 2024.
