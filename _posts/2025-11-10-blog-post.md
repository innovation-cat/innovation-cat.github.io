---
title: 'Inverse Problems with Generative Priors'
date: 2025-11-10
permalink: /posts/2025/11/inverse-problems/
tags:
  - Flow Matching
  - Diffusion Model
  - Consistency Models
  - Trajectory
  - Distillation
---



<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>üìö Table of Contents</strong></span>
  </summary>
  <ul>
    <li><a href="#section1">1. The Probabilistic Formulation of Inverse Problems</a>
		<ul>
		  <li><a href="#section1.1">1.1 Inverse Problems without Priors: Maximum Likelihood Estimation</a></li>
		  <li><a href="#section1.2">1.2 Inverse Problems with Priors: Maximum A Posteriori Estimation
</a></li>
		</ul>
	</li>
	<li><a href="#section2">2. Classical Regularization via Implicit Priors</a>
		<ul>
		  <li><a href="#section2.1">2.1 Smoothness Priors</a></li>
		  <li><a href="#section2.2">2.2 Sparsity Priors</a></li>
		  <li><a href="#section2.3">2.3 Self-Similarity and Low-Rank Priors</a></li>
		  <li><a href="#section2.4">2.4 Learning-Based Implicit Priors</a></li>
		  <li><a href="#section2.5">2.5 Summary of Classical Regularization Priors</a></li>
		</ul>
	</li>
	<li><a href="#section3">3. Deep Generative Models as Explicit Priors</a>
		<ul>
		  <li><a href="#section3.1">3.1 Generative Adversarial Networks: Manifold Priors</a></li>
		  <li><a href="#section3.2">3.2 Variational Autoencoders: Latent-Variable Priors</a></li>
		  <li><a href="#section3.3">3.3 Normalizing Flows: Exact Log-Density Priors</a></li>
		  <li><a href="#section3.4">3.4 Energy-Based Models: Explicit Energy Priors</a></li>
		</ul>
	</li>
	<li><a href="#section4">4. Diffusion-Based and Flow-Based Models as Generative Priors</a></li>
	<li><a href="#section11">11. References</a></li>
  </ul>
</details>




Inverse problems, which aim to recover a signal of interest from indirect and often corrupted measurements, are a cornerstone of computational science and engineering. These problems are typically ill-posed, necessitating the use of prior knowledge to regularize the solution space and ensure a unique and stable reconstruction. This paper provides a structured exposition of the evolution of priors in solving inverse problems, from classical formulations to the modern paradigm of deep generative models. We begin by formalizing the inverse problem from a probabilistic perspective, deriving the Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP) frameworks, which correspond to solutions without and with priors, respectively. We then review classical, non-generative regularization techniques, focusing on the influential Plug-and-Play (PnP) and Regularization by Denoising (RED) frameworks, which leverage off-the-shelf denoisers as implicit priors. The core of this paper is dedicated to the contemporary approach of employing deep generative models‚Äîincluding Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Normalizing Flows, and Diffusion Models‚Äîas powerful, explicit priors. We detail how the unique properties of each model class can be integrated into the inverse problem objective, enabling state-of-the-art performance by constraining solutions to a learned data manifold. This work aims to bridge the conceptual gap between classical and modern techniques, offering a unified view of the role of priors in solving generative inverse problems.

---

<h1 id="section1" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">1. The Probabilistic Formulation of Inverse Problems</h1>



An inverse problem seeks to recover an unknown signal $\mathbf{x} \in \mathbb{R}^n$ from a set of observed measurements $\mathbf{y} \in \mathbb{R}^m$. This process is typically modeled by a linear forward operator $\mathbf{A}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ and corrupted by additive noise $\mathbf{n} \in \mathbb{R}^m$:

$$
\mathbf{y} = \mathbf{A}\mathbf{x} + \mathbf{n}, \qquad \mathbf{n} \sim \mathcal{N}(0, \sigma\,I)
$$

The operator $\mathbf{A}$ encodes the physics or geometry of the measurement process. For example:

- In **image deblurring**, $\mathbf{A}$ is a convolution with a blur kernel.  
- In **super-resolution** or **compressed sensing**, $\mathbf{A}$ is a subsampling or projection operator.  
- In **computed tomography (CT)**, $\mathbf{A}$ is a (discretized) Radon transform.  
- In **magnetic resonance imaging (MRI)**, $\mathbf{A}$ corresponds to a subsampled Fourier transform.  

Beyond imaging, similar formulations appear in **audio denoising and deconvolution**, **geophysical exploration**, **astronomical imaging**, and more generally in **parameter estimation** for scientific systems.

The problem is deemed "inverse" because we seek to reverse the effect of $\mathbf{A}$. It is often **ill-posed**, meaning that a solution may not exist, may not be unique, or may not depend continuously on the data. This ill-posedness arises when $\mathbf{A}$ is rank-deficient or ill-conditioned ($m < n$ or singular values decay rapidly), making the recovery of $\mathbf{x}$ from $\mathbf{y}$ an ambiguous task.


---

<h1 id="section1.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">1.1 Inverse Problems without Priors: Maximum Likelihood Estimation</h1> 


In the absence of any prior knowledge about the signal $\mathbf{x}$, our estimation relies solely on the data formation model. A common and mathematically convenient assumption is that the noise $\mathbf{n}$ is independent and identically distributed (i.i.d.) Gaussian with zero mean and variance $\sigma^2$, i.e., 

$$\mathbf{n} \sim \mathcal{N}(\mathbf{0}, \sigma^2\mathbf{I})$$

This assumption allows us to formulate the problem within the framework of **Maximum Likelihood Estimation (MLE)**. The likelihood function $p(\mathbf{y}\mid \mathbf{x})$ describes the probability of observing the measurements $\mathbf{y}$ given a specific signal $\mathbf{x}$. Under the Gaussian noise assumption, the relationship $\mathbf{y} = \mathbf{A}\mathbf{x} + \mathbf{n}$ implies that $\mathbf{y}$ is also Gaussian-distributed, centered at $\mathbf{A}\mathbf{x}$:

$$
p(\mathbf{y}\mid \mathbf{x}) = \mathcal{N}(\mathbf{y}; \mathbf{A}\mathbf{x}, \sigma^2\mathbf{I})
$$

The probability density function for this multivariate Gaussian distribution is given by:

$$
p(\mathbf{y}\mid \mathbf{x}) = \frac{1}{(2\pi\sigma^2)^{m/2}} \exp \left(\,-\frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2\, \right)
$$

The MLE principle seeks the estimate $\hat{\mathbf{x}}_{\text{MLE}}$ that maximizes this likelihood. For computational stability and simplicity, it is standard practice to maximize the log-likelihood instead:

$$
\hat{\mathbf{x}}_{\text{MLE}} = \arg\max_{\mathbf{x}} \log p(\mathbf{y}\mid \mathbf{x}) = \arg\max_{\mathbf{x}} \left[ -\frac{m}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 \right]
$$

Since the first term is a constant with respect to $\mathbf{x}$, and maximizing the negative of a function is equivalent to minimizing the function itself, the MLE objective simplifies to a least-squares problem:

$$
\hat{\mathbf{x}}_{\text{MLE}} = \arg\min_{\mathbf{x}} \underbrace{\,\frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2\, }_{\text{data consistency}}
$$

This objective is also known as the Mean Squared Error (MSE) data fidelity term. While principled, it is insufficient for ill-posed problems, as many different signals $\mathbf{x}$ can yield a similarly small residual $\|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2$, often resulting in solutions dominated by noise.


---

<h1 id="section1.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">1.2 Inverse Problems with Priors: Maximum A Posteriori Estimation</h1> 

To overcome the limitations of MLE, we introduce prior knowledge about the expected properties of the signal $\mathbf{x}$. This is formalized using a Bayesian approach. We define a prior distribution $p(\mathbf{x})$ that assigns higher probability to signals that are plausible (e.g., natural images with smooth regions and sharp edges) and lower probability to others (e.g., pure noise).

Using Bayes' theorem, we can combine the likelihood $p(\mathbf{y}\mid \mathbf{x})$ with the prior $p(\mathbf{x})$ to obtain the posterior distribution $p(\mathbf{x}\mid \mathbf{y})$:

$$
p(\mathbf{x}\mid \mathbf{y}) = \frac{p(\mathbf{y}\mid \mathbf{x})p(\mathbf{x})}{p(\mathbf{y})}
$$

The posterior represents our updated belief about $\mathbf{x}$ after observing the data $\mathbf{y}$. The goal of **Maximum A Posteriori (MAP)** estimation is to find the signal $\hat{\mathbf{x}}_{\text{MAP}}$ that maximizes this posterior probability:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\max_{\mathbf{x}} p(\mathbf{x}\mid \mathbf{y}) = \arg\max_{\mathbf{x}} \frac{p(\mathbf{y}\mid \mathbf{x})p(\mathbf{x})}{p(\mathbf{y})}
$$

Since the denominator $p(\mathbf{y})$ is constant with respect to $\mathbf{x}$, the MAP objective becomes:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\max_{\mathbf{x}} p(\mathbf{y}\mid \mathbf{x})p(\mathbf{x})
$$

Again, we work with the log-posterior for convenience:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\max_{\mathbf{x}} \left[ \log p(\mathbf{y}|\mathbf{x}) + \log p(\mathbf{x}) \right]
$$

Substituting the log-likelihood term from the Gaussian noise model, we get:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\max_{\mathbf{x}} \left[ -\frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 + \log p(\mathbf{x}) \right]
$$

This is equivalent to minimizing the negative log-posterior:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\min_{\mathbf{x}} \left[ \frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 - \log p(\mathbf{x}) \right]
$$

This formulation elegantly connects Bayesian inference to regularized optimization. If we define a regularization function $R(\mathbf{x}) = -\log p(\mathbf{x})$ and absorbing all constants into a free regularization weight $\lambda > 0$, the MAP estimation is precisely equivalent to solving the following optimization problem:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\min_{\mathbf{x}} \left[ \underbrace{\frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2}_{\text{data consistency}} + \underbrace{\lambda R(\mathbf{x})}_{\text{Prior (Regularization)}} \right]
$$

Here, the data fidelity term ensures consistency with the measurements, while the regularization term $R(\mathbf{x})$ enforces the prior. The choice of $R(\mathbf{x})$ is paramount and has been the subject of extensive research.

---

<h1 id="section2" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">2. Classical Regularization with Non-Generative Priors</h1>



Before the widespread adoption of deep generative models, the regularization term $R(\mathbf{x})$ in the MAP objective was primarily designed based on handcrafted statistical assumptions about the signal. 

These priors do **not** attempt to model the full high-dimensional distribution $p(\mathbf{x})$; instead, they encode specific structural properties‚Äîsuch as smoothness, sparsity, or non-local self-similarity‚Äîthat are believed to hold for natural signals.


---

<h1 id="section2.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.1 Smoothness Priors (Handcrafted Explicit Priors)</h1> 


<strong>Core assumption:</strong> Natural signals, such as images, are predominantly smooth or piecewise-constant. High-frequency variations are often attributed to noise rather than the underlying signal structure. Representative Regularizers including:

1. **Tikhonov Regularization:** This is one of the earliest and most fundamental regularizers, penalizing the L2-norm of the signal's derivatives. From a Bayesian perspective, this corresponds to assuming a **zero-mean Gaussian prior** on the signal's derivative, i.e.,

   $$
   p(\mathbf{L}\mathbf{x}) \propto \exp \left(- \| \mathbf{L}\mathbf{x} \|_2^2 / 2\tau^2\right)\,\Longrightarrow\,R(\mathbf{x}) = -\log p(\mathbf{L}\mathbf{x}) = \|\mathbf{L}\mathbf{x}\|_2^2
   $$

   
   where $\mathbf{L}$ is a linear operator, typically a finite-difference approximation of the gradient or Laplacian. 

2. **Total Variation (TV):** A significant advancement over Tikhonov, the TV regularizer is defined as the L1-norm of the signal's gradient magnitude, this is equivalent to imposing a **Laplacian prior** on the gradient magnitudes, i.e.,

   $$
   p(\nabla \mathbf{x}) \propto \exp \left(- \|\nabla \mathbf{x}\|_1 / \tau\right)\,\Longrightarrow\,R(\mathbf{x}) =  -\log p(\nabla \mathbf{x}) = \|\nabla \mathbf{x}\|_1
   $$

   Unlike the L2-norm, which penalizes large gradients heavily, the L1-norm promotes solutions where gradients are sparse, effectively preserving sharp edges while enforcing smoothness in flat regions.

   
   
   


---

<h1 id="section2.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.2 Sparsity Priors</h1> 




<strong>Core assumption:</strong> While signals may not be sparse in their native domain (e.g., pixel space), they often admit a sparse representation in a suitable transform domain.

Let $\mathbf{\Psi}$ denote a transform (orthonormal basis, frame, or learned dictionary), and let
$$
\boldsymbol{\alpha} = \mathbf{\Psi}\mathbf{x}
$$
be the transform coefficients. Sparsity priors posit that most entries of $\boldsymbol{\alpha}$ are zero or near-zero. This choice can be viewed as imposing a **Laplace prior** on the coefficients, which is well known to promote sparsity, i.e.,

$$
p(\mathbf{\Psi}\mathbf{x}) \propto \exp \left(- \|\mathbf{\Psi}\mathbf{x}\|_1 / \tau\right) \,\Longrightarrow\, R(\mathbf{x}) = \|\mathbf{\Psi}\mathbf{x}\|_1.
$$

Depending on the application, $\mathbf{\Psi}$ may be:

- A fixed analytical transform (e.g., wavelets for piecewise-smooth images, DCT for photographic content).  
- A data-driven dictionary learned from example patches.  

Sparsity priors play a central role in **Compressed Sensing**, where one proves that sparse signals can be recovered from undersampled measurements under appropriate conditions on $\mathbf{A}$ and $\mathbf{\Psi}$.

While sparsity priors are powerful for signals with a known sparsifying transform and enjoy strong theoretical guarantees, they depend critically on choosing a suitable transform. They can also bias the magnitude of coefficients (shrinkage), potentially oversuppressing subtle but important details.





---

<h1 id="section2.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.3 Self-Similarity and Low-Rank Priors</h1> 


<strong>Core assumption:</strong> Natural images exhibit rich **non-local self-similarity**: small patches tend to repeat across the image, either exactly or with mild variations. A common way to exploit this is:

1. Extract overlapping patches from $\mathbf{x}$.  
2. Group together patches that are similar (e.g., by Euclidean distance in patch space).  
3. Stack each group of similar patches as columns of a matrix $\mathbf{P}_{\mathbf{x}}$.  

Within each group, the columns of $$\mathbf{P}_{\mathbf{x}}$$ are highly correlated, so $$\mathbf{P}_{\mathbf{x}}$$ is approximately low-rank. This suggests imposing a **low-rank prior** via the **nuclear norm** (sum of singular values), which convexly relaxes the rank:

$$
R(\mathbf{x}) = \|\mathbf{P}_{\mathbf{x}}\|_*.
$$

Prominent algorithms such as Non-Local Means (NL-Means) and BM3D operationalize this idea by combining patch grouping, collaborative filtering, and transform-domain shrinkage. These methods captured state-of-the-art performance in image denoising and related tasks for many years.

Self-similarity and low-rank priors are particularly effective for images with repetitive patterns and structured textures. However, they require non-trivial patch grouping and can be computationally demanding. Moreover, their assumptions may break down for objects or regions that are unique within the image, leading to over-smoothing or ghosting artifacts.



---

<h1 id="section2.4" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.4 Learning-Based Implicit Priors</h1> 



The priors discussed so far are defined by explicit analytical forms of $R(\mathbf{x})$. A natural evolution is to let data drive the prior, but still within the traditional optimization framework. **Learning-based implicit priors** achieve this by replacing explicit regularizers with powerful, pre-trained **denoisers**.

<strong>Core assumption:</strong> The manifold of natural signals can be characterized implicitly by the action of a high-quality denoiser $\mathcal{D}(\cdot)$. Clean signals are (approximate) fixed points, $\mathcal{D}(\mathbf{x}) \approx \mathbf{x}$, whereas noisy inputs are pushed towards the signal manifold. Two influential frameworks are:

1. <strong>Plug-and-Play (PnP) Priors.</strong>  Many classical algorithms for solving the MAP problem
   
   $$
   \arg\min_{\mathbf{x}} \frac{1}{2\sigma^2}\|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 + \lambda R(\mathbf{x})
   $$
   
   (e.g., ADMM, Half-Quadratic Splitting) alternate between a **data-fidelity step** and a **proximal step** of the form
   
   $$
   \text{prox}_{\lambda R}(\mathbf{z}) 
   = \arg\min_{\mathbf{x}} 
   \left[ 
     \frac{1}{2}\|\mathbf{x} - \mathbf{z}\|_2^2 + \lambda R(\mathbf{x})
   \right].
   $$
   
   For certain choices of $R(\mathbf{x})$ and under a Gaussian noise model, this proximal operator can be interpreted as a denoising operation.

   PnP methods take the **reverse perspective**: they start from a strong, possibly black-box denoiser $\mathcal{D}(\cdot)$ (e.g., BM3D or a deep CNN) and simply **replace** $\text{prox}_{\lambda R}(\cdot)$ with $\mathcal{D}(\cdot)$ inside the optimization algorithm. In doing so, they define an **implicit prior** via the behavior of the denoiser, without ever writing down $R(\mathbf{x})$ explicitly.

2. <strong>Regularization by Denoising (RED).</strong>  RED goes a step further by attempting to construct an **explicit energy** from a denoiser. Given a denoising operator $\mathcal{D}(\mathbf{x})$, RED defines a vector field
   
   $$
   \nabla R(\mathbf{x}) = \mathbf{x} - \mathcal{D}(\mathbf{x}),
   $$
   
   and uses it as the gradient of a regularization function $R(\mathbf{x})$. Under mild conditions on $\mathcal{D}$ (e.g., local homogeneity and a symmetric Jacobian), this vector field is conservative, so there exists a scalar functional $R(\mathbf{x})$ whose gradient matches the denoiser residual. This enables the use of standard gradient-based methods to minimize
   
   $$
   \arg\min_{\mathbf{x}} 
   \left[
     \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 + \lambda R(\mathbf{x})
   \right],
   $$
   
   where the gradient of $R$ is implemented via $\mathbf{x} - \mathcal{D}(\mathbf{x})$.

Learning-based implicit priors form a bridge between handcrafted regularization and fully generative models: they leverage powerful learned representations (denoisers) while retaining the structure of classical optimization algorithms. However, they still do not provide an explicit, normalized model of $p(\mathbf{x})$, and questions about the exact form or even existence of the underlying prior can be subtle.





  
---

<h1 id="section2.5" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.5 Summary of Classical Regularization Priors</h1> 

We summarize and compare the classic implicit priors listed above using the following table



| Prior Category | Core Assumption | Representative Regularizers / Methods | Advantages | Disadvantages |
| :--- | :--- | :--- | :--- | :--- |
| **Smoothness Priors** | Signals are predominantly smooth or piecewise-constant. High-frequency variations are noise. | ‚Ä¢ Tikhonov: $\|\mathbf{L}\mathbf{x}\|_2^2$ <br> ‚Ä¢ Total Variation (TV): $\|\nabla \mathbf{x}\|_1$ | ‚Ä¢ Simple, convex, and computationally efficient. <br> ‚Ä¢ Effective for signals that are inherently smooth or blocky. | ‚Ä¢ **Tikhonov**: Oversmooths, blurring sharp edges. <br> ‚Ä¢ **TV**: Can create "staircasing" artifacts. <br> ‚Ä¢ Both are too simple for complex textures. |
| **Sparsity Priors** | Signals have a sparse representation in a suitable transform domain (e.g., Wavelets, DCT). | ‚Ä¢ L1-norm of transform coefficients: $\|\mathbf{\Psi}\mathbf{x}\|_1$ <br> ‚Ä¢ Foundational to Compressed Sensing. | ‚Ä¢ Strong theoretical guarantees for recovery under certain conditions. <br> ‚Ä¢ Excellent for signals with a known sparsifying basis. | ‚Ä¢ Relies on finding a suitable transform $\mathbf{\Psi}$. <br> ‚Ä¢ Not effective for signals without a sparse representation. <br> ‚Ä¢ Can introduce bias in coefficient amplitudes. |
| **Self-Similarity & Low-Rank Priors** | Natural signals contain repetitive patterns. Grouped similar patches form a low-rank matrix. | ‚Ä¢ **Methods**: NL-Means, BM3D <br> ‚Ä¢ **Regularizer**: Nuclear Norm of patch matrix, $\|\mathbf{P}_{\mathbf{x}}\|_*$ | ‚Ä¢ Captures complex non-local structures and textures effectively. <br> ‚Ä¢ Achieved state-of-the-art results for many years. | ‚Ä¢ Computationally very expensive due to patch searching and grouping. <br> ‚Ä¢ Can introduce patch-based artifacts. <br> ‚Ä¢ Difficult to formulate as a simple, explicit regularizer. |
| **Learning-Based Implicit Priors** | The manifold of natural signals can be implicitly defined by the action of a powerful denoiser. | ‚Ä¢ **Plug-and-Play (PnP)**: Replaces proximal operator with a denoiser $\mathcal{D}(\cdot)$. <br> ‚Ä¢ **Regularization by Denoising (RED)**: Defines prior gradient as $\nabla R(\mathbf{x}) = \mathbf{x} - \mathcal{D}(\mathbf{x})$. | ‚Ä¢ Highly flexible and modular; leverages SOTA denoisers (incl. deep nets). <br> ‚Ä¢ Excellent empirical performance without needing to train a full generative model. | ‚Ä¢ A "black box" prior. <br> ‚Ä¢ Convergence guarantees can be weak, especially for PnP with complex denoisers. <br> ‚Ä¢ Performance is entirely dependent on the quality and domain of the pre-trained denoiser. |  
  

While powerful, these classical and implicit methods do not model the full data distribution $p(\mathbf{x})$. This limits their ability to, for instance, quantify uncertainty or generate diverse solutions consistent with the measurements. This limitation motivates the paradigm shift towards using explicit deep generative models as priors, as we will explore in the next section.
  
  
---

<h1 id="section3" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">3. Classical Generative Models as Explicit Priors</h1>  
  
  

PnP and RED use denoisers as powerful but implicit priors. The latest paradigm shift involves using deep generative models that explicitly learn the entire data distribution $p(\mathbf{x})$ from a training set. This allows for a more direct and expressive enforcement of the prior. The core idea is to **constrain the solution $\mathbf{x}$ to lie on the low-dimensional manifold learned by the model.**


---

<h1 id="section3.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.1 Generative Adversarial Networks: Manifold Priors</h1> 

A standard GAN, consisting of a generator $\mathcal{G}(\mathbf{z})$ and a discriminator $\mathcal{D}$. However, both of them are not provide a tractable method for evaluating $p(\mathbf{x})$ for an arbitrary signal $\mathbf{x}$. 

Therefore, applying a GAN prior does not involve formulating an explicit regularizer. Instead, the prior is enforced through a **reparameterization of the solution space**. It acts as a powerful **manifold constraint**, fundamentally changing the nature of the optimization problem itself.

Suppose a pre-trained GAN generator

$$
\mathcal{G}: \mathcal{Z}  \rightarrow \mathbb{R}^n\,\qquad \text{where}\, \mathcal{Z} \in \mathbb{R}^d\quad \text{and}\quad d \ll n
$$ 

learns a mapping from a simple, low-dimensional latent space $\mathcal{Z}$ to the high-dimensional signal space $\mathbb{R}^n$. The core assumption when using a GAN as a prior is that all plausible signals (e.g., natural images) lie on or very close to the low-dimensional manifold defined by the **range of the generator**. We denote this manifold as $\mathcal{M}$:

$$
\mathcal{M} = \{ \mathcal{G}(\mathbf{z}) \mid \mathbf{z} \in \mathcal{Z} \}
$$


This manifold assumption can be conceptually translated into an explicit, albeit impractical, regularizer. This regularizer would be an indicator function for the manifold $\mathcal{M}$:

$$
R_{\text{GAN}}(\mathbf{x}) = \begin{cases} -\log p(z) & \text{if } \mathbf{x}=\mathcal{G}(\mathbf{z}) \in \mathcal{M} \\ \\
+\infty & \text{if } \mathbf{x} \notin \mathcal{M} \end{cases}
$$

If we were to plug this into the MAP objective, we would get:

$$
\begin{align}
\hat{\mathbf{x}} & = \arg\min_{\mathbf{x} \in \mathbb{R}^n} \left( \frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 + \lambda R_{\text{GAN}}(\mathbf{x}) \right) \\[10pt]
\Longrightarrow \hat{\mathbf{z}} & = \arg\min_{\mathbf{z}} \left( \frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}(\mathcal{G}(\mathbf{z}))\|_2^2 - \lambda \log p(z) \right)
\end{align}
$$
    
This objective forces the solution $\mathbf{x}$ to be strictly within the manifold $\mathcal{M}$ (to avoid the infinite penalty), and among all possible solutions on the manifold, it finds the one that best fits the measurements $\mathbf{y}$.

$$
-\log p(\mathbf{z}) = -\log \left( \frac{1}{(2\pi)^{d/2}} \exp\left(-\frac{1}{2}\|\mathbf{z}\|_2^2\right) \right) = \frac{1}{2}\|\mathbf{z}\|_2^2 + \text{const.}
$$

This leads to the most common and practical objective function for GAN-based inverse problems:

$$
\hat{\mathbf{z}} = \arg\min_{\mathbf{z} \in \mathcal{Z}} \left( \frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}(\mathcal{G}(\mathbf{z}))\|_2^2 + \lambda \|\mathbf{z}\|_2^2 \right)
$$

Minimal python-like pseudocode summaries and  detailed inline comments:

```python
# GAN prior: optimize latent code z so that generated image G(z) fits measurements.
# This is the canonical "latent-MAP" for implicit-manifold priors (no tractable p(x)).
# Inputs:
#   A, y, G (generator), sigma2 (noise variance), lambda_z (latent prior weight)
# Hyperparams: eta (step size), K (iterations)

# Define Gaussian negative log-likelihood gradient for x (pixel space)
def fidelity_grad_x(x):
    # For Gaussian noise: grad_x (1/(2œÉ^2) ||Ax-y||^2) = (1/œÉ^2) A^T(Ax - y)
    return (1.0 / sigma2) * A.T( A(x) - y )

# Initialize z
z = z_init  # e.g., random normal, or encoder warm-start if available

for k in range(K):
    # Forward: map latent to pixels
    x = G(z)

    # Data-fidelity gradient (w.r.t. x) then push to z via vector-Jacobian product
    g_x = fidelity_grad_x(x)
    # Use autodiff VJP: g_z_data = (dG/dz)^T * g_x
    g_z_data = vjp(G, z, g_x)  # pseudo-call: compute vector-Jacobian product

    # Prior gradient in latent space: Gaussian prior p(z)=N(0,I) -> -‚àá log p(z) = z
    g_z_prior = z  # gradient of (1/2)||z||^2

    # Total gradient in z
    g_z = g_z_data + lambda_z * g_z_prior

    # Gradient step (could use Adam instead)
    z = z - eta * g_z

# Return reconstructed image
x_hat = G(z)
```



---

<h1 id="section3.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.2 Variational Autoencoders: Latent-Variable Priors</h1> 

VAE define a latent-variable generative model with an decoder (generator):

$$
x\sim p_\theta(x\mid z), \qquad \text{where}\,z\sim p(z)=\mathcal{N}(0,I)
$$

and an encoder $q_\phi(z\mid x)$. This allows the marginal likelihood $\log p_\theta(x)$ to be lower-bounded by the well-known Evidence Lower Bound (ELBO):

$$
\log p_\theta(x)\;\ge\; \mathbb{E}_{q_\phi(z\mid x)}\!\left[\log p_\theta(x\mid z)\right]-
D_{\mathrm{KL}}\!\left(q_\phi(z\mid x)\,\|\, p(z)\right)
\triangleq \mathrm{ELBO}(x;\theta,\phi).
$$

At first glance, the ELBO appears to offer a tractable surrogate for $-\log p(x)$, 

$$
\hat{\mathbf{x}} = \arg\min_{\mathbf{x}} \left( \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 - \lambda \cdot \text{ELBO}(\mathbf{x}) \right)
$$

However, this approach is problematic for several reasons:

1. **Problem 1:** The most significant issue is that the ELBO is a function of $\mathbf{x}$ **through the encoder** $q_{\phi}(\mathbf{z}\mid \mathbf{x})$. During the inverse problem solving phase, we are searching for an unknown, clean signal $\mathbf{x}$.

   When we try to minimize the objective with respect to $\mathbf{x}$, the gradient of the regularization term, $\nabla_{\mathbf{x}} \text{ELBO}(\mathbf{x})$, must be computed. This involves backpropagating through the entire encoder network $q_{\phi}$, the KL divergence calculation, and the expectation term. This creates an extremely complex, high-dimensional, and likely non-convex optimization landscape. The optimization is performed on the variable $\mathbf{x}$ which is simultaneously the **input** to the encoder.

2. **Problem 2:** The "Gap" Problem. ELBO is Only a Lower Bound, the inequality 

   $$\log p(\mathbf{x}) \ge \text{ELBO}(\mathbf{x})$$ 
   
   is central. This gap can be non-zero and highly variable.


Due to the above limitations, VAE-based inverse problem solvers almost universally adopt a **latent-space MAP formulation**. Since the VAE defines a simple Gaussian prior over (z), one instead parameterizes the reconstructed signal as

$$
x = G_\theta(z),
$$

where $G_\theta$ is the VAE decoder (the conditional mean of $p_\theta(x\mid z)$). The prior reduces to

$$
-\log p(z) = \tfrac{1}{2}\|z\|_2^2
$$

yielding the latent-variable MAP objective:

$$
z^\star = \arg\min_{z}
\|A(G_\theta(z)) - y\|^2\;+\;\lambda \|z\|_2^2 ,\qquad x^\star = G_\theta(z^\star).
$$

This approach eliminates the encoder entirely and transforms the inverse problem into a low-dimensional optimization over (z), yielding stable and efficient solvers.

Minimal python-like pseudocode summaries and  detailed inline comments:

```python
# VAE prior: optimize z under decoder x = G_theta(z).
# Although VAE provides ELBO, in practice we solve latent-MAP:
#   min_z  ||A(G(z)) - y||^2 + lambda_z * ||z||^2
# Inputs and hyperparams similar to GAN case.

def fidelity_grad_x(x):
    return (1.0 / sigma2) * A.T( A(x) - y )

z = z_init  # sample or encoder warm-start

for k in range(K):
    x = G(z)  # decoder forward

    # Data gradient w.r.t. x then to z
    g_x = fidelity_grad_x(x)
    g_z_data = vjp(G, z, g_x)

    # Latent prior gradient for Gaussian z
    g_z_prior = z

    g_z = g_z_data + lambda_z * g_z_prior
    z = z - eta * g_z

x_hat = G(z)
```

---

<h1 id="section3.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.3 Normalizing Flows: Exact Log-Density Priors</h1> 



Normalizing flows (NFs) define an invertible, differentiable map 

$$
f_\theta:\mathbb{R}^d\!\to\!\mathbb{R}^d
$$ 

with 

$$
x=f_\theta(z),\qquad z\sim p(z) = \mathcal N(0,I)
$$

The change-of-variables formula yields an **exact** log-density for any $x$:

$$
\log p_\theta(x) = \log p(z)\;-\;\log\!\left|\det J_{f_\theta}(z)\right|, \qquad z=f_\theta^{-1}(x),
$$

so the MAP objective is

$$
\hat{\mathbf{x}} = \arg\min_{\mathbf{x}} \left( \|Ax-y\|^2\;-\;\log p_\theta(x) \right)
$$

Crucially, because $f_\theta$ is **bijective**, we can optimize either **in pixel space** ($x$-space) or **in latent space** ($z$-space). The two are mathematically equivalent but have distinct computational trade-offs.

- **Pixel-Space MAP (optimize over $x$).** The Objective is .
  
  $$
  \begin{align}
  \hat{\mathbf{x}} & = \arg\min_{\mathbf{x}} \left( \|Ax-y\|^2\;-\;\log p_\theta(x) \right) \\[10pt]
  & = \arg\min_{\mathbf{x}} \left( \|Ax-y\|^2 - \log p\!\big(f_\theta^{-1}(x)\big)\;+\;\log\!\left|\det J_{f_\theta}\!\big(f_\theta^{-1}(x)\big)\right| \right)
  \end{align}
  $$
  
  The gradient with respect to $x$ can be computed with chain of rule. 

  $$
  \nabla_x\!\left[-\log p_\theta(x)\right] = -\Big(\nabla_z \log p(z) - \nabla_z \log |\det J_{f_\theta}(z)|\Big)\;J_{f^{-1}}(x).
  $$
  
  where 
  
  $$z=f_\theta^{-1}(x) \quad \text{and} \quad J_{f^{-1}}(x)=\partial z/\partial x$$
  
  Thus each step requires evaluating the inverse flow and backpropagating through the log-det term. This is direct but can be heavier in practice (especially when (f_\theta^{-1}) is costly).
  
  ```python
  # Flow prior in x-space
  # x ‚Üî z via invertible f_theta, with log p(x) exact
  x = x_init  # e.g., A^T y
  for k in range(K):
    # data gradient in x
    g_data = (1.0 / sigma2) * A.T( A(x) - y )

    # invert to z
    z = f_inv(x)

    # compute prior gradient in z space
    g_z_prior = z
    g_z_jac = grad_logdet_J_f(z)
    g_z = g_z_prior - g_z_jac

    # push g_z back to x with J_f^{-T} (use JVP/VJP)
    Jinv = jvp(f_inv, x, g_z)  # or equivalent autodiff op
    g_prior_x = Jinv

    # total gradient
    g_total = g_data + lambda_z * g_prior_x
    x = x - eta * g_total

  x_hat = x
  ```
  
- **Latent-Space MAP (optimize over $z$)**. Reparameterize $x=f_\theta(z)$ and minimize over $z$:

  $$
  \hat{\mathbf{z}} = \arg\min_{\mathbf{z}} \left( \|A(f_\theta(z))-y\|^2\;-\;\log p(z)\;+\;\log\!\left|\det J_{f_\theta}(z)\right| \right)
  $$
  
  This avoids the inverse pass and often yields smoother optimization. For 
  
  $$p(z)=\mathcal N(0,I) \quad \Longrightarrow \quad -\log p(z)=\frac{1}{2}|z|^2$$ 

  Compared to optimization in the $x$-space, optimization in the latent space $z$ is a more commonly used approach because it is **simpler**, more **stable**, and does not require backpropagating to compute the Jacobian at each step.


  ```python
  # Flow prior in z-space
  # Optimize in latent space z and then map to x = f(z)
  # Objective: min_z ||A f(z) - y||^2 - log p(z) + log|det J_f(z)|

  z = z_init  # e.g., zero or sample
  for k in range(K):
    x = f(z)  # forward flow

    # data fidelity gradient w.r.t x
    g_x = (1.0 / sigma2) * A.T( A(x) - y )

    # push fidelity gradient to z via VJP
    g_z_data = vjp(f, z, g_x)

    # prior term: -‚àá_z log p(z) ; for N(0,I) this is z
    g_z_prior = z

    # log-det Jacobian gradient: ‚àá_z log|det J_f(z)|
    g_z_jac = grad_logdet_J_f(z)  # domain-specific: sum of layer log-dets

    g_z = g_z_data + lambda_z *( g_z_prior - g_z_jac )

    z = z - eta * g_z

  x_hat = f(z)
  ```


---

<h1 id="section3.4" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.4 Energy-Based Models: Explicit Energy Priors</h1> 



Energy-Based Models (EBMs) provide a particularly direct way to define generative priors for inverse problems. Instead of representing $p_\theta(x)$ through a normalized likelihood or a decoder, EBMs define a scalar **energy function** $E_\theta(x)$ that assigns lower energy to more likely samples:

$$
p_\theta(x) \;=\; \frac{1}{Z_\theta} \, \exp\!\big(-E_\theta(x)\big)
$$

where 

$$Z_\theta=\int \exp(-E_\theta(x)) dx$$

is the (typically intractable) partition function. The logarithm of the prior is then:

$$
\log p_\theta(x) = -E_\theta(x) - \log Z_\theta\quad \Longrightarrow \quad  \nabla_x \log p_\theta(x) = -\nabla_x E_\theta(x).
$$


Because the gradient of the log-density depends only on the local energy derivative, **EBMs provide explicit access to the prior gradient** without requiring a normalized likelihood or latent variable, making them ideal for gradient-based inverse solvers.

Minimal python-like pseudocode summaries and  detailed inline comments:

```python
# EBM prior: p(x) ‚àù exp(-E(x)), so MAP solves:
#   min_x ||A x - y||^2 + Œª E(x)
# Direct gradient-based optimization is natural.

x = x_init  # e.g., A^T y
for k in range(K):
    # fidelity gradient (Gaussian case)
    g_data = (1.0 / sigma2) * A.T( A(x) - y )

    # prior gradient is gradient of energy: ‚àá_x E(x)
    g_prior = grad_E(x)  # requires E model to be differentiable

    # combined update (can use Adam)
    x = x - eta * ( g_data + lambda_e * g_prior )

# Optionally, run Langevin refinement to sample posterior:
# x <- x - eta*(g_data + lambda_e * g_prior) + sqrt(2*eta)*Normal(0,I)
x_hat = x
```  

---

<h1 id="section4" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">4. Diffusion-Based Models as Generative Priors</h1>  

The preceding section showed that in classical generative models‚ÄîGANs, VAEs, EBMs, and Normalizing Flows‚Äîthe prior enters the MAP objective as an **explicit regularizer** or as an **implicit manifold constraint** expressed through a low-dimensional parameterization $x = G(z)$.

Despite architectural differences, these models share a common geometric essence: **the inverse problem is solved by searching for a point on the generator‚Äôs manifold that best agrees with the observations**.

When moving to diffusion-family models, this geometric principle remains intact, but the **representation of the manifold and the means of enforcing the constraint** change fundamentally.
  

---

<h1 id="section4.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">4.1 From Explicit Manifolds to Implicit Stochastic Trajectories</h1> 
 


Diffusion models, score-based models, and their deterministic counterparts (probability-flow ODEs, rectified/mean/consistency flows) do not represent the data manifold via an explicit mapping $$G:\!z\mapsto x$$.

Instead, they define it **implicitly** through a **stochastic differential equation (SDE)** or its deterministic limit:

$$
d\mathbf{x}_t = f(\mathbf{x}_t,t),dt + g(t),d\mathbf{w}_t ,\qquad
\mathbf{x}*0 !\sim! p*{\text{data}}.
$$

Training learns the **score field**

$$s_\theta(\mathbf{x}_t,t) \approx \nabla_{\mathbf{x}_t}\!\log p_t(\mathbf{x}_t)$$

that characterizes the instantaneous geometry of the evolving density $$p_t(\mathbf{x}_t)$$. The reverse process,

$$
d\mathbf{x}_t = [\,f(\mathbf{x}_t,t) - g(t)^2 s_\theta(\mathbf{x}_t,t)\,]\,dt + g(t)\,d\bar{\mathbf{w}}_t
$$

generates samples by integrating backward from pure noise (\mathbf{x}_T!\sim!\mathcal{N}(0,I)) to a clean sample (\mathbf{x}_0). Every valid trajectory of this reverse dynamics terminates on the learned **data manifold**‚Äîthus the manifold is not an explicit surface but the **reachable set** of the learned flow field $s_\theta$.



Given a measurement model $$\mathbf{y}=A\mathbf{x}+\mathbf{n}$$, Bayes‚Äô rule yields the posterior

$$
\begin{align}
p(\mathbf{x}\mid \mathbf{y})\, & \propto\, p(\mathbf{y}\mid\mathbf{x})\,p(\mathbf{x}) \\[10pt]
\Longrightarrow \quad
\nabla_{\mathbf{x}}\log p(\mathbf{x}\mid\mathbf{y})
\,& =\, \underbrace{\nabla_{\mathbf{x}}\log p(\mathbf{y}\mid\mathbf{x})}_{\text{data consistency}}
+ \underbrace{\nabla_{\mathbf{x}}\log p(\mathbf{x})}_{\text{generative prior}}
\end{align}
$$

The second term is directly available from the diffusion model‚Äôs score estimator. Hence diffusion-family priors offer a **local gradient oracle** for the log-density of the data manifold, enabling two complementary solution paradigms.



---

<h1 id="section4.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">4.2 Sampling view: Posterior Sampling via Reverse Dynamics</h1> 


Most state-of-the-art inverse-problem solvers  adopt a **sampling view**. The idea is to modify the reverse-time SDE (or DDIM/ODE variant) so that each integration step follows the **posterior score**

$$
\nabla_{\mathbf{x}_t}\!\log p(\mathbf{x}_t|\mathbf{y})
= s_\theta(\mathbf{x}_t,t)
+ \nabla_{\mathbf{x}_t}\!\log p(\mathbf{y}\mid \mathbf{x}_t)
$$

For Gaussian measurement noise,

$$\nabla_{\mathbf{x}}\log p(\mathbf{y}\mid\mathbf{x}) = \sigma^{-2}A^{!\top}(\mathbf{y}-A\mathbf{x}))$$

In practice the likelihood gradient is evaluated on the denoised prediction

$$
\hat {\mathbf{x}}_{0}  = [\mathbf{x}_t - \sqrt{1-\bar\alpha_t},\epsilon_\theta(\mathbf{x}_t,t)]/\sqrt{\bar\alpha_t}
$$

This is because the observation relates to the clean variable (\mathbf{x}_0). The modified drift

$$
\tilde{f}(\mathbf{x}_t,t)
= f(\mathbf{x}_t,t)
- g(t)^2\!\big[\,s_\theta(\mathbf{x}_t,t)
+ \nabla_{\mathbf{x}_t}\!\log p(\mathbf{y}\mid{\hat {\mathbf{x}}_{0} })\big]
$$

drives samples toward regions consistent with both the learned prior and the measurements.
This stochastic integration **implicitly constrains trajectories to the intersection of the diffusion manifold $$\mathcal M_{\text{DM}}$$ and the observation manifold $$\mathcal M_{\text{obs}}$$**, producing diverse yet measurement-faithful reconstructions.



![manifold intersection](/images/posts/2025-11-10-blog-post/dm_manifold.jpg)



---

<h1 id="section4.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">4.3 Optimization view: Deterministic Optimization under Score Priors</h1> 




We can also views diffusion priors as **deterministic energy functions** rather than stochastic samplers. Using the score as an approximation of $\nabla_x \log p(x)$, one can define the MAP energy

$$
E(x)
= \frac{1}{2\sigma^2}\|A x - y\|^2 - \log p_\theta(x)
$$

take the derivative 

$$
\nabla_x E(x)
= \frac{A^{\top}(A x - y)}{\sigma^2} - s_\theta(x,t)
$$

Gradient-based solvers  perform iterative updates

$$
x_{k+1} = x_k - \eta \big[\,\frac{A^{\top}(A {\hat x_0} - y)}{\sigma^2} - s_\theta(x_k,t_k)\, \big],
$$

Unlike sampling, these methods yield a single deterministic MAP or gradient-refined reconstruction, trading diversity for speed and stability. They remain less prevalent because they require differentiable access to the score model and risk converging to local modes, but they clearly demonstrate that diffusion priors can also be interpreted as **optimization-based regularizers**.





---

<h1 id="section5" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">5. Posterior-Guided Sampling for Inverse Problems</h1>  

Up to this point, we have seen how deep generative models can serve as explicit or implicit priors in inverse problems, and how diffusion-family models provide local access to the score field $\nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t)$ along a stochastic trajectory. Sections 4.2 and 4.3 already hinted at a key conceptual shift: instead of optimizing a static MAP objective in the clean space $\mathbf{x}_0$, we can **embed the inverse problem directly into the reverse-time dynamics**, replacing the generative prior score with a **posterior score**.  

This section develops a unified view of this idea, which we refer to as **posterior-guided sampling**. In this paradigm, the inverse problem is solved by simulating a modified reverse SDE / ODE whose drift is steered by the posterior score

$$
\nabla_{\mathbf{x}_t}\log p(\mathbf{x}_t \mid \mathbf{y})
=
\underbrace{\nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t)}_{\text{prior score}}
+
\underbrace{\nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t)}_{\text{likelihood / data-consistency score}}.
$$

Different algorithms‚ÄîDPS, MCG, DDS, DDRM, and many flow-based variants‚Äîcan all be understood as particular discretizations and approximations of this posterior-guided dynamics, differing mainly in **where** and **how** the likelihood score is evaluated (in the noisy space $\mathbf{x}_t$ vs. the clean space $\mathbf{x}_0$), and in how strongly it is weighted over time.  

---

<h1 id="section5.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">5.1 From Reverse Dynamics to Posterior Guidance</h1>  

Recall the reverse-time SDE (or its deterministic PF-ODE limit) used by score-based diffusion models for unconditional generation:

$$
d\mathbf{x}_t = \Big(f(\mathbf{x}_t,t) - g(t)^2 \,\underbrace{\nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t)}_{\approx s_\theta(\mathbf{x}_t,t)}\Big)\,dt
\quad + \quad g(t)\,d\bar{\mathbf{w}}_t,
$$

or, in the deterministic ODE case,

$$
\frac{d\mathbf{x}_t}{dt}
=
f(\mathbf{x}_t,t) - \frac{1}{2}g(t)^2 s_\theta(\mathbf{x}_t,t),
$$

where $$s_\theta(\mathbf{x}_t,t)$$ is the learned score that approximates $$\nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t)$$.

In an inverse problem with measurements

$$
\mathbf{y} = A\mathbf{x}_0 + \mathbf{n}, \qquad \mathbf{n}\sim\mathcal{N}(0,\sigma^2 I),
$$

Bayes‚Äô rule gives the posterior at the clean level

$$
\log p(\mathbf{x}_0\mid \mathbf{y})
= \log p(\mathbf{y}\mid \mathbf{x}_0) + \log p(\mathbf{x}_0) + \text{const.}
$$

However, in diffusion models we do not sample $\mathbf{x}_0$ directly; instead we traverse a **noisy trajectory** $$\{\mathbf{x}_t\}_{ t \in [0,T] }$$. The natural object is therefore the **time-dependent posterior** $p(\mathbf{x}_t\mid \mathbf{y})$, whose score decomposes as

$$
\nabla_{\mathbf{x}_t}\log p(\mathbf{x}_t\mid \mathbf{y})
=
\nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t)
+
\nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t).
$$

Replacing the prior score in the reverse dynamics by this posterior score yields a **posterior-guided reverse SDE / ODE**:

$$
\frac{d\mathbf{x}_t}{dt}
=
f(\mathbf{x}_t,t)
-\frac{1}{2}g(t)^2\Big(
\nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t)
+
\nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t)
\Big).
$$

Intuitively:

- The **prior term** $\nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t)$ keeps the trajectory on the learned data manifold (in the sense of Section 4.1).
- The **likelihood term** $\nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t)$ pulls the trajectory toward the measurement-consistent manifold $\{\mathbf{x} : A\mathbf{x}\approx\mathbf{y}\}$.

Posterior-guided sampling therefore produces a trajectory that, in the limit of small step sizes and accurate scores, follows the gradient flow of the posterior $p(\mathbf{x}_t\mid \mathbf{y})$ in noisy space and concentrates near the clean posterior $p(\mathbf{x}_0\mid \mathbf{y})$ at $t\to 0$.

---

<h1 id="section5.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">5.2 Likelihood Scores via the Clean Space: Chain Rule Approximation</h1>  

The remaining challenge is the **likelihood term** $\nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t)$, which is generally intractable. The measurement model is defined on the clean signal $\mathbf{x}_0$:

$$
p(\mathbf{y}\mid \mathbf{x}_0) = \mathcal{N}(\mathbf{y}; A\mathbf{x}_0, \sigma^2 I),
\qquad 
\nabla_{\mathbf{x}_0}\log p(\mathbf{y}\mid \mathbf{x}_0)
= \frac{1}{\sigma^2}A^\top(\mathbf{y}-A\mathbf{x}_0).
$$

In contrast, $\mathbf{x}_t$ is obtained from $\mathbf{x}_0$ through the **forward noising process**, e.g.

$$
\mathbf{x}_t = s(t)\mathbf{x}_0 + \sigma(t)\boldsymbol{\epsilon},\qquad \boldsymbol{\epsilon}\sim\mathcal{N}(0,I),
$$

and the distribution $p(\mathbf{y}\mid \mathbf{x}_t)$ is a complicated mixture:

$$
p(\mathbf{y}\mid \mathbf{x}_t)
= \int p(\mathbf{y}\mid \mathbf{x}_0)\,p(\mathbf{x}_0\mid \mathbf{x}_t)\,d\mathbf{x}_0.
$$

Computing $\nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t)$ exactly would require differentiating through this integral, which is intractable. Posterior-guided methods therefore rely on two key approximations:

1. **Plug-in approximation (concentrated posterior in the clean space).**  
   For sufficiently small noise levels, the conditional $p(\mathbf{x}_0\mid \mathbf{x}_t)$ is sharply concentrated around a **denoised estimate** $\hat{\mathbf{x}}_0(\mathbf{x}_t,t)$, often given by a Tweedie-type estimator:

   $$
   \hat{\mathbf{x}}_0(\mathbf{x}_t,t)
   \approx \mathbb{E}[\mathbf{x}_0\mid \mathbf{x}_t].
   $$

   Under this assumption,

   $$
   p(\mathbf{y}\mid \mathbf{x}_t)
   \approx
   p(\mathbf{y}\mid \hat{\mathbf{x}}_0(\mathbf{x}_t,t)),
   \quad
   \log p(\mathbf{y}\mid \mathbf{x}_t)
   \approx
   \log p(\mathbf{y}\mid \hat{\mathbf{x}}_0(\mathbf{x}_t,t)).
   $$

2. **Chain rule through the denoiser.**  
   Treat $\hat{\mathbf{x}}_0$ as a deterministic function of $\mathbf{x}_t$. Then

   $$
   \nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t)
   \;\approx\;
   \bigg(\frac{\partial \hat{\mathbf{x}}_0}{\partial \mathbf{x}_t}\bigg)^\top
   \nabla_{\mathbf{x}_0}\log p(\mathbf{y}\mid \mathbf{x}_0)\Big|_{\mathbf{x}_0=\hat{\mathbf{x}}_0(\mathbf{x}_t,t)}.
   $$

   The Jacobian $\partial \hat{\mathbf{x}}_0 / \partial \mathbf{x}_t$ can be evaluated (or implicitly applied) via standard vector‚ÄìJacobian products through the score network or $x_0$-prediction network.

The **Manifold-Constrained Gradient (MCG)** perspective further interprets $$\partial \hat{\mathbf{x}}_0 / \partial \mathbf{x}_t$$ in geometric terms: in the small-noise limit, the denoiser behaves like a **projection onto the data manifold**, and its Jacobian approximates the **orthogonal projector onto the tangent space** $$T_{\hat{\mathbf{x}}_0}\mathcal{M}_0$$. Consequently, the likelihood gradient in noisy space

$$
\nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t)
\approx
P_{T_{\hat{\mathbf{x}}_0}\mathcal{M}_0}
\,\nabla_{\mathbf{x}_0}\log p(\mathbf{y}\mid \mathbf{x}_0)
$$

is constrained to move **along the data manifold**, rather than drifting into off-manifold directions that correspond to implausible images.

---

<h1 id="section5.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">5.3 Noisy-Space Posterior Guidance: DPS and MCG</h1>  

Under the approximations above, we can construct a **noisy-space likelihood score** and add it to the prior score during sampling. In a discretized PF-ODE solver with time steps $t_K > \dots > t_1 > t_0=0$, a typical DPS/MCG-style update at time $t_k$ has the following structure:

1. **Prior step.**  Given $\mathbf{x}_{t_k}$, perform one unconditional diffusion step (e.g., PF-ODE / DDIM / DPM-Solver):

   $$
   \tilde{\mathbf{x}}_{t_k}
   = \texttt{PriorStep}(\mathbf{x}_{t_k}, t_k),
   $$

   using only the trained score $s_\theta(\cdot,t)$.

2. **Denoise to the clean estimate.**  

   $$
   \hat{\mathbf{x}}_0
   = \hat{\mathbf{x}}_0(\tilde{\mathbf{x}}_{t_k}, t_k),
   $$

   via $x_0$-prediction or Tweedie‚Äôs formula.

3. **Compute a clean-space likelihood gradient.**

   $$
   \mathbf{g}_{x_0}
   = \nabla_{\mathbf{x}_0}\log p(\mathbf{y}\mid \mathbf{x}_0)\Big|_{\mathbf{x}_0=\hat{\mathbf{x}}_0}
   =
   \frac{1}{\sigma^2}A^\top(\mathbf{y}-A\hat{\mathbf{x}}_0),
   $$

   for Gaussian noise.

4. **Map the gradient back to noisy space.**  

   $$ 
   \mathbf{g}_{x_t}
   \approx
   \bigg(\frac{\partial \hat{\mathbf{x}}_0}{\partial \mathbf{x}_t}\Big|_{\mathbf{x}_t=\tilde{\mathbf{x}}_{t_k}}\bigg)^\top \mathbf{g}_{x_0}.
   $$

   In practice this is implemented as a vector‚ÄìJacobian product through the $x_0$-prediction network.

5. **Likelihood-guided correction.**  

   $$
   \mathbf{x}_{t_k}^{\text{guided}}
   =
   \tilde{\mathbf{x}}_{t_k}
   + \eta_k \mathbf{g}_{x_t},
   $$

   where $\eta_k$ is a step size or guidance weight.

6. **Time step to $t_{k-1}$.**  
   Use $$\mathbf{x}_{t_k}^{\text{guided}}$$ as the initial state for the next PF-ODE/DDIM integration step to time $t_{k-1}$.

Conceptually, this is a **discrete-time approximation** of the posterior-guided dynamics in which the total vector field is split into a **prior part** and a **data-consistency part**. This operator-splitting view offers several advantages:

- It makes it easy to **tune the relative strength** of prior and likelihood contributions by adjusting $\eta_k$.
- It allows reusing any existing sampler (DDPM, DDIM, DPM-Solver, Heun, etc.) as a black box for the prior step.
- It stabilizes training and sampling, since the potentially sharp likelihood gradient is applied as a separate, controlled correction.

DPS and MCG differ mainly in how carefully they approximate the Jacobian projection and in how they schedule the guidance strength across time, but both can be understood as noisy-space posterior-guided samplers in this unified framework.

---


<h1 id="section5.4" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">5.4 Clean-Space Posterior Guidance: DDS and Decomposed Samplers</h1>  

An alternative viewpoint is to **move the DC step entirely into the clean space $\mathbf{x}_0$**. Instead of directly constructing $\nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t)$, methods like DDS perform the following decomposition at each time step:

1. **Prior projection to clean space.**  
   From $\mathbf{x}_t$, compute a clean estimate

   $$
   \hat{\mathbf{x}}_0 = \hat{\mathbf{x}}_0(\mathbf{x}_t,t),
   $$

   which can be interpreted as the **prior-implied local mode** of $p(\mathbf{x}_0\mid \mathbf{x}_t)$.

2. **Local clean-space MAP / proximal update.**  
   Instead of just taking one gradient step on the likelihood, DDS solves a **small optimization problem in $\mathbf{x}_0$** that trades off data consistency and deviation from $\hat{\mathbf{x}}_0$:

   $$
   \mathbf{x}_0^\star
   \approx
   \arg\min_{\mathbf{x}_0}
   \underbrace{
   \frac{1}{2\sigma^2}\|\mathbf{y}-A\mathbf{x}_0\|_2^2
   }_{\text{data term}}
   +
   \underbrace{
   \frac{\lambda_t}{2}\|\mathbf{x}_0 - \hat{\mathbf{x}}_0\|_2^2
   }_{\text{local prior term}},
   $$

   where $\lambda_t$ controls how strongly we trust the prior prediction at time $t$. This is a convex quadratic problem and can be solved efficiently with conjugate gradients (CG), even for large-scale $A$.

3. **Re-noising to the next noisy state.**  
   Finally, the updated clean estimate $\mathbf{x}_0^\star$ is mapped back to the next noisy state $\mathbf{x}_{t-\Delta t}$ via the forward noising rule (or its deterministic DDIM/ODE equivalent):

   $$
   \mathbf{x}_{t-\Delta t}
   = \texttt{ReNoise}(\mathbf{x}_0^\star, t-\Delta t).
   $$

Geometrically, this procedure can be interpreted as:

- The **prior** first projects $\mathbf{x}_t$ to the data manifold via $\hat{\mathbf{x}}_0$;
- The **local MAP problem** moves $\hat{\mathbf{x}}_0$ along the manifold to better satisfy the measurements $\mathbf{y}$;
- The **re-noising step** sends the updated clean point back to the appropriate noisy level to continue the trajectory.

Compared to noisy-space guidance (DPS/MCG):

- DDS avoids computing Jacobians of $\hat{\mathbf{x}}_0$ with respect to $\mathbf{x}_t$, and instead relies on repeated applications of $A$ and $A^\top$ inside CG. This is attractive when $A$ has a known fast structure (convolution, FFT, Radon transform, etc.).
- The DC step is often **more aggressive**, since solving the quadratic subproblem can be interpreted as performing multiple implicit gradient steps in one shot.
- Prior and likelihood are handled in a more symmetric way inside the clean-space subproblem, where the prior appears as a quadratic term anchored at $\hat{\mathbf{x}}_0$.

Nevertheless, in the limit of small time steps and appropriate choices of $\lambda_t$, DDS can be seen as another discretization of the same posterior-guided dynamics, now operating in a **‚Äú$x_0$-first, then re-noise‚Äù** fashion.

---

<h1 id="section5.5" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">5.5 Unified Perspective and Design Trade-offs</h1>  

Posterior-guided sampling provides a unifying lens on a broad family of generative inverse problem solvers built on diffusion and flow-based models:

- **Target object.**  
  All methods aim to approximate sampling (or gradient flow) under the posterior $p(\mathbf{x}_0\mid \mathbf{y})$, but they do so by operating along a noisy trajectory $\{\mathbf{x}_t\}$ rather than directly in the clean space.

- **Prior access.**  
  The generative prior is always accessed via a **local score oracle** $s_\theta(\mathbf{x}_t,t)\approx \nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t)$ (or its flow-based counterpart). This term keeps the trajectory near the data manifold.

- **Likelihood access.**  
  The likelihood term is defined at the clean level, $\log p(\mathbf{y}\mid \mathbf{x}_0)$, and must be transported to noisy space by either:
  - Chain rule through the denoiser (DPS/MCG), yielding a noisy-space gradient;
  - Or an explicit local optimization in $x_0$ followed by re-noising (DDS and decomposed samplers).

- **Operator splitting vs. monolithic integration.**  
  In continuous time, both contributions can be combined into a single posterior vector field. In practice, discretizations usually adopt **operator splitting (prior step + DC step)**, which increases flexibility in:
  - Scheduling guidance strength over time;
  - Reusing existing high-order samplers for the prior dynamics;
  - Balancing numerical stability against convergence speed.

- **Geometry.**  
  From a geometric viewpoint, posterior-guided sampling seeks the **intersection of two manifolds**:
  - The generative manifold (reachable set of the diffusion flow);
  - The measurement manifold (set of signals consistent with $\mathbf{y}$).
  Noisy-space methods achieve this by steering trajectories with projected DC gradients; clean-space methods do so by iteratively solving small MAP problems on the data manifold and lifting them back to the diffusion trajectory.

Finally, although we have framed this discussion in the language of diffusion models, the same principles extend almost verbatim to **flow matching, rectified flows, and consistency models**: as long as we have a time-dependent vector field that transports a simple base distribution to the data distribution, we can build **posterior-guided versions** of that flow by injecting likelihood gradients into the dynamics. This perspective will be particularly useful when we discuss continuous-time flow and consistency models in subsequent sections.



















---

<h1 id="section6" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">
6. Clean-Space Local-MAP Sampling with Diffusion Priors
</h1>

In Chapter 5, we characterized a broad family of generative inverse problem solvers as **posterior-guided samplers** operating in the noisy space $$\{\mathbf{x}_t\}_{t\in[0,T]}$$. In that view, methods such as DPS and MCG modify the reverse diffusion dynamics by injecting a likelihood score $$\nabla_{\mathbf{x}_t}\log p(\mathbf{y}\mid \mathbf{x}_t)$$ directly into the vector field, yielding a noisy-space approximation of the posterior score $$\nabla_{\mathbf{x}_t}\log p(\mathbf{x}_t\mid \mathbf{y})$$.

In this chapter we turn to a complementary paradigm, exemplified by **Decomposed Diffusion Sampling (DDS)**, in which the inverse problem is tackled directly in the **clean signal space** $\mathbf{x}_0$. Rather than constructing the posterior score in $\mathbf{x}_t$ and integrating a single posterior-guided SDE/ODE, DDS alternates between:

- a **generative prior step** that maps noisy states $\mathbf{x}_t$ to a denoised estimate $\hat{\mathbf{x}}_0$ and back; and  
- a **clean-space optimization step** that locally solves a (regularized) maximum likelihood or maximum a posteriori (MAP) problem in $\mathbf{x}_0$.

This yields what we will refer to as a **clean-space local-MAP sampling paradigm**. The outer loop still follows a diffusion-style trajectory in $t$, but each time step is mediated by an inner optimization in the clean space. This decouples the roles of the generative prior and the measurement model in a way that is conceptually close to classical inverse problem formulations, while retaining the expressive power of modern diffusion priors.

---

<h1 id="section6.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">6.1 From Global MAP to Local Clean-Space Refinement</h1>

We consider the standard linear inverse problem

$$
\mathbf{y} = A\mathbf{x}_0 + \mathbf{n}, \qquad 
\mathbf{n} \sim \mathcal{N}(\mathbf{0}, \sigma_y^2 I),
$$

and a diffusion prior over $\mathbf{x}_0$ implicitly defined by the forward noising process and its learned reverse dynamics. The global MAP problem in the clean space is

$$
\hat{\mathbf{x}}_0^{\text{MAP}} =
\arg\min_{\mathbf{x}_0} \Big[ -\log p(\mathbf{y}\mid \mathbf{x}_0) - \log p(\mathbf{x}_0) \Big]
$$

In principle, if we could evaluate (or differentiate) $$\log p(\mathbf{x}_0)$$ directly, we might attempt to solve this optimization problem by gradient-based methods in $$\mathbf{x}_0$$ (cf. Chapter 3). However, in diffusion models $p(\mathbf{x}_0)$ is given only implicitly through a time-dependent score oracle $$\nabla_{\mathbf{x}_t}\log p_t(\mathbf{x}_t)$$ or equivalently through the reverse-time vector field. There is no tractable closed form for $$-\log p(\mathbf{x}_0)$$ itself.

Posterior-guided noisy-space samplers (Chapter 5) respond to this difficulty by working entirely in $\mathbf{x}_t$ space and approximating $$\nabla_{\mathbf{x}_t}\log p(\mathbf{x}_t\mid\mathbf{y})$$. DDS takes a different route:

- It **never attempts to explicitly evaluate $-\log p(\mathbf{x}_0)$**.  

- Instead, it uses the diffusion prior to provide, at each time $t$, a **local anchor** $\hat{\mathbf{x}}_0(\mathbf{x}_t,t)$ around which $-\log p(\mathbf{x}_0)$ can be approximated.  

- It then formulates a **local optimization problem** in $\mathbf{x}_0$ that blends:
  - a data term derived from $-\log p(\mathbf{y}\mid \mathbf{x}_0)$, and  
  - a local regularizer anchored at $\hat{\mathbf{x}}_0$, which acts as a quadratic approximation to $-\log p(\mathbf{x}_0)$ in a small neighborhood.

Thus, the global MAP objective is replaced by a sequence of **time-indexed local subproblems** that can be solved efficiently with classical optimization tools (e.g., conjugate gradients). The diffusion prior appears in two places:

1. It determines the **anchor point** $\hat{\mathbf{x}}_0(\mathbf{x}_t,t)$ and the **tangent subspace** where updates are allowed.  
2. It supplies the **re-noising mapping** that embeds the refined clean estimate back into the diffusion trajectory at the next time step.

---

<h1 id="section6.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">6.2 The Noiseless Setting: Local Maximum Likelihood in the Clean Space</h1>

We first consider the **noiseless measurement** case

$$
\mathbf{y} = A\mathbf{x}_0
$$

and assume the forward operator $A$ is known. In this setting, the (negative) log-likelihood reduces to a pure data-consistency term

$$
-\log p(\mathbf{y}\mid \mathbf{x}_0) \propto \tfrac{1}{2}\|\mathbf{y}-A\mathbf{x}_0\|_2^2.
$$

The core DDS update at time $t$ consists of three conceptual steps:

1. **Denoising / prior projection.**  From the current noisy state $\mathbf{x}_t$, we compute a clean estimate
   
   $$
   \hat{\mathbf{x}}_0 = \hat{\mathbf{x}}_0(\mathbf{x}_t,t)
   $$
   
   via an $x_0$-prediction network or Tweedie‚Äôs formula. Geometrically, $\hat{\mathbf{x}}_0$ can be interpreted as a local projection of $\mathbf{x}_t$ onto the data manifold.

2. **Local MLE refinement in the clean space.**  We use $\hat{\mathbf{x}}_0$ as an initialization and search for a refined estimate $\mathbf{x}_0^\star$ that better satisfies the measurement equation:
   
   $$
   \mathbf{x}_0^\star 
   \approx
   \arg\min_{\mathbf{x}_0}
   \frac{1}{2}\|\mathbf{y}-A\mathbf{x}_0\|_2^2
   \quad
   \text{subject to } \mathbf{x}_0 \in \hat{\mathbf{x}}_0 + \mathcal{T}_t.
   $$
   
   Here $$\mathcal{T}_t$$ is a subspace that approximates the **tangent space** of the data manifold at $\hat{\mathbf{x}}_0$. In practice, $\mathcal{T}_t$ is instantiated as a Krylov subspace generated by $A^\*A$ and the initial residual, which effectively constrains updates to directions that both:
   - lie in the local manifold tangent space; and  
   - are relevant for improving data consistency.

   Concretely, DDS performs a fixed number $M$ of conjugate gradient (CG) iterations on the normal equations
   
   $$
   A^* A \mathbf{x} = A^* \mathbf{y}
   $$
   
   with initial point $\hat{\mathbf{x}}_0$, implicitly restricting $\mathbf{x}$ to a low-dimensional Krylov subspace. The resulting $\mathbf{x}_0^\star$ can be viewed as a **local maximum likelihood** estimate under the constraint that updates remain approximately tangential to the generative manifold.

3. **Re-noising / embedding back into the trajectory.**  The refined clean estimate $\mathbf{x}_0^\star$ is then mapped back to a noisy state at the next time level $t-\Delta t$ via the forward noising rule (or its deterministic PF-ODE/DDIM counterpart):
   
   $$
   \mathbf{x}_{t-\Delta t} = \texttt{ReNoise}(\mathbf{x}_0^\star, t-\Delta t).
   $$

In the noiseless setting, the local clean-space problem contains **only the data term**. The prior acts indirectly via:

- the initial estimate $\hat{\mathbf{x}}_0(\mathbf{x}_t,t)$, which is already close to the data manifold; and  
- the choice of search subspace $\mathcal{T}_t$, which restricts updates to directions compatible with the learned manifold geometry.

This realizes a clean-space variant of the ‚Äúprior + data-consistency‚Äù decomposition: the prior is used to set up a well-posed low-dimensional ML problem, which is then solved using standard linear algebra.

---

<h1 id="section6.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">6.3 The Noisy Setting: Proximal Local-MAP Updates</h1>

When the measurements are **noisy**,

$$
\mathbf{y} = A\mathbf{x}_0 + \mathbf{n},
\quad \mathbf{n}\sim\mathcal{N}(0,\sigma_y^2 I),
$$

the pure MLE objective

$$
\frac{1}{2}\|\mathbf{y}-A\mathbf{x}_0\|_2^2
$$


is no longer sufficient: directly minimizing this term tends to overfit the noise and may severely amplify ill-posedness in $A$. In this regime, DDS extends the clean-space subproblem into a **local MAP or proximal update** of the form

$$
\mathbf{x}_0^\star
\approx
\arg\min_{\mathbf{x}_0}
\underbrace{\frac{\gamma}{2}\|\mathbf{y}-A\mathbf{x}_0\|_2^2}_{\text{data term}}
+
\underbrace{\frac{1}{2}\|\mathbf{x}_0-\hat{\mathbf{x}}_0\|_2^2}_{\text{local prior / trust-region term}}.
\tag{6.1}
$$

Here $\gamma>0$ is a scalar balancing the influence of the noisy data and the prior anchor $\hat{\mathbf{x}}_0$. This formulation admits several complementary interpretations.

---

<h2 id="section6.3.1" style="color: #1E40AF; font-size: 22px; font-weight: bold;">6.3.1 Local Gaussian Prior Approximation</h2>

Suppose that, in a neighborhood of $\hat{\mathbf{x}}_0$, the global prior $p(\mathbf{x}_0)$ can be approximated by a Gaussian

$$
p(\mathbf{x}_0) \approx \mathcal{N}(\hat{\mathbf{x}}_0, I),
$$

i.e.,

$$
-\log p(\mathbf{x}_0) \approx \text{const} + \frac{1}{2}\|\mathbf{x}_0-\hat{\mathbf{x}}_0\|_2^2.
$$

Then the negative log-posterior in this neighborhood is

$$
-\log p(\mathbf{x}_0\mid \mathbf{y})
\approx
\frac{1}{2\sigma_y^2}\|\mathbf{y}-A\mathbf{x}_0\|_2^2
+
\frac{1}{2}\|\mathbf{x}_0-\hat{\mathbf{x}}_0\|_2^2
+ \text{const},
$$


which is exactly of the form (6.1) (up to absorbing $\sigma_y^2$ into $\gamma$). Thus, the local subproblem in DDS can be viewed as a **local MAP problem** that replaces the intractable global prior $-\log p(\mathbf{x}_0)$ by a quadratic approximation around the denoised estimate $\hat{\mathbf{x}}_0$.

Under this interpretation:

- The diffusion prior still governs the global geometry and evolution of $\hat{\mathbf{x}}_0(\mathbf{x}_t,t)$ along the trajectory;  
- Within each time slice, the local term $\|\mathbf{x}_0-\hat{\mathbf{x}}_0\|^2$ instantiates a **local probabilistic surrogate** for $-\log p(\mathbf{x}_0)$ that is tractable and convex.

---

<h2 id="section6.3.2" style="color: #1E40AF; font-size: 22px; font-weight: bold;">6.3.2 Proximal and Trust-Region Viewpoints</h2>

Equation (6.1) can also be recognized as a **proximal subproblem**:

$$
\mathbf{x}_0^\star
=
\operatorname{prox}_{\gamma f}(\hat{\mathbf{x}}_0),
\quad
f(\mathbf{x}_0) = \tfrac{1}{2}\|\mathbf{y}-A\mathbf{x}_0\|_2^2,
$$

where

$$
\operatorname{prox}_{\gamma f}(\hat{\mathbf{x}}_0)
=
\arg\min_{\mathbf{x}_0}
\Big(
\gamma f(\mathbf{x}_0)
+
\tfrac12\|\mathbf{x}_0 - \hat{\mathbf{x}}_0\|^2
\Big).
$$

From this proximal perspective, the DDS clean-space step at time $t$ computes a **noise-aware refinement** of $\hat{\mathbf{x}}_0$ that:

- moves in the direction of decreasing data misfit;  
- but keeps $\mathbf{x}_0^\star$ within a **trust region** around $\hat{\mathbf{x}}_0$ whose size is implicitly controlled by $\gamma$.

This trust-region behavior is crucial in noisy or ill-posed settings:

- It prevents the local update from chasing noise in $\mathbf{y}$ too aggressively;  
- It improves the conditioning of the normal equations
  
  $$
  (\gamma A^\*A + I)\,\mathbf{x}_0 = \hat{\mathbf{x}}_0 + \gamma A^\*\mathbf{y},
  $$
  
  leading to more stable and faster CG convergence;  

- It allows the algorithm designer to explicitly trade off **data fidelity** (via $\gamma$) against **adherence to the prior trajectory** (via the proximity to $\hat{\mathbf{x}}_0$).

Thus, the local prior term serves a dual role: it is both a **local approximation to the generative prior** and a **numerical regularizer** that stabilizes the inner optimization in the presence of measurement noise.

---

<h1 id="section6.4" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">6.4 Algorithmic Structure: Decoupling Prior Trajectory and Clean-Space Optimization</h1>

A key structural feature of the clean-space local-MAP paradigm is the **explicit decoupling** between:

- the **prior trajectory in noisy space** (governed solely by the diffusion model); and  
- the **inverse-problem solve in clean space** (governed solely by $A$ and the data term).

A generic DDS-style iteration at time $t_k$ can be summarized as:

1. **Prior denoising:**
   
   $$
   \hat{\mathbf{x}}_0 = \hat{\mathbf{x}}_0(\mathbf{x}_{t_k}, t_k).
   $$

2. **Local clean-space refinement:** Noiseless case:
   $$
     \mathbf{x}_0^\star 
     \approx 
     \arg\min_{\mathbf{x}_0}
     \frac{1}{2}\|\mathbf{y}-A\mathbf{x}_0\|_2^2
     \quad
     \text{(solved by $M$ CG steps in a Krylov subspace)}.
   $$
   
   Whild for Noisy case:
   
   $$
     \mathbf{x}_0^\star 
     \approx 
     \arg\min_{\mathbf{x}_0}
     \frac{\gamma}{2}\|\mathbf{y}-A\mathbf{x}_0\|_2^2
     +
     \frac{1}{2}\|\mathbf{x}_0-\hat{\mathbf{x}}_0\|_2^2
     \quad
     \text{(again solved by CG)}.
   $$

3. **Re-noising and time stepping:**
   
   $$
   \mathbf{x}_{t_{k-1}} 
   = 
   \texttt{ReNoise}(\mathbf{x}_0^\star, t_{k-1}).
   $$

This structure contrasts sharply with noisy-space posterior-guided samplers, where the prior score and likelihood score are combined into a **single vector field** inside the ODE/SDE solver. Here, the **clean-space optimization is a standalone module** that:

- can be tuned independently (e.g., number of CG steps, choice of regularization, stopping criterion);  
- can exploit problem-specific structure of $A$ (e.g., FFTs for deblurring, NUFFTs for MRI, Radon transforms for CT);  
- does not require backpropagating through the diffusion network (no Jacobians of $\hat{\mathbf{x}}_0$ with respect to $\mathbf{x}_t$).

This decoupling yields a modular design: the diffusion model provides a powerful **learned prior denoiser**, while the local clean-space solver acts as a **classical inverse problem solver** that is plugged into every time step of the diffusion trajectory.

---

<h1 id="section6.5" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">6.5 Geometric Interpretation: Manifold Projection and Measurement Manifold Intersection</h1>

The clean-space local-MAP paradigm admits an intuitive geometric interpretation that parallels the discussion in Chapter 4.

Let $\mathcal{M}_0$ denote the data manifold of clean images. At a given time $t$, the noising process generates a ‚Äúblurred‚Äù manifold $\mathcal{M}_t$ consisting of points $\mathbf{x}_t$ lying in a tubular neighborhood of $\mathcal{M}_0$. A single DDS step at time $t$ can be viewed as:

1. **Manifold projection (prior step).**  
   The mapping $$\mathbf{x}_t \mapsto \hat{\mathbf{x}}_0(\mathbf{x}_t,t)$$ approximately projects $$\mathbf{x}_t$$ onto $$\mathcal{M}_0$$ (or a high-density region of $$p(\mathbf{x}_0)$$). In the small-noise regime, the Jacobian of this mapping approximates the orthogonal projector onto the tangent space $$T_{\hat{\mathbf{x}}_0}\mathcal{M}_0$$.

2. **Clean-space movement along the manifold.**  
   The local optimization in $\mathbf{x}_0$ moves $\hat{\mathbf{x}}_0$ along directions that simultaneously:

   - reduce the data misfit $$\|\mathbf{y}-A\mathbf{x}_0\|$$;  
   - remain within (or close to) the tangent space $$T_{\hat{\mathbf{x}}_0}\mathcal{M}_0$$;  
   - in the noisy case, stay within a trust region centered at $$\hat{\mathbf{x}}_0$$.

   This step therefore pushes the estimate toward the **intersection** of:
   - the generative manifold $$\mathcal{M}_0$$; and  
   - the measurement manifold $$\{\mathbf{x}_0 : A\mathbf{x}_0 \approx \mathbf{y}\}$$.

3. **Re-embedding into the noisy manifold.**  
   The re-noising map $$\mathbf{x}_0^\star \mapsto \mathbf{x}_{t-\Delta t}$$ transports the refined clean point back to the appropriate noisy level, from which the process repeats with a slightly smaller noise scale.

In this picture, the diffusion prior is responsible for **keeping the trajectory close to $\mathcal{M}_0$**, while the clean-space optimization is responsible for **enforcing measurement consistency**. DDS achieves this by alternating between:

- projection-like moves (prior denoising and re-noising), and  
- tangential moves (local ML/MAP in $\mathbf{x}_0$).

---

<h1 id="section6.6" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">6.6 Relation to Noisy-Space Guidance and Plug-and-Play Methods</h1>

From the unified perspective developed in Chapters 3‚Äì5, clean-space local-MAP sampling sits at the intersection of:

- **posterior-guided diffusion** (DPS/MCG-style methods), and  
- **plug-and-play / RED** approaches in classical inverse problems.

Compared to noisy-space posterior-guided sampling (DPS/MCG):

- Both paradigms seek to approximate the posterior $p(\mathbf{x}_0\mid \mathbf{y})$ by modifying a diffusion trajectory;  
- Noisy-space methods operate directly on $\mathbf{x}_t$ and require Jacobian‚Äìvector products through the denoiser network to transport the likelihood gradient into noisy space;  
- Clean-space methods operate on $\mathbf{x}_0$, solve a local ML/MAP problem using only $A$ and $A^\*$, and then re-noise to maintain compatibility with the diffusion trajectory.

Compared to plug-and-play and RED:

- PnP/RED treat a generic denoiser $D_\sigma$ as an implicit prior and insert it into iterative optimization schemes for inverse problems;  
- DDS can be viewed as a **time-continuous PnP framework**, where the denoiser is the diffusion model‚Äôs $x_0$-prediction network and the outer loop follows a decreasing noise schedule;  
- The local clean-space subproblem (with or without proximal regularization) plays the role of a **data-consistency module**, akin to the gradient step in PnP-ADMM or PnP-FISTA.

Thus, the clean-space local-MAP paradigm can be interpreted as an instance of **diffusion-based PnP** where:

- the prior is expressed as a time-dependent denoiser along the diffusion path;  
- the inverse problem is solved by alternating between **denoising** and **data-consistency** in the clean space, with diffusion re-noising ensuring global consistency across time.

---

<h1 id="section6.7" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">
6.7 Design Choices and Open Questions
</h1>

The clean-space local-MAP paradigm exposes several important design degrees of freedom:

- **Number of inner iterations.**  
  The number $M$ of CG steps per time level controls the strength of the data-consistency enforcement. Small $M$ yields light-touch corrections akin to gradient steps, while large $M$ approaches solving the local subproblem to high accuracy. The optimal trade-off between computational cost and reconstruction quality remains problem-dependent.

- **Scheduling of $\gamma_t$ and noise levels.**  
  In noisy settings, the parameter $\gamma_t$ modulates the relative weight of data and local prior in (6.1). It is natural to consider time-varying schedules where:
  - early (high-noise) stages rely more strongly on the prior (smaller $\gamma_t$), and  
  - later (low-noise) stages increasingly trust the data (larger $\gamma_t$).

- **Choice of search subspace.**  
  Beyond classical Krylov subspaces, one may consider learned or structure-aware subspaces that better capture the local geometry of $\mathcal{M}_0$ and the action of $A$.

- **Extensions beyond linear measurements.**  
  The current formulation assumes a linear $A$. Extending the clean-space local-MAP paradigm to nonlinear forward models raises new challenges: the inner problem may become non-convex, and efficient solvers must exploit both the structure of $A$ and the properties of the diffusion prior.

These open questions highlight that DDS-style clean-space local-MAP sampling is not merely an algorithmic variant of posterior-guided diffusion, but rather a distinct and flexible paradigm. It bridges global generative priors and classical inverse problem optimization by embedding a **sequence of local ML/MAP solves** into a diffusion trajectory, and thereby offers a principled route to leveraging powerful learned priors within well-understood optimization frameworks.




---

<h1 id="section3" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">3. References</h1>


[^Reflow]: Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022.

[^nvp]: Dinh L, Sohl-Dickstein J, Bengio S. Density estimation using real nvp[J]. arXiv preprint arXiv:1605.08803, 2016.

[^Glow]: Kingma D P, Dhariwal P. Glow: Generative flow with invertible 1x1 convolutions[J]. Advances in neural information processing systems, 2018, 31.

[^Neural_ODE]: Chen R T Q, Rubanova Y, Bettencourt J, et al. Neural ordinary differential equations[J]. Advances in neural information processing systems, 2018, 31.

[^Ffjord]: Grathwohl W, Chen R T Q, Bettencourt J, et al. Ffjord: Free-form continuous dynamics for scalable reversible generative models[J]. arXiv preprint arXiv:1810.01367, 2018.

[^rectified_flow]: Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022.

[^FM]: Lipman Y, Chen R T Q, Ben-Hamu H, et al. Flow matching for generative modeling[J]. arXiv preprint arXiv:2210.02747, 2022.

[^SI]: Albergo M S, Boffi N M, Vanden-Eijnden E. Stochastic interpolants: A unifying framework for flows and diffusions[J]. arXiv preprint arXiv:2303.08797, 2023.

[^SI_1]: Albergo M S, Vanden-Eijnden E. Building normalizing flows with stochastic interpolants[J]. arXiv preprint arXiv:2209.15571, 2022.

[^SI_2]: Albergo M S, Goldstein M, Boffi N M, et al. Stochastic interpolants with data-dependent couplings[J]. arXiv preprint arXiv:2310.03725, 2023.

[^improve_rf]: Lee S, Lin Z, Fanti G. Improving the training of rectified flows[J]. Advances in neural information processing systems, 2024, 37: 63082-63109.

[^instaflow]: Liu X, Zhang X, Ma J, et al. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation[C]//The Twelfth International Conference on Learning Representations. 2023.

[^meanflow]: Geng Z, Deng M, Bai X, et al. Mean flows for one-step generative modeling[J]. arXiv preprint arXiv:2505.13447, 2025.

[^flow_map]: Boffi N M, Albergo M S, Vanden-Eijnden E. Flow map matching with stochastic interpolants: A mathematical framework for consistency models[J]. Transactions on Machine Learning Research, 2025.

[^ayf]: Sabour A, Fidler S, Kreis K. Align Your Flow: Scaling Continuous-Time Flow Map Distillation[J]. arXiv preprint arXiv:2506.14603, 2025.

[^cmt]: Hu Z, Lai C H, Mitsufuji Y, et al. CMT: Mid-Training for Efficient Learning of Consistency, Mean Flow, and Flow Map Models[J]. arXiv preprint arXiv:2509.24526, 2025.

[^cm]: Song Y, Dhariwal P, Chen M, et al. Consistency models[J]. 2023.

[^ctm]: Kim D, Lai C H, Liao W H, et al. Consistency trajectory models: Learning probability flow ode trajectory of diffusion[J]. arXiv preprint arXiv:2310.02279, 2023.