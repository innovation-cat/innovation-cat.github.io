---
title: 'Inverse Problems with Generative Priors'
date: 2025-11-10
permalink: /posts/2025/11/inverse-problems/
tags:
  - Flow Matching
  - Diffusion Model
  - Consistency Models
  - Trajectory
  - Distillation
---



<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>ðŸ“š Table of Contents</strong></span>
  </summary>
  <ul>
    <li><a href="#section1">1. The Probabilistic Formulation of Inverse Problems</a>
		<ul>
		  <li><a href="#section1.1">1.1 Inverse Problems without Priors: Maximum Likelihood Estimation</a></li>
		  <li><a href="#section1.2">1.2 Inverse Problems with Priors: Maximum A Posteriori Estimation
</a></li>
		</ul>
	</li>
	<li><a href="#section2">2. Classical Regularization via Implicit Priors</a>
		<ul>
		  <li><a href="#section2.1">2.1 Smoothness Priors</a></li>
		  <li><a href="#section2.2">2.2 Sparsity Priors</a></li>
		  <li><a href="#section2.3">2.3 Self-Similarity and Low-Rank Priors</a></li>
		  <li><a href="#section2.4">2.4 Learning-Based Implicit Priors</a></li>
		  <li><a href="#section2.5">2.5 Summary of Classical Regularization Priors</a></li>
		</ul>
	</li>
	<li><a href="#section3">3. Deep Generative Models as Explicit Priors</a>
		<ul>
		  <li><a href="#section3.1">3.1 Generative Adversarial Networks: Manifold Priors</a></li>
		  <li><a href="#section3.2">3.2 Variational Autoencoders: Latent-Variable Priors</a></li>
		  <li><a href="#section3.3">3.3 Normalizing Flows: Exact Log-Density Priors</a></li>
		  <li><a href="#section3.4">3.4 Diffusion Models: Score-Based Priors</a></li>
		</ul>
	</li>
	<li><a href="#section11">11. References</a></li>
  </ul>
</details>




Inverse problems, which aim to recover a signal of interest from indirect and often corrupted measurements, are a cornerstone of computational science and engineering. These problems are typically ill-posed, necessitating the use of prior knowledge to regularize the solution space and ensure a unique and stable reconstruction. This paper provides a structured exposition of the evolution of priors in solving inverse problems, from classical formulations to the modern paradigm of deep generative models. We begin by formalizing the inverse problem from a probabilistic perspective, deriving the Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP) frameworks, which correspond to solutions without and with priors, respectively. We then review classical, non-generative regularization techniques, focusing on the influential Plug-and-Play (PnP) and Regularization by Denoising (RED) frameworks, which leverage off-the-shelf denoisers as implicit priors. The core of this paper is dedicated to the contemporary approach of employing deep generative modelsâ€”including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), Normalizing Flows, and Diffusion Modelsâ€”as powerful, explicit priors. We detail how the unique properties of each model class can be integrated into the inverse problem objective, enabling state-of-the-art performance by constraining solutions to a learned data manifold. This work aims to bridge the conceptual gap between classical and modern techniques, offering a unified view of the role of priors in solving generative inverse problems.

---

<h1 id="section1" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">1. The Probabilistic Formulation of Inverse Problems</h1>



An inverse problem seeks to recover an unknown signal $\mathbf{x} \in \mathbb{R}^n$ from a set of observed measurements $\mathbf{y} \in \mathbb{R}^m$. This process is typically modeled by a linear forward operator $\mathbf{A}: \mathbb{R}^n \rightarrow \mathbb{R}^m$ and corrupted by additive noise $\mathbf{n} \in \mathbb{R}^m$:

$$
\mathbf{y} = \mathbf{A}\mathbf{x} + \mathbf{n}
$$

The operator $\mathbf{A}$ models the physics of the measurement process, such as a blurring kernel in deblurring, a Radon transform in computed tomography (CT), or a subsampling mask in magnetic resonance imaging (MRI). The problem is deemed "inverse" because we seek to reverse the effect of $\mathbf{A}$. It is often **ill-posed**, meaning that a solution may not exist, may not be unique, or may not depend continuously on the data. This ill-posedness arises when $\mathbf{A}$ is rank-deficient or ill-conditioned ($m < n$ or singular values decay rapidly), making the recovery of $\mathbf{x}$ from $\mathbf{y}$ an ambiguous task.


---

<h1 id="section1.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">1.1 Inverse Problems without Priors: Maximum Likelihood Estimation</h1> 


In the absence of any prior knowledge about the signal $\mathbf{x}$, our estimation relies solely on the data formation model. A common and mathematically convenient assumption is that the noise $\mathbf{n}$ is independent and identically distributed (i.i.d.) Gaussian with zero mean and variance $\sigma^2$, i.e., 

$$\mathbf{n} \sim \mathcal{N}(\mathbf{0}, \sigma^2\mathbf{I})$$

This assumption allows us to formulate the problem within the framework of Maximum Likelihood Estimation (MLE). The likelihood function $p(\mathbf{y}\mid \mathbf{x})$ describes the probability of observing the measurements $\mathbf{y}$ given a specific signal $\mathbf{x}$. Under the Gaussian noise assumption, the relationship $\mathbf{y} = \mathbf{A}\mathbf{x} + \mathbf{n}$ implies that $\mathbf{y}$ is also Gaussian-distributed, centered at $\mathbf{A}\mathbf{x}$:

$$
p(\mathbf{y}\mid \mathbf{x}) = \mathcal{N}(\mathbf{y}; \mathbf{A}\mathbf{x}, \sigma^2\mathbf{I})
$$

The probability density function for this multivariate Gaussian distribution is given by:

$$
p(\mathbf{y}\mid \mathbf{x}) = \frac{1}{(2\pi\sigma^2)^{m/2}} \exp \left(\,-\frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2\, \right)
$$

The MLE principle seeks the estimate $\hat{\mathbf{x}}_{\text{MLE}}$ that maximizes this likelihood. For computational stability and simplicity, it is standard practice to maximize the log-likelihood instead:

$$
\hat{\mathbf{x}}_{\text{MLE}} = \arg\max_{\mathbf{x}} \log p(\mathbf{y}\mid \mathbf{x}) = \arg\max_{\mathbf{x}} \left[ -\frac{m}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 \right]
$$

Since the first term is a constant with respect to $\mathbf{x}$, and maximizing the negative of a function is equivalent to minimizing the function itself, the MLE objective simplifies to a least-squares problem:

$$
\hat{\mathbf{x}}_{\text{MLE}} = \arg\min_{\mathbf{x}} \underbrace{\,\|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2\, }_{\text{data consistency}}
$$

This objective is also known as the Mean Squared Error (MSE) data fidelity term. While principled, it is insufficient for ill-posed problems, as many different signals $\mathbf{x}$ can yield a similarly small residual $\|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2$, often resulting in solutions dominated by noise.


---

<h1 id="section1.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">1.2 Inverse Problems with Priors: Maximum A Posteriori Estimation</h1> 

To overcome the limitations of MLE, we introduce prior knowledge about the expected properties of the signal $\mathbf{x}$. This is formalized using a Bayesian approach. We define a prior distribution $p(\mathbf{x})$ that assigns higher probability to signals that are plausible (e.g., natural images with smooth regions and sharp edges) and lower probability to others (e.g., pure noise).

Using Bayes' theorem, we can combine the likelihood $p(\mathbf{y}\mid \mathbf{x})$ with the prior $p(\mathbf{x})$ to obtain the posterior distribution $p(\mathbf{x}\mid \mathbf{y})$:

$$
p(\mathbf{x}\mid \mathbf{y}) = \frac{p(\mathbf{y}\mid \mathbf{x})p(\mathbf{x})}{p(\mathbf{y})}
$$

The posterior represents our updated belief about $\mathbf{x}$ after observing the data $\mathbf{y}$. The goal of Maximum A Posteriori (MAP) estimation is to find the signal $\hat{\mathbf{x}}_{\text{MAP}}$ that maximizes this posterior probability:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\max_{\mathbf{x}} p(\mathbf{x}\mid \mathbf{y}) = \arg\max_{\mathbf{x}} \frac{p(\mathbf{y}\mid \mathbf{x})p(\mathbf{x})}{p(\mathbf{y})}
$$

Since the evidence $p(\mathbf{y})$ is constant with respect to $\mathbf{x}$, the MAP objective becomes:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\max_{\mathbf{x}} p(\mathbf{y}\mid \mathbf{x})p(\mathbf{x})
$$

Again, we work with the log-posterior for convenience:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\max_{\mathbf{x}} \left[ \log p(\mathbf{y}|\mathbf{x}) + \log p(\mathbf{x}) \right]
$$

Substituting the log-likelihood term from the Gaussian noise model, we get:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\max_{\mathbf{x}} \left[ -\frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 + \log p(\mathbf{x}) \right]
$$

This is equivalent to minimizing the negative log-posterior:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\min_{\mathbf{x}} \left[ \frac{1}{2\sigma^2} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 - \log p(\mathbf{x}) \right]
$$

This formulation elegantly connects Bayesian inference to regularized optimization. If we define a regularization function $R(\mathbf{x}) = -\log p(\mathbf{x})$ and a regularization parameter $\lambda = 2\sigma^2$, the MAP estimation is precisely equivalent to solving the following optimization problem:

$$
\hat{\mathbf{x}}_{\text{MAP}} = \arg\min_{\mathbf{x}} \left[ \underbrace{\|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2}_{\text{data consistency}} + \underbrace{\lambda R(\mathbf{x})}_{\text{Prior / Regularization}} \right]
$$

Here, the data fidelity term ensures consistency with the measurements, while the regularization term $R(\mathbf{x})$ enforces the prior. The choice of $R(\mathbf{x})$ is paramount and has been the subject of extensive research.

---

<h1 id="section2" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">2. Classical Regularization via Implicit Priors</h1>



Before the widespread adoption of deep generative models, the regularization term $R(\mathbf{x})$ in the MAP objective was primarily designed based on handcrafted statistical assumptions about the signal. These priors, while not explicitly modeling the entire data distribution $p(\mathbf{x})$, were instrumental in making ill-posed problems solvable. They can be broadly categorized based on their underlying assumptions.


---

<h1 id="section2.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.1 Smoothness Priors</h1> 


The Core Assumption is that: Natural signals, such as images, are predominantly smooth or piecewise-constant. High-frequency variations are often attributed to noise rather than the underlying signal structure. Representative Regularizers including:

1. **Tikhonov Regularization:** This is one of the earliest and most fundamental regularizers, penalizing the L2-norm of the signal's derivatives. The functional is $R(\mathbf{x}) = \|\mathbf{L}\mathbf{x}\|_2^2$, where $\mathbf{L}$ is a linear operator, typically a discrete approximation of the gradient or Laplacian operator. T

2. **Total Variation (TV):** A significant advancement over Tikhonov, the TV regularizer is defined as the L1-norm of the signal's gradient magnitude: $R(\mathbf{x}) = \|\nabla \mathbf{x}\|_1$. Unlike the L2-norm, which penalizes large gradients heavily, the L1-norm promotes solutions where gradients are sparse, effectively preserving sharp edges while enforcing smoothness in flat regions.


---

<h1 id="section2.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.2 Sparsity Priors</h1> 


The Core Assumption is that: Many signals, while not sparse in their native domain (e.g., pixel space), can be represented sparsely in a suitable transform domain. This means that when the signal $\mathbf{x}$ is transformed by a basis or dictionary $\mathbf{\Psi}$, the resulting coefficient vector $\mathbf{\alpha} = \mathbf{\Psi}\mathbf{x}$ has very few non-zero entries.

The most common regularizer to enforce sparsity is the L1-norm of the transform coefficients: 

$$R(\mathbf{x}) = \|\mathbf{\Psi}\mathbf{x}\|_1$$ 

The choice of $\mathbf{\Psi}$ is critical and domain-specific, with common choices including the Wavelet transform for piecewise-smooth images, the Discrete Cosine Transform (DCT) for photographic images, or a learned dictionary. This forms the cornerstone of the Compressed Sensing theory.


---

<h1 id="section2.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.3 Self-Similarity and Low-Rank Priors</h1> 



The Core Assumption is that: Natural images exhibit a high degree of redundancy, where small patches often repeat themselves throughout the image, either exactly or with slight variations. A collection of such similar patches, when vectorized and stacked as columns of a matrix, forms a matrix with a low rank. Representative Regularizers including:

1. **Non-Local Priors:** Algorithms like Non-Local Means (NL-Means) leverage this assumption for denoising by averaging pixels weighted by the similarity of their surrounding patches, not just spatial proximity.

2. **Low-Rank Matrix Regularization:** This idea is formalized by constructing a matrix $\mathbf{P}_{\mathbf{x}}$ from similar patches in the image $\mathbf{x}$ and imposing a low-rank constraint. Since the rank function is non-convex and difficult to optimize, its convex relaxation, the **nuclear norm** $\|\cdot\|_*$ (the sum of singular values), is used as the regularizer: $R(\mathbf{x}) = \|\mathbf{P}_{\mathbf{x}}\|_*$. Landmark methods like BM3D combine patch grouping (self-similarity) with collaborative sparse coding of the patch groups.

---

<h1 id="section2.4" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.4 Learning-Based Implicit Priors</h1> 


This class represents a bridge between handcrafted priors and explicit generative models. Instead of defining a fixed analytical regularizer, these methods learn an implicit prior from data, encapsulated within a powerful denoising algorithm.

**Core Assumption:** The manifold of natural signals can be implicitly characterized by the action of a state-of-the-art denoiser. A clean signal should be a fixed point of the denoiser, i.e., $\mathcal{D}(\mathbf{x}) \approx \mathbf{x}$, while a noisy signal will be moved towards the manifold. Representative Frameworks including:

1. **Plug-and-Play (PnP):** The PnP framework leverages optimization algorithms that split the MAP objective into separate data-fidelity and regularization steps (e.g., ADMM, HQS). The key insight is that the proximal operator associated with the regularizer, $\text{prox}_{\lambda R}(\cdot)$, is mathematically equivalent to a denoising operation. PnP replaces this abstract proximal step with a call to a powerful, pre-trained denoiser $\mathcal{D}(\cdot)$, such as BM3D or a deep network like DnCNN.

2. **Regularization by Denoising (RED):** RED provides a more formal foundation by constructing an explicit regularizer $R(\mathbf{x})$ from a denoiser $\mathcal{D}(\mathbf{x})$, whose gradient is defined as $\nabla R(\mathbf{x}) = \mathbf{x} - \mathcal{D}(\mathbf{x})$. This allows the use of standard gradient-based optimization methods on the MAP objective: $\arg\min_{\mathbf{x}} \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 + \lambda R(\mathbf{x})$, with the gradient of the second term provided by the denoiser's residual.



  
---

<h1 id="section2.5" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.5 Summary of Classical Regularization Priors</h1> 

While powerful, these classical and implicit methods do not model the full data distribution $p(\mathbf{x})$. This limits their ability to, for instance, quantify uncertainty or generate diverse solutions consistent with the measurements. This limitation motivates the paradigm shift towards using explicit deep generative models as priors, as we will explore in the next section.

| Prior Category | Core Assumption | Representative Regularizers / Methods | Advantages | Disadvantages |
| :--- | :--- | :--- | :--- | :--- |
| **Smoothness Priors** | Signals are predominantly smooth or piecewise-constant. High-frequency variations are noise. | â€¢ Tikhonov: $\|\mathbf{L}\mathbf{x}\|_2^2$ <br> â€¢ Total Variation (TV): $\|\nabla \mathbf{x}\|_1$ | â€¢ Simple, convex, and computationally efficient. <br> â€¢ Effective for signals that are inherently smooth or blocky. | â€¢ **Tikhonov**: Oversmooths, blurring sharp edges. <br> â€¢ **TV**: Can create "staircasing" artifacts. <br> â€¢ Both are too simple for complex textures. |
| **Sparsity Priors** | Signals have a sparse representation in a suitable transform domain (e.g., Wavelets, DCT). | â€¢ L1-norm of transform coefficients: $\|\mathbf{\Psi}\mathbf{x}\|_1$ <br> â€¢ Foundational to Compressed Sensing. | â€¢ Strong theoretical guarantees for recovery under certain conditions. <br> â€¢ Excellent for signals with a known sparsifying basis. | â€¢ Relies on finding a suitable transform $\mathbf{\Psi}$. <br> â€¢ Not effective for signals without a sparse representation. <br> â€¢ Can introduce bias in coefficient amplitudes. |
| **Self-Similarity & Low-Rank Priors** | Natural signals contain repetitive patterns. Grouped similar patches form a low-rank matrix. | â€¢ **Methods**: NL-Means, BM3D <br> â€¢ **Regularizer**: Nuclear Norm of patch matrix, $\|\mathbf{P}_{\mathbf{x}}\|_*$ | â€¢ Captures complex non-local structures and textures effectively. <br> â€¢ Achieved state-of-the-art results for many years. | â€¢ Computationally very expensive due to patch searching and grouping. <br> â€¢ Can introduce patch-based artifacts. <br> â€¢ Difficult to formulate as a simple, explicit regularizer. |
| **Learning-Based Implicit Priors** | The manifold of natural signals can be implicitly defined by the action of a powerful denoiser. | â€¢ **Plug-and-Play (PnP)**: Replaces proximal operator with a denoiser $\mathcal{D}(\cdot)$. <br> â€¢ **Regularization by Denoising (RED)**: Defines prior gradient as $\nabla R(\mathbf{x}) = \mathbf{x} - \mathcal{D}(\mathbf{x})$. | â€¢ Highly flexible and modular; leverages SOTA denoisers (incl. deep nets). <br> â€¢ Excellent empirical performance without needing to train a full generative model. | â€¢ A "black box" prior. <br> â€¢ Convergence guarantees can be weak, especially for PnP with complex denoisers. <br> â€¢ Performance is entirely dependent on the quality and domain of the pre-trained denoiser. |  
  
  
  
  
---

<h1 id="section3" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">3. Deep Generative Models as Explicit Priors</h1>  
  
  

PnP and RED use denoisers as powerful but implicit priors. The latest paradigm shift involves using deep generative models that explicitly learn the entire data distribution $p(\mathbf{x})$ from a training set. This allows for a more direct and expressive enforcement of the prior. The core idea is to **constrain the solution $\mathbf{x}$ to lie on the low-dimensional manifold learned by the model.**


---

<h1 id="section3.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.1 Generative Adversarial Networks: Manifold Priors</h1> 

A standard GAN, consisting of a generator $\mathcal{G}(\mathbf{z})$ and a discriminator $\mathcal{D}$. However, both of them are not provide a tractable method for evaluating $p(\mathbf{x})$ for an arbitrary signal $\mathbf{x}$. 

Therefore, applying a GAN prior does not involve formulating an explicit regularizer 

$$R(\mathbf{x})=-\log p(x)$$ 

that is then added to the data consistency term. Instead, the prior is enforced through a **reparameterization of the solution space**. It acts as a powerful **manifold constraint**, fundamentally changing the nature of the optimization problem itself.

Suppose a pre-trained GAN generator

$$
\mathcal{G}: \mathcal{Z}  \rightarrow \mathbb{R}^n\,\qquad \text{where} \mathcal{Z} \in \mathbb{R}^d$\, \text{and}\, $d \ll n
$$ 

learns a mapping from a simple, low-dimensional latent space $\mathcal{Z}$ to the high-dimensional signal space $\mathbb{R}^n$. The core assumption when using a GAN as a prior is that all plausible signals (e.g., natural images) lie on or very close to the low-dimensional manifold defined by the **range of the generator**. We denote this manifold as $\mathcal{M}$:

$$
\mathcal{M} = \{ \mathcal{G}(\mathbf{z}) \mid \mathbf{z} \in \mathcal{Z} \}
$$


This manifold assumption can be conceptually translated into an explicit, albeit impractical, regularizer. This regularizer would be an indicator function for the manifold $\mathcal{M}$:

$$
R_{\text{GAN}}(\mathbf{x}) = \begin{cases} -\log p(z) & \text{if } \mathbf{x}=\mathcal{G}(\mathbf{z}) \in \mathcal{M} \\ \\
+\infty & \text{if } \mathbf{x} \notin \mathcal{M} \end{cases}
$$

If we were to plug this into the MAP objective, we would get:

$$
\begin{align}
\hat{\mathbf{x}} & = \arg\min_{\mathbf{x} \in \mathbb{R}^n} \left( \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 + \lambda R_{\text{GAN}}(\mathbf{x}) \right) \\[10pt]
\Longrightarrow \hat{\mathbf{z}} & = \arg\min_{\mathbf{z}} \|\mathbf{y} - \mathbf{A}(\mathcal{G}(\mathbf{z}))\|_2^2 + \lambda R(\mathbf{z})
\end{align}
$$
    
This objective forces the solution $\mathbf{x}$ to be strictly within the manifold $\mathcal{M}$ (to avoid the infinite penalty), and among all possible solutions on the manifold, it finds the one that best fits the measurements $\mathbf{y}$.

$$
-\log p(\mathbf{z}) = -\log \left( \frac{1}{(2\pi)^{d/2}} \exp\left(-\frac{1}{2}\|\mathbf{z}\|_2^2\right) \right) = \frac{1}{2}\|\mathbf{z}\|_2^2 + \text{const.}
$$

This leads to the most common and practical objective function for GAN-based inverse problems:

$$
\hat{\mathbf{z}} = \arg\min_{\mathbf{z} \in \mathcal{Z}} \left( \|\mathbf{y} - \mathbf{A}(\mathcal{G}(\mathbf{z}))\|_2^2 + \lambda \|\mathbf{z}\|_2^2 \right)
$$

Minimal pseudocode summaries:

```python
# GAN latent optimization: x = G_phi(z), z ~ N(0, I)
initialize z ~ N(0, I)
for k in range(iters):
    x = G_phi(z)
    data = (1/(2*sigma2)) * ||A x - y||^2
    prior = (1/2) * ||z||^2                  # -log p(z)
    # (optional) add identity/perceptual/CLIP regularizers as needed
    z = z - Î· * âˆ‡_z (data + Î» * prior)
return G_phi(z)
```



---

<h1 id="section3.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.2 Variational Autoencoders: Latent-Variable Priors</h1> 

VAE define a latent-variable generative model with an decoder (generator):

$$
x\sim p_\theta(x\mid z), \qquad \text{where}\,z\sim p(z)=\mathcal{N}(0,I)
$$

and an encoder $q_\phi(z\mid x)$. This allows the marginal likelihood $\log p_\theta(x)$ to be lower-bounded by the well-known Evidence Lower Bound (ELBO):

$$
\log p_\theta(x)\;\ge\; \mathbb{E}_{q_\phi(z\mid x)}\!\left[\log p_\theta(x\mid z)\right]-
D_{\mathrm{KL}}\!\left(q_\phi(z\mid x)\,\|\, p(z)\right)
\triangleq \mathrm{ELBO}(x;\theta,\phi).
$$

At first glance, the ELBO appears to offer a tractable surrogate for $-\log p(x)$, 

$$
\hat{\mathbf{x}} = \arg\min_{\mathbf{x}} \left( \|\mathbf{y} - \mathbf{A}\mathbf{x}\|_2^2 - \lambda \cdot \text{ELBO}(\mathbf{x}) \right)
$$

However, this approach is problematic for several reasons:

1. **Problem 1:** The most significant issue is that the ELBO is a function of $\mathbf{x}$ **through the encoder** $q_{\phi}(\mathbf{z}\mid \mathbf{x})$. During the inverse problem solving phase, we are searching for an unknown, clean signal $\mathbf{x}$.

   When we try to minimize the objective with respect to $\mathbf{x}$, the gradient of the regularization term, $\nabla_{\mathbf{x}} \text{ELBO}(\mathbf{x})$, must be computed. This involves backpropagating through the entire encoder network $q_{\phi}$, the KL divergence calculation, and the expectation term. This creates an extremely complex, high-dimensional, and likely non-convex optimization landscape. The optimization is performed on the variable $\mathbf{x}$ which is simultaneously the **input** to the encoder.

2. **Problem 2:** The "Gap" Problem. ELBO is Only a Lower Bound, the inequality 

   $$\log p(\mathbf{x}) \ge \text{ELBO}(\mathbf{x})$$ 
   
   is central. This gap can be non-zero and highly variable.


Due to the above limitations, VAE-based inverse problem solvers almost universally adopt a **latent-space MAP formulation**. Since the VAE defines a simple Gaussian prior over (z), one instead parameterizes the reconstructed signal as

$$
x = G_\theta(z),
$$

where $G_\theta$ is the VAE decoder (the conditional mean of $p_\theta(x\mid z)$). The prior reduces to

$$
-\log p(z) = \tfrac{1}{2}\|z\|_2^2
$$

yielding the latent-variable MAP objective:

$$
z^\star = \arg\min_{z}
\|A(G_\theta(z)) - y\|^2\;+\;\lambda \|z\|_2^2 ,\qquad x^\star = G_\theta(z^\star).
$$

This approach eliminates the encoder entirely and transforms the inverse problem into a low-dimensional optimization over (z), yielding stable and efficient solvers.

Minimal pseudocode summaries:

```python
# VAE prior: x = G_theta(z), z ~ N(0, I)
initialize z ~ N(0, I)
for k in range(iters):
    x = G_theta(z)
    data = (1/(2*sigma2)) * ||A x - y||^2     # or -log p(x|z) if decoder has explicit likelihood
    prior = (1/2) * ||z||^2                   # -log p(z)
    z = z - Î· * âˆ‡_z (data + Î» * prior)
return G_theta(z)
```

---

<h1 id="section3.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.3 Normalizing Flows: Exact Log-Density Priors</h1> 



Normalizing flows (NFs) define an invertible, differentiable map 

$$f_\theta:\mathbb{R}^d\!\to\!\mathbb{R}^d$$ 

with 

$$x=f_\theta(z),\qquad z\sim p(z) = \mathcal N(0,I)$$. 

The change-of-variables formula yields an **exact** log-density for any $x$:

$$
\log p_\theta(x) = \log p(z)\;-\;\log\!\left|\det J_{f_\theta}(z)\right|, \qquad z=f_\theta^{-1}(x),
$$

so the MAP objective is

$$
\hat{\mathbf{x}} & = \arg\min_{\mathbf{x}} \left( \|Ax-y\|^2\;-\;\log p_\theta(x) \right)
$$

Crucially, because $f_\theta$ is **bijective**, we can optimize either **in pixel space** ($x$-space) or **in latent space** ($z$-space). The two are mathematically equivalent but have distinct computational trade-offs.

- **Pixel-Space MAP (optimize over $x$).** The Objective is .
  
  $$
  \begin{align}
  \hat{\mathbf{x}} & = \arg\min_{\mathbf{x}} \left( \|Ax-y\|^2\;-\;\log p_\theta(x) \right) \\[10pt]
  & = \arg\min_{\mathbf{x}} \left( \|Ax-y\|^2 - \log p\!\big(f_\theta^{-1}(x)\big)\;+\;\log\!\left|\det J_{f_\theta}\!\big(f_\theta^{-1}(x)\big)\right| \right)
  $$
  
  The gradient with respect to $x$ can be computed with chain of rule. 

  $$
  \nabla_x\!\left[-\log p_\theta(x)\right] = -\Big(\nabla_z \log p(z) - \nabla_z \log |\det J_{f_\theta}(z)|\Big)\;J_{f^{-1}}(x).
  $$
  
  where 
  
  $$z=f_\theta^{-1}(x) \quad \text{and} \quad J_{f^{-1}}(x)=\partial z/\partial x$$,
  
  Thus each step requires evaluating the inverse flow and backpropagating through the log-det term. This is direct but can be heavier in practice (especially when (f_\theta^{-1}) is costly).
  
  ```python
  # Flow prior in x-space
  # x â†” z via invertible f_theta, with log p(x) exact
  initialize x
  for k in range(iters):
    z = f_inv_theta(x)                         # z = f^{-1}(x)
    logp_x = log_pz(z) - logdetJ_f_theta(z)
    # compute âˆ‡_x log p(x) via implicit differentiation / JVPs
    grad_prior = grad_x_logp_x(x, z)          # framework-specific
    x = x + Î· * (grad_prior - grad_likelihood(x) * sigma2)  # MAP step
  return x
  ```
  
- **Latent-Space MAP (optimize over $z$)**. Reparameterize $x=f_\theta(z)$ and minimize over $z$:

  $$
  \hat{\mathbf{z}} & = \arg\min_{\mathbf{z}} \left( \|Ax-y\|^2\;-\;\log p(z)\;+\;\log\!\left|\det J_{f_\theta}(z)\right|. \right)
  $$
  
  This avoids the inverse pass and often yields smoother optimization. For 
  
  $$p(z)=\mathcal N(0,I) \quad \Longrightarrow \quad -\log p(z)=\frac{1}{2}|z|^2$$ 

  Compared to optimization in the $x$-space, optimization in the latent space $z$ is a more commonly used approach because it is **simpler**, more **stable**, and does not require backpropagating to compute the Jacobian at each step.


  ```python
  # Flow prior in z-space
  initialize z ~ N(0, I)
  for k in range(iters):
    x = f_theta(z)
    loss = (1/(2*sigma2)) * ||A x - y||^2 - log_pz(z) + logdetJ_f_theta(z)
    z = z - Î· * âˆ‡_z loss
  return f_theta(z)
  ```
  

---

<h1 id="section3.4" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.4 Diffusion Models: Score-Based Priors</h1> 


Diffusion models have recently emerged as the state-of-the-art in generative modeling. They learn to reverse a diffusion process that gradually adds noise to data. This is equivalent to learning the score function of the data distribution, $\nabla_{\mathbf{x}} \log p_t(\mathbf{x}_t)$, at different noise levels $t$.

This property is exceptionally well-suited for solving inverse problems via posterior sampling or optimization. The score of the posterior distribution $\log p(\mathbf{x}\mid \mathbf{y})$ can be derived using Bayes' rule:

$$
\nabla_{\mathbf{x}} \log p(\mathbf{x}|\mathbf{y}) = \nabla_{\mathbf{x}} \log p(\mathbf{y}|\mathbf{x}) + \nabla_{\mathbf{x}} \log p(\mathbf{x})
$$

The two terms on the right are both tractable:
1.  **Likelihood Score:** For Gaussian noise, $\log p(\mathbf{y}\mid \mathbf{x}) \propto -\|\mathbf{y}-\mathbf{A}\mathbf{x}\|_2^2 / (2\sigma^2)$. The gradient is $\nabla_{\mathbf{x}} \log p(\mathbf{y}\mid \mathbf{x}) = \frac{1}{\sigma^2}\mathbf{A}^T(\mathbf{y} - \mathbf{A}\mathbf{x})$.

2.  **Prior Score:** The term $\nabla_{\mathbf{x}} \log p(\mathbf{x})$ is precisely what the pre-trained diffusion model provides.

This allows for iterative algorithms that guide the reverse diffusion (sampling) process. At each step of the reverse process, the standard denoising update (which corresponds to the prior score) is perturbed by a step in the direction of the likelihood score. This effectively steers the generation process towards samples that are not only realistic (high $p(\mathbf{x})$) but also consistent with the measurements $\mathbf{y}$ (high $p(\mathbf{y}\mid \mathbf{x})$). This approach, seen in methods like Diffusion Posterior Sampling (DPS), has achieved remarkable results across a wide range of challenging inverse problems.


  
  
  

---

<h1 id="section3" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">3. References</h1>


[^Reflow]: Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022.

[^nvp]: Dinh L, Sohl-Dickstein J, Bengio S. Density estimation using real nvp[J]. arXiv preprint arXiv:1605.08803, 2016.

[^Glow]: Kingma D P, Dhariwal P. Glow: Generative flow with invertible 1x1 convolutions[J]. Advances in neural information processing systems, 2018, 31.

[^Neural_ODE]: Chen R T Q, Rubanova Y, Bettencourt J, et al. Neural ordinary differential equations[J]. Advances in neural information processing systems, 2018, 31.

[^Ffjord]: Grathwohl W, Chen R T Q, Bettencourt J, et al. Ffjord: Free-form continuous dynamics for scalable reversible generative models[J]. arXiv preprint arXiv:1810.01367, 2018.

[^rectified_flow]: Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022.

[^FM]: Lipman Y, Chen R T Q, Ben-Hamu H, et al. Flow matching for generative modeling[J]. arXiv preprint arXiv:2210.02747, 2022.

[^SI]: Albergo M S, Boffi N M, Vanden-Eijnden E. Stochastic interpolants: A unifying framework for flows and diffusions[J]. arXiv preprint arXiv:2303.08797, 2023.

[^SI_1]: Albergo M S, Vanden-Eijnden E. Building normalizing flows with stochastic interpolants[J]. arXiv preprint arXiv:2209.15571, 2022.

[^SI_2]: Albergo M S, Goldstein M, Boffi N M, et al. Stochastic interpolants with data-dependent couplings[J]. arXiv preprint arXiv:2310.03725, 2023.

[^improve_rf]: Lee S, Lin Z, Fanti G. Improving the training of rectified flows[J]. Advances in neural information processing systems, 2024, 37: 63082-63109.

[^instaflow]: Liu X, Zhang X, Ma J, et al. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation[C]//The Twelfth International Conference on Learning Representations. 2023.

[^meanflow]: Geng Z, Deng M, Bai X, et al. Mean flows for one-step generative modeling[J]. arXiv preprint arXiv:2505.13447, 2025.

[^flow_map]: Boffi N M, Albergo M S, Vanden-Eijnden E. Flow map matching with stochastic interpolants: A mathematical framework for consistency models[J]. Transactions on Machine Learning Research, 2025.

[^ayf]: Sabour A, Fidler S, Kreis K. Align Your Flow: Scaling Continuous-Time Flow Map Distillation[J]. arXiv preprint arXiv:2506.14603, 2025.

[^cmt]: Hu Z, Lai C H, Mitsufuji Y, et al. CMT: Mid-Training for Efficient Learning of Consistency, Mean Flow, and Flow Map Models[J]. arXiv preprint arXiv:2509.24526, 2025.

[^cm]: Song Y, Dhariwal P, Chen M, et al. Consistency models[J]. 2023.

[^ctm]: Kim D, Lai C H, Liao W H, et al. Consistency trajectory models: Learning probability flow ode trajectory of diffusion[J]. arXiv preprint arXiv:2310.02279, 2023.