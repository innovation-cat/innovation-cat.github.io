---
title: 'Stabilizing Diffusion Training: The Evolution of Network Architectures'
excerpt: "This article explores how architectural choices — from classical U-Nets to ADM refinements, latent diffusion, and the latest Transformer-based designs — fundamentally shape training stability. We examine normalization and conditioning mechanisms, residual and skip pathway innovations, and integrative paradigms such as EDM that unify architecture with preconditioning. Along the way, we highlight practical stability enhancements and emerging trends that point toward the next generation of robust diffusion architectures."
date: 2025-03-02
permalink: /posts/2025/03/diffusion-training-2/
tags:
  - Diffusion Model
  - UNET
  - Transformer
  - DiT
---

<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>📚 Table of Contents</strong></span>
  </summary>
  <ul style="margin:0; padding-left:1.1rem;">
	<li><a href="#section1">1. Why Architecture Matters for Stability</a>
		<ul>
			<li><a href="#section1.1">1.1 Gradient Flow, Conditioning, and Stability</a></li>
			<li><a href="#section1.2">1.2 Balancing Capacity vs. Robustness</a></li>
			<li><a href="#section1.3">1.3 Architecture–Noise Schedule Coupling</a></li>
		</ul>
	</li>
	<li><a href="#section2">2. Evolution of Diffusion Architectures</a>
		<ul>
			<li><a href="#section2.1">2.1 Classical U-Net Foundations</a></li>
			<li><a href="#section2.2">2.2 ADM Improvements (Attention, Class Conditioning)</a></li>
			<li><a href="#section2.3">2.3 Latent U-Net (Stable Diffusion, SDXL)</a></li>
			<li><a href="#section2.4">2.4 Transformer-based Designs (DiT, MMDiT-X, Hybrid Models)</a></li>
			<li><a href="#section2.5">2.5 Extensions to Video and 3D Diffusion (Video U-Net, Gaussian Splatting)</a></li>
			<li><a href="#section2.6">2.6 Lightweight & Memory-efficient Designs (MobileDiffusion, LightDiffusion)</a></li>
		</ul>
	</li>	
	<li><a href="#section3">3. Conditioning, Normalization & Tokenization</a>
		<ul>
			<li><a href="#section3.1">3.1 Normalization Evolution: GroupNorm → AdaLN-Zero</a></li>
			<li><a href="#section3.2">3.2 Time / σ Embedding Strategies (Sinusoidal, Fourier, FiLM)</a></li>
			<li><a href="#section3.3">3.3 Query-Key Normalization (e.g., SD3.5, MMDiT-X)</a></li>
			<li><a href="#section3.4">3.4 Multi-modal Token Conditioning (Cross-Attention, LoRA, Deep Fusion)</a></li>
			<li><a href="#section3.4">3.5 Token Compression and Merging for Efficiency & Stability</a></li>
		</ul>
	</li>
	<li><a href="#section4">4. Residual & Skip Pathway Designs</a>
		<ul>
			<li><a href="#section4.1">4.1 Classic Skip Connections in U-Net</a></li>
			<li><a href="#section4.2">4.2 Residual-in-Residual Architectures</a></li>
			<li><a href="#section4.3">4.3 Sparse Skip & Hourglass Designs (DiC)</a></li>
			<li><a href="#section4.4">4.4 Dynamic and Gated Skip Pathways</a></li>
			<li><a href="#section4.5">4.5 Progressive Growing & Skip Fading Strategies</a></li>
		</ul>
	</li>
	<li><a href="#section5">5. Transformer Architectures & Architectural Variants</a>
		<ul>
			<li><a href="#section5.1">5.1 Diffusion Transformers (DiT) and Scaling Laws</a></li>
			<li><a href="#section5.2">5.2 Decoupled Design (DDT: Encoder–Decoder Separation)</a></li>
			<li><a href="#section5.3">5.3 Automated Architecture Search (DiffusionNAG)</a></li>
			<li><a href="#section5.4">5.4 Multi-resolution Networks with Time-dependent Norms</a></li>
			<li><a href="#section5.5">5.5 State Space Models (S4, Mamba) as Alternatives to Transformers</a></li>
			<li><a href="#section5.6">5.6 Parallel vs. Sequential Transformer Architectures</a></li>
		</ul>
	</li>
	<li><a href="#section6">6. Improved EDM as an Integrative Design Paradigm</a>
		<ul>
			<li><a href="#section6.1">6.1 Preconditioning with AdaLN-Zero</a></li>
			<li><a href="#section6.2">6.2 Hybridization of Architectural and Regularization Strategies</a></li>
			<li><a href="#section6.3">6.3 Structurally Balanced Design for Training Stability</a></li>
			<li><a href="#section6.4">6.4 Architecture–Noise Schedule Co-design</a></li>
		</ul>
	</li>
	<li><a href="#section7">7. Practical Stability Enhancements</a>
		<ul>
			<li><a href="#section7.1">7.1 Weight Initialization and Parameter Scaling</a></li>
			<li><a href="#section7.2">7.2 Gradient Clipping and EMA for Stable Convergence</a></li>
			<li><a href="#section7.3">7.3 Mixed Precision Training and Stability Trade-offs</a></li>
			<li><a href="#section7.4">7.4 Parameter-efficient Modules: LoRA, DoRA, T-Fixup</a></li>
			<li><a href="#section7.5">7.5 Training-free Stability Tricks (Noise Rescaling, Variance Matching)</a></li>
		</ul>
	</li>
	<li><a href="#section8">8. Conclusion</a></li>
	<li><a href="#section9">9. References</a></li>
  </ul>
</details>


When discussing the stability of diffusion model training, much of the focus often falls on noise schedules, loss weighting strategies, or optimization tricks (Please refer [our post](https://innovation-cat.github.io/posts/2025/01/diffusion-model-2/)). While these aspects are undeniably important, an equally critical — yet sometimes underemphasized — factor is the choice of network architecture itself. The structure of the model fundamentally determines how signals, gradients, and conditioning information propagate across different noise levels, and whether the training process converges smoothly or collapses into instability.

---

# <a id="section1">1. Why Architecture Matters for Stability</a>

Network architecture is more than a vessel for function approximation in diffusion models — it is the key component that determines whether training succeeds or fails.


## <a id="section1.1">1.1 Gradient Flow, Conditioning, and Stability</a>

Diffusion models are trained under extreme conditions: inputs span a spectrum from nearly clean signals to pure Gaussian noise. This makes them particularly sensitive to how gradients are normalized, how residuals accumulate, and how skip connections or attention layers interact with noisy features.

- **Improper gradient flow** can cause exploding updates at low-noise regimes or vanishing signals at high-noise regimes.
- **Conditioning pathways** (e.g., cross-attention for text or multimodal prompts) introduce additional sensitivity, as misaligned normalization or unbalanced skip pathways can destabilize learning.

Architectural innovations such as **GroupNorm, AdaLN-Zero, and preconditioning layers** have been specifically introduced to address these gradient stability issues, ensuring that the network remains trainable across a wide dynamic range of noise.

---

## <a id="section1.2">1.2 Balancing Capacity vs. Robustness</a>

A second challenge lies in the tension between **capacity** (the ability of the architecture to represent complex distributions) and **robustness** (the ability to generalize under noisy, unstable conditions).

- Early **U-Net designs** offered robustness through simplicity and skip connections, but limited capacity for scaling.
- **Transformer-based diffusion models (DiT, MMDiT-X)** introduced massive representational power, but at the cost of more fragile training dynamics.
- Newer architectures explore hybrid or modular designs — combining convolutional inductive biases, residual pathways, and attention — to find a stable equilibrium between these two competing goals.


## <a id="section1.3">1.3 Architecture–Noise Schedule Coupling</a>

Finally, the stability of diffusion training cannot be isolated from the **noise schedule**. Architectural design interacts tightly with how noise levels are distributed and parameterized:

- A model with **time-dependent normalization layers** may remain stable under variance-preserving schedules but collapse under variance-exploding ones.
- EDM (Elucidated Diffusion Models) highlight that **architecture and preconditioning must be co-designed** with the training noise distribution, rather than treated as independent modules.

This coupling implies that progress in diffusion training stability comes not only from better solvers or schedules, but from **holistic architectural design** that accounts for gradient dynamics, representation capacity, and their interplay with noise parameterization.


---

# <a id="section2">2. Evolution of Diffusion Architectures</a>

The architectural journey of diffusion models mirrors the evolution of deep learning itself: from simple convolutional backbones to large-scale Transformers, and now toward specialized multi-modal and efficiency-driven designs. Each stage has sought to reconcile two opposing pressures — **increasing representational power** while **preserving training stability**. In this section, we trace this trajectory across six key phases.

---

## <a id="section2.1">2.1 Classical U-Net Foundations</a>

The **U-Net architecture** [^unet] is the canonical backbone of early diffusion models such as DDPM [^ddpm]. Although originally proposed for biomedical image segmentation [^unet] , its **encoder–decoder structure with skip connections** turned out to be perfectly suited for denoising across different noise levels. The elegance of U-Net lies not only in its symmetry, but also in how it balances **global context extraction** with **local detail preservation**. A typical unet structure applied to the training of diffusion models is as follows

![U-Net architecture](/images/posts/2025-03-02-blog-post/unet.jpg)

where **ResB** represents residual block, who consists of multiple "norm-act-conv2d" layers. **Attent** represents self-Attention block. 

### <a id="section2.1.1">2.1.1 Encoder: From Local Features to Global Context</a>

The **encoder path** consists of repeated **convolutional residual blocks** and **downsampling operations** (e.g., strided convolutions or pooling). As the spatial resolution decreases and channel width expands, the network progressively shifts its representational emphasis:

- **High-resolution feature maps (early layers)** capture **fine-grained local structures** — edges, textures, and small patterns that are critical when denoising images at low noise levels.
- **Low-resolution feature maps (deeper layers)** aggregate **global context** — object shapes, spatial layout, and long-range dependencies. This is especially important at high noise levels, when much of the local structure has been destroyed and only global semantic cues can guide reconstruction.

Thus, the encoder effectively builds a **multi-scale hierarchy** of representations, transitioning from local to global as resolution decreases.

### <a id="section2.1.2">2.1.2 Bottleneck: Abstract Representation</a>

At the center lies the **bottleneck block**, where feature maps have the smallest spatial size but the largest channel capacity. This stage acts as the **semantic aggregator**:

- It condenses the global context extracted from the encoder.
- It often includes **attention layers** (in later refinements) to explicitly model long-range interactions.
  In the classical U-Net used by DDPM, the bottleneck is still purely convolutional, yet it already plays the role of a semantic “bridge” between encoding and decoding.

### <a id="section2.1.3">2.1.3 Decoder: Reconstructing Local Detail under Global Guidance</a>

The **decoder path** mirrors the encoder, consisting of **upsampling operations** followed by convolutional residual blocks. The role of the decoder is not merely to increase resolution, but to **inject global semantic context back into high-resolution predictions**:

- **Upsampling layers** expand the spatial resolution but initially lack fine detail.
- **Skip connections** from the encoder reintroduce high-frequency local features (edges, boundaries, textures) that would otherwise be lost in downsampling.
- By concatenating or adding these skip features to the decoder inputs, the network **fuses global context (from low-res encoder features)** with **local precision (from high-res encoder features)**.

This synergy ensures that the denoised outputs are both **semantically coherent** and **visually sharp**.



---

### <a id="section2.1.4">2.1.4 Timestep Embedding and Conditioning</a>

Unlike the U-Net’s original role in segmentation, a diffusion U-Net must also be conditioned on the **diffusion timestep** $t$, since the network’s task changes continuously as noise levels vary. In the classical DDPM implementation, this conditioning is realized in a relatively simple but effective way:

1. **Sinusoidal embedding.**
   Each integer timestep $t$ is mapped to a high-dimensional vector using sinusoidal position encodings (analogous to Transformers), ensuring that different timesteps are represented as distinct, smoothly varying signals.

2. **MLP transformation.**
   The sinusoidal embedding is passed through a small multilayer perceptron (usually two linear layers with a SiLU activation) to produce a richer time embedding vector $\mathbf{z}_t$.

3. **Additive injection into residual blocks.**
   In every residual block of the U-Net, $\mathbf{z}_t$ is projected to match the number of feature channels and then **added as a bias term** to the intermediate activations (typically after the first convolution).

This additive conditioning allows **each residual block** to adapt its computation based on the current noise level, without introducing extra normalization or complex modulation. The following figure shows the way how to inject timestep $t$ in each residual block.



![time embedding injection](/images/posts/2025-03-02-blog-post/time_embedding.jpg)

---



### <a id="section2.1.5">2.1.5 Why U-Net Works Well for Diffusion</a>

In diffusion training, inputs vary drastically in signal-to-noise ratio:

- At **low noise levels**, local details still survive; skip connections ensure these details propagate to the output.
- At **high noise levels**, local detail is destroyed; the decoder relies more on global semantics from the bottleneck.
- Across all levels, the encoder–decoder interaction guarantees that both **local fidelity** and **global plausibility** are preserved.

This explains why U-Nets became the **default backbone**: their **multi-scale design matches perfectly with the multi-scale nature of noise in diffusion models**. Later improvements (attention layers, latent-space U-Nets, Transformer backbones) all build upon this foundation, but the core idea remains: **stability in diffusion training emerges from balanced local–global feature fusion.**

---




## <a id="section2.2">2.2 ADM Improvements (Ablated Diffusion Models)</a>

While the classical U-Net backbone of DDPM demonstrated the feasibility of diffusion-based generation, it was still limited in stability and scalability. In the landmark work *“Diffusion Models Beat GANs on Image Synthesis”* [^adm], the authors performed extensive ablations to identify which architectural and training choices were critical at ImageNet scale. The resulting recipe is commonly referred to as **ADM (Ablated Diffusion Models)**. Rather than introducing a single new module, ADM represents a carefully engineered upgrade to the baseline U-Net, designed to balance **capacity, conditioning, and stability**.

### <a id="section2.2.1">2.2.1 Scaling the U-Net: Wider Channels and Deeper Residual Blocks</a>

The most straightforward but highly effective change was scaling up the model. The ADM UNet is significantly larger than the one used in the original DDPM paper.

- Wider Channels: The base channel count was increased (e.g., from 128 to 256), and the channel multipliers for deeper layers were adjusted, resulting in a much wider network.
- More Residual Blocks: The number of residual blocks per resolution level was increased, making the network deeper.

**Why it helps:** A larger model capacity allows the network to learn more complex and subtle details of the data distribution, leading to a direct improvement in sample fidelity.

### <a id="section2.2.2">2.2.2 Multi-Resolution Self-Attention</a>

While DDPM's UNet used self-attention, it was typically applied only at a single, low resolution (e.g., 16x16). ADM recognized that long-range dependencies are important at various scales. 

In ADM, Self-attention blocks were added at multiple resolutions (e.g., 32x32, 16x16, and 8x8). Additionally, the number of attention heads was increased.

- Attention at higher resolutions (32x32) helps capture relationships between medium-sized features and textures; 
- Attention at lower resolutions (8x8) helps coordinate the global structure and semantic layout of the image.

**Why it helps:** This multi-scale approach gives the model a more holistic understanding of the image, preventing structural inconsistencies and improving overall coherence.

### <a id="section2.2.3">2.2.3 Conditioning via Adaptive Group Normalization (AdaGN)</a> 

This is arguably the most significant architectural contribution of ADM. It fundamentally changes how conditional information (like timesteps and class labels) is integrated into the network.

- In DDPM: The time embedding was processed by an MLP and then simply added to the feature maps within each residual block. This acts as a global bias, which is a relatively weak form of conditioning.

- In ADM (AdaGN) [^film]: The model learns to modulate the activations using the conditional information. The process is as follows: a). The timestep embedding and the class embedding (for class-conditional models) are combined into a single conditioning vector; b). This vector is passed through a linear layer to predict two new vectors: a scale ($\gamma$) and a shift ($\beta$) parameter for each channel. c). Within each residual block, the feature map undergoes Group Normalization, and then its output is modulated by these predicted parameters.

![time embedding injection](/images/posts/2025-03-02-blog-post/adagn.jpg)

**Why it helps:** Modulation is a much more powerful mechanism than addition. It allows the conditional information to control the mean and variance of each feature map on a channel-by-channel basis. This gives the model fine-grained control over the generated features, dramatically improving its ability to adhere to the given conditions (i.e., generating a specific class at a specific noise level).

### <a id="section2.2.4">2.2.4  BigGAN-inspired Residual Blocks for Up/Downsampling</a>

ADM also identifies that the choice of downsampling and upsampling operations affects stability. 

- In DDPM: Downsampling might be a simple pooling or strided convolution, and upsampling might be a standard upsample layer followed by a convolution.
- In ADM: The upsampling and downsampling operations were integrated into specialized residual blocks, a design inspired by the highly successful BigGAN architecture [^biggan]. This ensures that information flows more smoothly as the resolution changes, minimizing information loss.

It favors **strided convolutions** for downsampling and **nearest-neighbor upsampling followed by convolution** for upsampling. 

**Why it helps:** This leads to better preservation of features across different scales, contributing to sharper and more detailed final outputs.

### <a id="section2.2.5">2.2.5 Rescaling of Residual Connections</a>

For very deep networks, it's crucial to maintain well-behaved activations. ADM introduced a simple but effective trick: The output of each residual block was scaled by a constant factor of 1/${\sqrt{2}}$ before being added back to the skip connection.

**Why it helps:** This technique helps to balance the variance contribution from the skip connection and the residual branch, preventing the signal from exploding in magnitude as it passes through many layers. This improves training stability for very deep models.

### <a id="section2.2.6">2.2.6 Why ADM Matters for Stability</a>

Relative to the classical DDPM U-Net, in conclusion, the ADM UNet is a masterclass in architectural refinement. By systematically enhancing every major component—from its overall scale to the precise mechanism of conditional injection—it provided the powerful backbone necessary for diffusion models to finally surpass GANs in image synthesis quality.

---

## <a id="section2.3">2.3 Latent U-Net: The Efficiency Revolution with Stable Diffusion and SDXL</a>

While the ADM architecture (Section [2.2](#section2.2)) marked the pinnacle of **pixel-space** diffusion models, achieving state-of-the-art quality by meticulously refining the U-Net, it faced a significant and inherent limitation: computational cost. Training and running diffusion models directly on high-resolution images (e.g., 512x512 or 1024x1024) is incredibly demanding in terms of both memory and processing power. The U-Net must process massive tensors at every denoising step, making the process slow and resource-intensive.

The introduction of Latent Diffusion Models (LDMs) [^sd], famously realized in Stable Diffusion, proposed a revolutionary solution: instead of performing the expensive diffusion process in the high-dimensional pixel space, why not perform it in a much smaller, perceptually equivalent **latent space**? This insight effectively decouples the task of perceptual compression from the generative learning process, leading to a massive leap in efficiency and accessibility.

---

## <a id="section2.3.1">2.3.1 The Core Idea: Diffusion in a Compressed Latent Space</a>

The training architecture of LDM is a two-stage process.

**Stage 1: Perceptual Compression**. A powerful, pretrained **Variational Autoencoder (VAE)** is trained to map high-resolution images into a compact latent representation and back. The encoder, $E$, compresses an image x into a latent vector $z = E(x)$. The decoder, $D$, reconstructs the image from the latent, . Crucially, this is not just any compression; it is perceptual compression, meaning the VAE is trained to discard high-frequency details that are imperceptible to the human eye while preserving critical semantic and structural information.

**Stage 2: Latent Space Diffusion**. Instead of training a U-Net on images x, we train it on the latent codes $z$. The forward diffusion process adds noise to $z$ to get $z_t$, and the U-Net's task is to predict the noise in this latent space.

The impact of this shift is dramatic. A 512x512x3 pixel image (786,432 dimensions) can be compressed by the VAE into a 64x64x4 latent tensor (16,384 dimensions)—a **48x reduction** in dimensionality. The U-Net now operates on these much smaller tensors, enabling faster training and significantly lower inference requirements.

The full generative (inference) process for a text-to-image model like Stable Diffusion is as follows:

- **Stage 1**: Text Prompt $\to$ Text Encoder $\to$ Conditioning Vector $c$.
- **Stage 2**: Random Noise $z_T$ $\to$ U-Net Denoising Loop in Latent Space, conditioned on $c$ $\to$ Clean Latent $$z_0$$.
- **Stage 3**: Clean Latent $z_0$ $\to$ VAE Decoder $\to$ Final Image $x$.

---

# <a id="section6">References</a>

[^unet]: Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation[C]//International Conference on Medical image computing and computer-assisted intervention. Cham: Springer international publishing, 2015: 234-241.

[^adm]: Dhariwal P, Nichol A. Diffusion models beat gans on image synthesis[J]. Advances in neural information processing systems, 2021, 34: 8780-8794.

[^biggan]: Brock A, Donahue J, Simonyan K. Large scale GAN training for high fidelity natural image synthesis[J]. arXiv preprint arXiv:1809.11096, 2018.

[^film]: Perez E, Strub F, De Vries H, et al. Film: Visual reasoning with a general conditioning layer[C]//Proceedings of the AAAI conference on artificial intelligence. 2018, 32(1).


[^sd]: Rombach R, Blattmann A, Lorenz D, et al. High-resolution image synthesis with latent diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 10684-10695.

[^sdxl]: Podell D, English Z, Lacey K, et al. Sdxl: Improving latent diffusion models for high-resolution image synthesis[J]. arXiv preprint arXiv:2307.01952, 2023.

[^iedm]: Karras T, Aittala M, Lehtinen J, et al. Analyzing and improving the training dynamics of diffusion models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 24174-24184.

[^edm]: Karras T, Aittala M, Aila T, et al. Elucidating the design space of diffusion-based generative models[J]. Advances in neural information processing systems, 2022, 35: 26565-26577.

[^Kingma]: Kingma D, Gao R. Understanding diffusion objectives as the elbo with simple data augmentation[J]. Advances in Neural Information Processing Systems, 2023, 36: 65484-65516.

[^Salimans]: Salimans T, Ho J. Progressive distillation for fast sampling of diffusion models[J]. arXiv preprint arXiv:2202.00512, 2022.

[^p2]: Choi J, Lee J, Shin C, et al. Perception prioritized training of diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 11472-11481.

[^min_snr]: Hang T, Gu S, Li C, et al. Efficient diffusion training via min-snr weighting strategy[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2023: 7441-7451.

[^max_snr]: Salimans T, Ho J. Progressive distillation for fast sampling of diffusion models[J]. arXiv preprint arXiv:2202.00512, 2022.

[^snr_based]: Kingma D, Gao R. Understanding diffusion objectives as the elbo with simple data augmentation[J]. Advances in Neural Information Processing Systems, 2023, 36: 65484-65516.

[^ddpm]: Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[J]. Advances in neural information processing systems, 2020, 33: 6840-6851.

[^iddpm]: Nichol A Q, Dhariwal P. Improved denoising diffusion probabilistic models[C]//International conference on machine learning. PMLR, 2021: 8162-8171.

[^ZTSNR]: Lin S, Liu B, Li J, et al. Common diffusion noise schedules and sample steps are flawed[C]//Proceedings of the IEEE/CVF winter conference on applications of computer vision. 2024: 5404-5411.