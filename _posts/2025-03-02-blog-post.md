---
title: 'Stabilizing Diffusion Training: The Evolution of Network Architectures'
excerpt: "This article explores how architectural choices â€” from classical U-Nets to ADM refinements, latent diffusion, and the latest Transformer-based designs â€” fundamentally shape training stability. We examine normalization and conditioning mechanisms, residual and skip pathway innovations, and integrative paradigms such as EDM that unify architecture with preconditioning. Along the way, we highlight practical stability enhancements and emerging trends that point toward the next generation of robust diffusion architectures."
date: 2025-03-02
permalink: /posts/2025/03/diffusion-training-2/
tags:
  - Diffusion Model
  - UNET
  - Transformer
  - DiT
---

<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>ðŸ“š Table of Contents</strong></span>
  </summary>
  <ul style="margin:0; padding-left:1.1rem;">
	<li><a href="#section1">1. Why Architecture Matters for Stability</a>
		<ul>
			<li><a href="#section1.1">1.1 Gradient Flow, Conditioning, and Stability</a></li>
			<li><a href="#section1.2">1.2 Balancing Capacity vs. Robustness</a></li>
			<li><a href="#section1.3">1.3 Architectureâ€“Noise Schedule Coupling</a></li>
		</ul>
	</li>
	<li><a href="#section2">2. Evolution of Diffusion Architectures</a>
		<ul>
			<li><a href="#section2.1">2.1 Classical U-Net Foundations</a></li>
			<li><a href="#section2.2">2.2 ADM Improvements (Attention, Class Conditioning)</a></li>
			<li><a href="#section2.3">2.3 Latent U-Net (Stable Diffusion, SDXL)</a></li>
			<li><a href="#section2.4">2.4 Transformer-based Designs (DiT, MMDiT-X, Hybrid Models)</a></li>
			<li><a href="#section2.5">2.5 Extensions to Video and 3D Diffusion (Video U-Net, Gaussian Splatting)</a></li>
			<li><a href="#section2.6">2.6 Lightweight & Memory-efficient Designs (MobileDiffusion, LightDiffusion)</a></li>
		</ul>
	</li>	
	<li><a href="#section3">3. Conditioning, Normalization & Tokenization</a>
		<ul>
			<li><a href="#section3.1">3.1 Normalization Evolution: GroupNorm â†’ AdaLN-Zero</a></li>
			<li><a href="#section3.2">3.2 Time / Ïƒ Embedding Strategies (Sinusoidal, Fourier, FiLM)</a></li>
			<li><a href="#section3.3">3.3 Query-Key Normalization (e.g., SD3.5, MMDiT-X)</a></li>
			<li><a href="#section3.4">3.4 Multi-modal Token Conditioning (Cross-Attention, LoRA, Deep Fusion)</a></li>
			<li><a href="#section3.4">3.5 Token Compression and Merging for Efficiency & Stability</a></li>
		</ul>
	</li>
	<li><a href="#section4">4. Residual & Skip Pathway Designs</a>
		<ul>
			<li><a href="#section4.1">4.1 Classic Skip Connections in U-Net</a></li>
			<li><a href="#section4.2">4.2 Residual-in-Residual Architectures</a></li>
			<li><a href="#section4.3">4.3 Sparse Skip & Hourglass Designs (DiC)</a></li>
			<li><a href="#section4.4">4.4 Dynamic and Gated Skip Pathways</a></li>
			<li><a href="#section4.5">4.5 Progressive Growing & Skip Fading Strategies</a></li>
		</ul>
	</li>
	<li><a href="#section5">5. Transformer Architectures & Architectural Variants</a>
		<ul>
			<li><a href="#section5.1">5.1 Diffusion Transformers (DiT) and Scaling Laws</a></li>
			<li><a href="#section5.2">5.2 Decoupled Design (DDT: Encoderâ€“Decoder Separation)</a></li>
			<li><a href="#section5.3">5.3 Automated Architecture Search (DiffusionNAG)</a></li>
			<li><a href="#section5.4">5.4 Multi-resolution Networks with Time-dependent Norms</a></li>
			<li><a href="#section5.5">5.5 State Space Models (S4, Mamba) as Alternatives to Transformers</a></li>
			<li><a href="#section5.6">5.6 Parallel vs. Sequential Transformer Architectures</a></li>
		</ul>
	</li>
	<li><a href="#section6">6. Improved EDM as an Integrative Design Paradigm</a>
		<ul>
			<li><a href="#section6.1">6.1 Preconditioning with AdaLN-Zero</a></li>
			<li><a href="#section6.2">6.2 Hybridization of Architectural and Regularization Strategies</a></li>
			<li><a href="#section6.3">6.3 Structurally Balanced Design for Training Stability</a></li>
			<li><a href="#section6.4">6.4 Architectureâ€“Noise Schedule Co-design</a></li>
		</ul>
	</li>
	<li><a href="#section7">7. Practical Stability Enhancements</a>
		<ul>
			<li><a href="#section7.1">7.1 Weight Initialization and Parameter Scaling</a></li>
			<li><a href="#section7.2">7.2 Gradient Clipping and EMA for Stable Convergence</a></li>
			<li><a href="#section7.3">7.3 Mixed Precision Training and Stability Trade-offs</a></li>
			<li><a href="#section7.4">7.4 Parameter-efficient Modules: LoRA, DoRA, T-Fixup</a></li>
			<li><a href="#section7.5">7.5 Training-free Stability Tricks (Noise Rescaling, Variance Matching)</a></li>
		</ul>
	</li>
	<li><a href="#section8">8. References</a></li>
  </ul>
</details>


When discussing the stability of diffusion model training, much of the focus often falls on noise schedules, loss weighting strategies, or optimization tricks (Please refer [our post](https://innovation-cat.github.io/posts/2025/01/diffusion-model-2/)). While these aspects are undeniably important, an equally critical â€” yet sometimes underemphasized â€” factor is the choice of network architecture itself. The structure of the model fundamentally determines how signals, gradients, and conditioning information propagate across different noise levels, and whether the training process converges smoothly or collapses into instability.

---

# <a id="section1">1. Why Architecture Matters for Stability</a>

Network architecture is more than a vessel for function approximation in diffusion models â€” it is the key component that determines whether training succeeds or fails.


## <a id="section1.1">1.1 Gradient Flow, Conditioning, and Stability</a>

Diffusion models are trained under extreme conditions: inputs span a spectrum from nearly clean signals to pure Gaussian noise. This makes them particularly sensitive to how gradients are normalized, how residuals accumulate, and how skip connections or attention layers interact with noisy features.

- **Improper gradient flow** can cause exploding updates at low-noise regimes or vanishing signals at high-noise regimes.
- **Conditioning pathways** (e.g., cross-attention for text or multimodal prompts) introduce additional sensitivity, as misaligned normalization or unbalanced skip pathways can destabilize learning.

Architectural innovations such as **GroupNorm, AdaLN-Zero, and preconditioning layers** have been specifically introduced to address these gradient stability issues, ensuring that the network remains trainable across a wide dynamic range of noise.

---

## <a id="section1.2">1.2 Balancing Capacity vs. Robustness</a>

A second challenge lies in the tension between **capacity** (the ability of the architecture to represent complex distributions) and **robustness** (the ability to generalize under noisy, unstable conditions).

- Early **U-Net designs** offered robustness through simplicity and skip connections, but limited capacity for scaling.
- **Transformer-based diffusion models (DiT, MMDiT-X)** introduced massive representational power, but at the cost of more fragile training dynamics.
- Newer architectures explore hybrid or modular designs â€” combining convolutional inductive biases, residual pathways, and attention â€” to find a stable equilibrium between these two competing goals.


## <a id="section1.3">1.3 Architectureâ€“Noise Schedule Coupling</a>

Finally, the stability of diffusion training cannot be isolated from the **noise schedule**. Architectural design interacts tightly with how noise levels are distributed and parameterized:

- A model with **time-dependent normalization layers** may remain stable under variance-preserving schedules but collapse under variance-exploding ones.
- EDM (Elucidated Diffusion Models) highlight that **architecture and preconditioning must be co-designed** with the training noise distribution, rather than treated as independent modules.

This coupling implies that progress in diffusion training stability comes not only from better solvers or schedules, but from **holistic architectural design** that accounts for gradient dynamics, representation capacity, and their interplay with noise parameterization.


---


# <a id="section6">References</a>

[^iedm]: Karras T, Aittala M, Lehtinen J, et al. Analyzing and improving the training dynamics of diffusion models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 24174-24184.

[^edm]: Karras T, Aittala M, Aila T, et al. Elucidating the design space of diffusion-based generative models[J]. Advances in neural information processing systems, 2022, 35: 26565-26577.

[^Kingma]: Kingma D, Gao R. Understanding diffusion objectives as the elbo with simple data augmentation[J]. Advances in Neural Information Processing Systems, 2023, 36: 65484-65516.

[^Salimans]: Salimans T, Ho J. Progressive distillation for fast sampling of diffusion models[J]. arXiv preprint arXiv:2202.00512, 2022.

[^p2]: Choi J, Lee J, Shin C, et al. Perception prioritized training of diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 11472-11481.

[^min_snr]: Hang T, Gu S, Li C, et al. Efficient diffusion training via min-snr weighting strategy[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2023: 7441-7451.

[^max_snr]: Salimans T, Ho J. Progressive distillation for fast sampling of diffusion models[J]. arXiv preprint arXiv:2202.00512, 2022.

[^snr_based]: Kingma D, Gao R. Understanding diffusion objectives as the elbo with simple data augmentation[J]. Advances in Neural Information Processing Systems, 2023, 36: 65484-65516.

[^ddpm]: Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[J]. Advances in neural information processing systems, 2020, 33: 6840-6851.

[^iddpm]: Nichol A Q, Dhariwal P. Improved denoising diffusion probabilistic models[C]//International conference on machine learning. PMLR, 2021: 8162-8171.

[^ZTSNR]: Lin S, Liu B, Li J, et al. Common diffusion noise schedules and sample steps are flawed[C]//Proceedings of the IEEE/CVF winter conference on applications of computer vision. 2024: 5404-5411.