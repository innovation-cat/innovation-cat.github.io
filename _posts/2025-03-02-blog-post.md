---
title: 'Stabilizing Diffusion Training: The Evolution of Network Architectures'
excerpt: "This article explores how architectural choices — from classical U-Nets to ADM refinements, latent diffusion, and the latest Transformer-based designs — fundamentally shape training stability. We examine normalization and conditioning mechanisms, residual and skip pathway innovations, and integrative paradigms such as EDM that unify architecture with preconditioning. Along the way, we highlight practical stability enhancements and emerging trends that point toward the next generation of robust diffusion architectures."
date: 2025-03-02
permalink: /posts/2025/03/diffusion-training-2/
tags:
  - Diffusion Model
  - UNET
  - Transformer
  - DiT
---

<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>📚 Table of Contents</strong></span>
  </summary>
  <ul style="margin:0; padding-left:1.1rem;">
	<li><a href="#section1">1. Why Architecture Matters for Stability</a>
		<ul>
			<li><a href="#section1.1">1.1 Gradient Flow, Conditioning, and Stability</a></li>
			<li><a href="#section1.2">1.2 Balancing Capacity vs. Robustness</a></li>
			<li><a href="#section1.3">1.3 Architecture–Noise Schedule Coupling</a></li>
		</ul>
	</li>
	<li><a href="#section2">2. Evolution of Diffusion Architectures</a>
		<ul>
			<li><a href="#section2.1">2.1 Classical U-Net Foundations</a></li>
			<li><a href="#section2.2">2.2 ADM Improvements (Attention, Class Conditioning)</a></li>
			<li><a href="#section2.3">2.3 Latent U-Net (Stable Diffusion, SDXL)</a></li>
			<li><a href="#section2.4">2.4 Transformer-based Designs (DiT, MMDiT-X, Hybrid Models)</a></li>
			<li><a href="#section2.5">2.5 Extensions to Video and 3D Diffusion (Video U-Net, Gaussian Splatting)</a></li>
			<li><a href="#section2.6">2.6 Lightweight & Memory-efficient Designs (MobileDiffusion, LightDiffusion)</a></li>
		</ul>
	</li>	
	<li><a href="#section3">3. Conditioning, Normalization & Tokenization</a>
		<ul>
			<li><a href="#section3.1">3.1 Normalization Evolution: GroupNorm → AdaLN-Zero</a></li>
			<li><a href="#section3.2">3.2 Time / σ Embedding Strategies (Sinusoidal, Fourier, FiLM)</a></li>
			<li><a href="#section3.3">3.3 Query-Key Normalization (e.g., SD3.5, MMDiT-X)</a></li>
			<li><a href="#section3.4">3.4 Multi-modal Token Conditioning (Cross-Attention, LoRA, Deep Fusion)</a></li>
			<li><a href="#section3.4">3.5 Token Compression and Merging for Efficiency & Stability</a></li>
		</ul>
	</li>
	<li><a href="#section4">4. Residual & Skip Pathway Designs</a>
		<ul>
			<li><a href="#section4.1">4.1 Classic Skip Connections in U-Net</a></li>
			<li><a href="#section4.2">4.2 Residual-in-Residual Architectures</a></li>
			<li><a href="#section4.3">4.3 Sparse Skip & Hourglass Designs (DiC)</a></li>
			<li><a href="#section4.4">4.4 Dynamic and Gated Skip Pathways</a></li>
			<li><a href="#section4.5">4.5 Progressive Growing & Skip Fading Strategies</a></li>
		</ul>
	</li>
	<li><a href="#section5">5. Transformer Architectures & Architectural Variants</a>
		<ul>
			<li><a href="#section5.1">5.1 Diffusion Transformers (DiT) and Scaling Laws</a></li>
			<li><a href="#section5.2">5.2 Decoupled Design (DDT: Encoder–Decoder Separation)</a></li>
			<li><a href="#section5.3">5.3 Automated Architecture Search (DiffusionNAG)</a></li>
			<li><a href="#section5.4">5.4 Multi-resolution Networks with Time-dependent Norms</a></li>
			<li><a href="#section5.5">5.5 State Space Models (S4, Mamba) as Alternatives to Transformers</a></li>
			<li><a href="#section5.6">5.6 Parallel vs. Sequential Transformer Architectures</a></li>
		</ul>
	</li>
	<li><a href="#section6">6. Improved EDM as an Integrative Design Paradigm</a>
		<ul>
			<li><a href="#section6.1">6.1 Preconditioning with AdaLN-Zero</a></li>
			<li><a href="#section6.2">6.2 Hybridization of Architectural and Regularization Strategies</a></li>
			<li><a href="#section6.3">6.3 Structurally Balanced Design for Training Stability</a></li>
			<li><a href="#section6.4">6.4 Architecture–Noise Schedule Co-design</a></li>
		</ul>
	</li>
	<li><a href="#section7">7. Practical Stability Enhancements</a>
		<ul>
			<li><a href="#section7.1">7.1 Weight Initialization and Parameter Scaling</a></li>
			<li><a href="#section7.2">7.2 Gradient Clipping and EMA for Stable Convergence</a></li>
			<li><a href="#section7.3">7.3 Mixed Precision Training and Stability Trade-offs</a></li>
			<li><a href="#section7.4">7.4 Parameter-efficient Modules: LoRA, DoRA, T-Fixup</a></li>
			<li><a href="#section7.5">7.5 Training-free Stability Tricks (Noise Rescaling, Variance Matching)</a></li>
		</ul>
	</li>
	<li><a href="#section8">8. Conclusion</a></li>
	<li><a href="#section9">9. References</a></li>
  </ul>
</details>


When discussing the stability of diffusion model training, much of the focus often falls on noise schedules, loss weighting strategies, or optimization tricks (Please refer [our post](https://innovation-cat.github.io/posts/2025/01/diffusion-model-2/)). While these aspects are undeniably important, an equally critical — yet sometimes underemphasized — factor is the choice of network architecture itself. The structure of the model fundamentally determines how signals, gradients, and conditioning information propagate across different noise levels, and whether the training process converges smoothly or collapses into instability.

---

# <a id="section1">1. Why Architecture Matters for Stability</a>

Network architecture is more than a vessel for function approximation in diffusion models — it is the key component that determines whether training succeeds or fails.


## <a id="section1.1">1.1 Gradient Flow, Conditioning, and Stability</a>

Diffusion models are trained under extreme conditions: inputs span a spectrum from nearly clean signals to pure Gaussian noise. This makes them particularly sensitive to how gradients are normalized, how residuals accumulate, and how skip connections or attention layers interact with noisy features.

- **Improper gradient flow** can cause exploding updates at low-noise regimes or vanishing signals at high-noise regimes.
- **Conditioning pathways** (e.g., cross-attention for text or multimodal prompts) introduce additional sensitivity, as misaligned normalization or unbalanced skip pathways can destabilize learning.

Architectural innovations such as **GroupNorm, AdaLN-Zero, and preconditioning layers** have been specifically introduced to address these gradient stability issues, ensuring that the network remains trainable across a wide dynamic range of noise.

---

## <a id="section1.2">1.2 Balancing Capacity vs. Robustness</a>

A second challenge lies in the tension between **capacity** (the ability of the architecture to represent complex distributions) and **robustness** (the ability to generalize under noisy, unstable conditions).

- Early **U-Net designs** offered robustness through simplicity and skip connections, but limited capacity for scaling.
- **Transformer-based diffusion models (DiT, MMDiT-X)** introduced massive representational power, but at the cost of more fragile training dynamics.
- Newer architectures explore hybrid or modular designs — combining convolutional inductive biases, residual pathways, and attention — to find a stable equilibrium between these two competing goals.


## <a id="section1.3">1.3 Architecture–Noise Schedule Coupling</a>

Finally, the stability of diffusion training cannot be isolated from the **noise schedule**. Architectural design interacts tightly with how noise levels are distributed and parameterized:

- A model with **time-dependent normalization layers** may remain stable under variance-preserving schedules but collapse under variance-exploding ones.
- EDM (Elucidated Diffusion Models) highlight that **architecture and preconditioning must be co-designed** with the training noise distribution, rather than treated as independent modules.

This coupling implies that progress in diffusion training stability comes not only from better solvers or schedules, but from **holistic architectural design** that accounts for gradient dynamics, representation capacity, and their interplay with noise parameterization.


---

# <a id="section2">2. Evolution of Diffusion Architectures</a>

The architectural journey of diffusion models mirrors the evolution of deep learning itself: from simple convolutional backbones to large-scale Transformers, and now toward specialized multi-modal and efficiency-driven designs. Each stage has sought to reconcile two opposing pressures — **increasing representational power** while **preserving training stability**. In this section, we trace this trajectory across six key phases.

---

## <a id="section2.1">2.1 Classical U-Net Foundations</a>

The **U-Net architecture** [^unet] is the canonical backbone of early diffusion models such as DDPM [^ddpm]. Although originally proposed for biomedical image segmentation [^unet] , its **encoder–decoder structure with skip connections** turned out to be perfectly suited for denoising across different noise levels. The elegance of U-Net lies not only in its symmetry, but also in how it balances **global context extraction** with **local detail preservation**. A typical unet structure applied to the training of diffusion models is as follows

![U-Net architecture](/images/posts/2025-03-02-blog-post/unet.jpg)

where **ResB** represents residual block, **Attent** represents self-Attention block. 

### Encoder: From Local Features to Global Context

The **encoder path** consists of repeated **convolutional residual blocks** and **downsampling operations** (e.g., strided convolutions or pooling). As the spatial resolution decreases and channel width expands, the network progressively shifts its representational emphasis:

- **High-resolution feature maps (early layers)** capture **fine-grained local structures** — edges, textures, and small patterns that are critical when denoising images at low noise levels.
- **Low-resolution feature maps (deeper layers)** aggregate **global context** — object shapes, spatial layout, and long-range dependencies. This is especially important at high noise levels, when much of the local structure has been destroyed and only global semantic cues can guide reconstruction.

Thus, the encoder effectively builds a **multi-scale hierarchy** of representations, transitioning from local to global as resolution decreases.

### Bottleneck: Abstract Representation

At the center lies the **bottleneck block**, where feature maps have the smallest spatial size but the largest channel capacity. This stage acts as the **semantic aggregator**:

- It condenses the global context extracted from the encoder.
- It often includes **attention layers** (in later refinements) to explicitly model long-range interactions.
  In the classical U-Net used by DDPM, the bottleneck is still purely convolutional, yet it already plays the role of a semantic “bridge” between encoding and decoding.

### Decoder: Reconstructing Local Detail under Global Guidance

The **decoder path** mirrors the encoder, consisting of **upsampling operations** followed by convolutional residual blocks. The role of the decoder is not merely to increase resolution, but to **inject global semantic context back into high-resolution predictions**:

- **Upsampling layers** expand the spatial resolution but initially lack fine detail.
- **Skip connections** from the encoder reintroduce high-frequency local features (edges, boundaries, textures) that would otherwise be lost in downsampling.
- By concatenating or adding these skip features to the decoder inputs, the network **fuses global context (from low-res encoder features)** with **local precision (from high-res encoder features)**.

This synergy ensures that the denoised outputs are both **semantically coherent** and **visually sharp**.



---

### Timestep Embedding and Conditioning

Unlike the U-Net’s original role in segmentation, a diffusion U-Net must also be conditioned on the **diffusion timestep** $t$, since the network’s task changes continuously as noise levels vary. In the classical DDPM implementation, this conditioning is realized in a relatively simple but effective way:

1. **Sinusoidal embedding.**
   Each integer timestep $t$ is mapped to a high-dimensional vector using sinusoidal position encodings (analogous to Transformers), ensuring that different timesteps are represented as distinct, smoothly varying signals.

2. **MLP transformation.**
   The sinusoidal embedding is passed through a small multilayer perceptron (usually two linear layers with a SiLU activation) to produce a richer time embedding vector $\mathbf{z}_t$.

3. **Additive injection into residual blocks.**
   In every residual block of the U-Net, $\mathbf{z}_t$ is projected to match the number of feature channels and then **added as a bias term** to the intermediate activations (typically after the first convolution).

This additive conditioning allows **each residual block** to adapt its computation based on the current noise level, without introducing extra normalization or complex modulation. The following figure shows the way how to inject timestep $t$ in each residual block.



![time embedding injection](/images/posts/2025-03-02-blog-post/time_embedding.jpg)

---



### Why U-Net Works Well for Diffusion

In diffusion training, inputs vary drastically in signal-to-noise ratio:

- At **low noise levels**, local details still survive; skip connections ensure these details propagate to the output.
- At **high noise levels**, local detail is destroyed; the decoder relies more on global semantics from the bottleneck.
- Across all levels, the encoder–decoder interaction guarantees that both **local fidelity** and **global plausibility** are preserved.

This explains why U-Nets became the **default backbone**: their **multi-scale design matches perfectly with the multi-scale nature of noise in diffusion models**. Later improvements (attention layers, latent-space U-Nets, Transformer backbones) all build upon this foundation, but the core idea remains: **stability in diffusion training emerges from balanced local–global feature fusion.**

---

## <a id="section2.2">2.2 ADM </a>

---


# <a id="section6">References</a>

[^unet]: Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation[C]//International Conference on Medical image computing and computer-assisted intervention. Cham: Springer international publishing, 2015: 234-241.

[^adm]: Dhariwal P, Nichol A. Diffusion models beat gans on image synthesis[J]. Advances in neural information processing systems, 2021, 34: 8780-8794.

[^iedm]: Karras T, Aittala M, Lehtinen J, et al. Analyzing and improving the training dynamics of diffusion models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 24174-24184.

[^edm]: Karras T, Aittala M, Aila T, et al. Elucidating the design space of diffusion-based generative models[J]. Advances in neural information processing systems, 2022, 35: 26565-26577.

[^Kingma]: Kingma D, Gao R. Understanding diffusion objectives as the elbo with simple data augmentation[J]. Advances in Neural Information Processing Systems, 2023, 36: 65484-65516.

[^Salimans]: Salimans T, Ho J. Progressive distillation for fast sampling of diffusion models[J]. arXiv preprint arXiv:2202.00512, 2022.

[^p2]: Choi J, Lee J, Shin C, et al. Perception prioritized training of diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 11472-11481.

[^min_snr]: Hang T, Gu S, Li C, et al. Efficient diffusion training via min-snr weighting strategy[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2023: 7441-7451.

[^max_snr]: Salimans T, Ho J. Progressive distillation for fast sampling of diffusion models[J]. arXiv preprint arXiv:2202.00512, 2022.

[^snr_based]: Kingma D, Gao R. Understanding diffusion objectives as the elbo with simple data augmentation[J]. Advances in Neural Information Processing Systems, 2023, 36: 65484-65516.

[^ddpm]: Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[J]. Advances in neural information processing systems, 2020, 33: 6840-6851.

[^iddpm]: Nichol A Q, Dhariwal P. Improved denoising diffusion probabilistic models[C]//International conference on machine learning. PMLR, 2021: 8162-8171.

[^ZTSNR]: Lin S, Liu B, Li J, et al. Common diffusion noise schedules and sample steps are flawed[C]//Proceedings of the IEEE/CVF winter conference on applications of computer vision. 2024: 5404-5411.