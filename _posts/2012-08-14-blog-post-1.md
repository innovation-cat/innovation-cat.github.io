---
title: 'Diffusion Model'
date: 2025-02-14
permalink: /posts/2025/02/diffusion-model-1/
tags:
  - diffusion model
  - AIGC
  - SDE
  - ODE
---

Diffusion models have been shown to be a highly promising approach in the field of image generation. They treat image generation as two independent processes: the forward process, which transforms a complex data distribution into a known prior distribution (typically a standard normal distribution) by gradually injecting noise; and the reverse process, which transforms the prior distribution back into the complex data distribution by gradually removing the noise.

---

## The unified forward process --- from a discrete perspective

In most cases, the forward process, also known as noise schedules, does not contain any learnable parameters, you only need to "manually" define it in advance. We assume that the data distribution is $p_{data}$, while the prior distribution is $p_{init}$. For any time step $t$, the noised image $x_t$ can be obtained by adding noise $\varepsilon$ ( $\varepsilon \sim p_{init}$)   to a real image $x_0$ ( $x_0 \sim p_{data}$). We can formalize it as the following formula:

$$
x_t=s(t)*x_0+\sigma(t)*\varepsilon
$$

Where $s(t)$ represents the signal coefficient, and $\sigma(t)$ represents the noise coefficient. The two mainstream types of noise schedules are **Variance Preserving (VP)** and **Variance Exploding (VE)**.

**VPï¼š** At any time step $t$, the variance of the noise image $x_t$ is equal to a constant (usually 1), which means that: $Var(x_t)=s(t)^2+\sigma(t)^2=1$. We can refer the denoising formula of DDPM to rewrite the VP-type noise scheduling:

$$
x_t=\sqrt {\bar {\alpha_t}}*x_0+ \sqrt {1-\bar {\alpha_t}} *\varepsilon
$$

Where $s(t) = \sqrt {\bar {\alpha_t}}$ , $\sigma(t) =  \sqrt {1-\bar {\alpha_t}} $. Two commonly used VP-type noise scheduling include linear scheduling (DDPM[^ddpm] and DDIM[^ddim]) and cosine scheduling (iDDPM[^iddpm])

### References

[^ddpm]: Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[J]. Advances in neural information processing systems, 2020, 33: 6840-6851.

[^ddim]: Song J, Meng C, Ermon S. Denoising diffusion implicit models[J]. arXiv preprint arXiv:2010.02502, 2020.

[^iddpm]: Nichol A Q, Dhariwal P. Improved denoising diffusion probabilistic models[C]//International conference on machine learning. PMLR, 2021: 8162-8171.

------