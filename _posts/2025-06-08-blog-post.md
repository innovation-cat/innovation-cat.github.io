---
title: 'From Diffusion to Flow — Seeking the Elegant Path'
date: 2025-06-08
excerpt: "In this post, we uncovered the foundations of Flow Matching: the limitations of diffusion models, the constraints of continuous flows, and the transformative idea of directly learning the path between distributions. From the intuition of Rectified Flow to the unifying lens of Stochastic Interpolants, Flow Matching emerged as more than a method — it is a paradigm that reframes generation as learning currents of transformation. With this conceptual map in hand, we are now ready to move from theory to practice."
permalink: /posts/2025/06/flow-matching-1/
tags:
  - Generative Models
  - Discussion Models
  - Normalizing Flows
  - Flow Matching
  - Rectified Flow
  - Conditional Flow Matching
  - Stochastic Interpolants
---


<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>📚 Table of Contents</strong></span>
  </summary>
  <ul>
	<li><a href="#section1">1. Introduction</a>
		<ul>
			<li><a href="#section1.1">1.1 The Rise and Bottlenecks of Diffusion Models</a></li>
			<li><a href="#section1.2">1.2 The Elegance and Constraints of Continuous Normalizing Flows</a></li>
			<li><a href="#section1.3">1.3 The Breaking Point: Toward Direct Paths Between Distributions</a></li>
		</ul>
	</li>
	<li><a href="#section2">2. Preliminaries</a>
		<ul>
			<li><a href="#section2.1">2.1 Vector Field, Trajectory and Flow</a></li>
			<li><a href="#section2.2">2.2 Probability path</a></li>
		</ul>
	</li>
	<li><a href="#section3">3. Origins of the Idea: From Rectified Flow</a></li>
	<li><a href="#section4">4. Establishing the Flow Matching Paradigm</a>
		<ul>
			<li><a href="#section4.1">4.1 From RF to FM: a general probabilistic path</a></li>
			<li><a href="#section4.2">4.2 Core objective: vector-field regression</a></li>
			<li><a href="#section4.3">4.3 Consistency with the Continuity Equation</a></li>
			<li><a href="#section4.4">4.4 Translating Perspectives: connections to CNF and PF-ODE</a></li>
		</ul>
	</li>
	<li><a href="#section5">5. Extension: Conditional Flow Matching</a></li>
	<li><a href="#section6">6. A Unified Narrative: Stochastic Interpolants</a>
		<ul>
			<li><a href="#section6.1">6.1 From Deterministic to Stochastic Interpolations</a></li>
			<li><a href="#section6.2">6.2 Diffusion, RF, and FM as Mutual Special Cases</a></li>
		</ul>
	</li>
	<li><a href="#section7">7. Conclusion & Bridge</a></li>
	<li><a href="#section8">8. References</a></li>
  </ul>
</details>

This series, **the Flow Matching Trilogy**, is a journey through the past, present, and future of flow-based generative models. 

- [Part 1](https://innovation-cat.github.io/posts/2025/06/flow-matching-1/): introduces the theoretical foundations of Flow Matching, tracing its emergence from diffusion processes and normalizing flows toward a unified framework. 

- [Part 2](https://innovation-cat.github.io/posts/2025/07/flow-matching-2/): turns theory into practice, offering a systematic engineering guide on how to design, control, and accelerate Flow Matching systems. 

- [Part 3](https://innovation-cat.github.io/posts/2025/08/flow-matching-3/): looks outward, exploring the frontiers of research — from discrete and structured data to large-scale and multimodal models.

In the first article, we establishes the foundations of Flow Matching. It reviews the bottlenecks of diffusion models, the constraints of normalizing flows, and introduces the key idea of directly learning transformation paths between distributions. From Rectified Flow to Conditional Flow Matching and Stochastic Interpolants, it outlines how a simple intuition matured into a unified generative framework, paving the way for the practical design discussions in Part 2.

---

# <a id="section1">1. Introduction: Seeking the Third Road of Generation</a>

Before we arrive at Flow Matching itself, it is essential to understand the two dominant paradigms that shaped the landscape of modern generative modeling: diffusion models and normalizing flows. Each offers a distinct philosophy — diffusion relies on stochastic forward processes and iterative denoising, while flows construct invertible mappings grounded in probability theory. Both reveal valuable insights but also expose fundamental limitations. By examining their strengths and bottlenecks, we prepare the ground for a third road - Flow Matching.

## <a id="section1.1">1.1 The Rise and Bottlenecks of Diffusion Models</a>

Diffusion models have defined the state of the art in generative modeling. By gradually adding noise through a fixed forward process and then learning to reverse it, they can approximate extremely complex data distributions with remarkable fidelity. We have discussed many training and sampling techniques for diffusion models in the previous posts.

However, their reliance on **iterative denoising** is costly. Sampling often requires hundreds of steps, each invoking a deep network, which makes inference slow and error-prone. The method is powerful but **simulation-heavy**, as both training (via noisy forward diffusion) and inference (via numerical solvers) depend on repeatedly simulating stochastic dynamics.

---

## <a id="section1.2">1.2 The Elegance and Constraints of Continuous Normalizing Flows</a>


At the heart of Normalizing Flows (NFs) [^Nice] [^nvp] [^Glow] lies a simple but powerful principle: the **change-of-variables** formula. It connects the density of a data point $z$ in one space to the density of its transformed representation $x$ in another .

![Conditional probability path and Marginal probability path](/images/posts/2025-06-08-blog-post/1.jpg)

Suppose we have a random variable $z \sim p(z)$, where $p_z$ is a simple Gaussian distribution, we then construct a chain of $K$ invertible mappings:

$$
z_0 \sim p_z(z_0), \quad z_1 = f_1(z_0), \quad \dots, \quad x = z_K = f_K \circ \cdots \circ f_1(z_0).
$$

By repeatedly applying the change-of-variables formula, we get the final variable $x \sim p(x)$, the density of $x$ is given by

$$
\log p(x) = \log p(z_0) - \sum_{k=1}^K \log \left|\det J_{f_i}\right|.
$$

Where $J_{f_i}$ is Jacobian Matrix. Thus, the entire model is **exact likelihoods mathematically**, as long as each transformation is invertible and its Jacobian determinant is tractable. However, the computation the Jacobian determinant of $f_i$ is very expensive in high dimensions. 

To make Jacobian computation feasible, invertible mappings should be designed for special architecture to guarantee a tractable Jacobian determinant. For example, Nice [^Nice] using additive coupling layer to make Jacobian determinant is equal to 1.

![NICE](/images/posts/2025-06-08-blog-post/2.jpg)

Real NVP [^nvp] extends NICE’s additive coupling to an affine coupling layer, which guarantees that the Jacobian determinant is just the product of elementwise scales (a diagonal matrix). This makes log-likelihood tractable while keeping the model invertible and expressive.

![Real_NVP](/images/posts/2025-06-08-blog-post/3.jpg)

The move from **discrete transformations** to **continuous-time transformations** gave rise to **Continuous Normalizing Flows (CNFs)** [^Neural_ODE] [^Ffjord]. Instead of stacking finitely many mappings, we let the transformation evolve under an ODE:

$$
\frac{dz}{dt} = v_\theta(z, t), \quad z(0) \sim p_z, \quad z(1) \sim p_x.
$$

The corresponding density evolution is given by the **instantaneous change-of-variables formula** (Chen et al., 2018):

$$
\frac{d}{dt} \log p(z(t)) = - \text{Tr}\!\left(\frac{\partial v_\theta(z(t),t)}{\partial z(t)}\right).
$$

In theory, CNFs remove the architectural rigidity of discrete NFs, allowing any sufficiently expressive neural network to parameterize the vector field $v_\theta$. They provide a mathematically elegant and continuous-time perspective on generative modeling, bridging flows and differential equations.

However, the need to compute **Jacobian traces** is computationally expensive, and CNFs often demand architectures designed specifically to trade off expressivity and tractability. Training and sampling can be slow, and scaling to high-dimensional data remains difficult.

---

## <a id="section1.3">1.3 The Breaking Point: Toward Direct Paths Between Distributions</a>

Between diffusion’s slow stochastic wanderings and CNF’s constrained elegance lies a natural question: *must generative modeling always depend on heavy simulation or exact density tracking?*

Imagine instead that we could **directly learn the transformation path** between two distributions — for example, between noise and data. Rather than simulating a process step by step, or computing exact likelihoods, we could regress a **vector field** that prescribes how samples should flow from one distribution to another. This reframes generative modeling as the study of **currents of transformation**, where the emphasis shifts from *simulation* to *direct learning*.

This insight marks the turning point for **Flow Matching**. It signals the possibility of a third road: a paradigm that is at once intuitive, efficient, and theoretically unifying.

---

# <a id="section2">2. Preliminaries</a>

In this section, we first summarize the key terms and terminology needed for learning flow matching.

## <a id="section2.1">2.1 Vector Field, Trajectory and Flow</a>

**Vector Field**: vector field is a function that assigns to each spatial point $x_t \in \mathbb R^d$ and time $t \in [0, 1]$ an instantaneous velocity $v_{\theta}(t, x_t)$:

$$
v_{\theta}:\ \mathbb R^d \times [0,1] \to \mathbb R^d\label{eq:1}
$$

**ODE**: ODE (Ordinary Differential Equation) is the dynamical law you impose using that vector field:

$$
\frac{dx_t}{dt}=v_{\theta}(t, x_t)\label{eq:2}
$$

Solving this ODE from $t=0$ to $t=1$ is equivalent to sampling, whose goal is to transport an initial point $x_0$ to a target $x_1$ through space according to the learned velocities.

**Trajectory**: A trajectory $(x_0, \dots, x_{t}, \dots,x_1)$, is simply the solution of the above ODE for a given start point $x_0$.  It’s the continuous path that the “particle” traces out under the influence of the vector field:

$$
x(t)=x(0) + \int_0^tv_{\theta}(s, x(s))ds\label{eq:3}
$$

Or using the Euler method to  solve in a discrete time step:

$$
x_t=x_{t-1}+h*v_{\theta}(t-1, x_{t-1})\label{eq:4}
$$

**Flow**: a flow is essentially a collection of trajectories that follows the ODE, that means by solving the above ODE we gather a lot of solutions for different initial points

---


## <a id="section2.2">2.2 Probability path</a>

The first step of FM is to define a probability path, who specifies a gradual interpolation from initial distribution $p_{init}$ to target distribution $p_{data}$.

**Conditional probability path:**  given an end point $z \sim p_{data}$, a conditional probability path is the distribution of an intermediate sample conditioned on $z$, denoted as $p_t(x_t\|z)$ such that

$$
p_0(\cdot\|z)=p_{init},\ \ \ p_0(\cdot\|z)=\delta_z \ \ \ \ {\rm for\ all}\ z \in \mathbb R^d\label{eq:5}
$$

**Marginal probability path:** marginal probability path defined as the distribution that we obtain by first sampling a data point $z ∼ p_{data}$ from the data distribution and then sampling from $p_t(x_t \|z)$, we can formalize it as:

$$
p_t(x_t)=\int p_t(x_t|z) p_{data}(z)dz\label{eq:6}
$$

$p_t(x_t)$ satisfy $p_0(\cdot)=p_{init}$ and $p_1(\cdot)=p_{data}$. The difference between the marginal probability path and the conditional probability path can be illustrated as follows.

![Conditional probability path and Marginal probability path](/images/posts/post_5/2.gif)


> **Takeaway.** Please be noticed that, in diffusion models, it is more common to use **$x_0$** to represent the **clear original image** and **$x_1$** to represent **noise**; whereas in flow matching, the opposite is true, with **$x_1$** more commonly used to represent the **clear original image** and **$x_0$** to represent **noise**.

---





# <a id="section3">3. Origins of the Idea: From Rectified Flow</a>

The question raised at the end of the [$1](#section1) — *can we directly learn the path between distributions?* — found its first concrete answer in **Rectified Flow (RF)** [^rectified_flow]. Rather than simulating a noisy process step by step, RF proposed a remarkably simple intuition: what if we could connect the data distribution and the noise distribution by the **shortest, straightest path**?


Suppose we have a data sample \$x\_1 \sim p\_{\text{data}}\$ and a noise sample \$x\_0 \sim p\_{\text{noise}}\$. The most direct connection between them is the **linear interpolation**

$$
x_t = (1 - t)\,x_0 + t\,x_1, \quad t \in [0,1].
$$

The velocity along this path is simply the constant vector

$$
v(x_t, t) = x_1 - x_0.
$$

Thus, instead of relying on stochastic simulation, RF frames generation as **learning a deterministic vector field** that drives samples along these straight trajectories. The learning objective is to regress a vector field \$v\_\theta(x,t)\$ that matches the ground-truth velocity:

$$
\mathcal{L}_{\text{RF}}(\theta) = \mathbb{E}_{x_0 \sim p_{\text{data}},\, x_1 \sim p_{\text{noise}},\, t \sim \mathcal{U}[0,1]} 
\big[\, \| v_\theta(x_t,t) - (x_1 - x_0) \|^2 \,\big].
$$

This turns training into standard **supervised regression problem**, with no need to simulate stochastic noise trajectories. At inference, RF generates by solving the learned ODE

$$
\frac{dx}{dt} = v_\theta(x,t), \quad x(0) \sim p_{\text{noise}}\label{eq:14}
$$



---

# <a id="section4">4. Establishing the Flow Matching Paradigm</a>

Rectified Flow showed that generation can be reframed as **regressing velocities along straight-line paths** between noise and data. But this is only one choice of path. The key insight of **Flow Matching (FM)** [^FM] is that the idea generalizes: *any explicit, sampleable probability path can define a regression target whose solution transports noise into data.*

## <a id="section4.1">4.1 From Straight Lines to General Paths</a>

Instead of a fixed linear interpolation, Flow Matching begins by defining a more general **marginal probability path**, $p_t(x_t)$, that connects any pair of points $x_0 \sim p_0$ (noise) and $x_1 \sim p_1$ (data). This path is a time-dependent probability distribution that satisfies the boundary conditions:

- at $t=0$, it concentrates on the starting point: $p_0(x_0) = p_0$.
- at $t=1$, it concentrates on the endpoint: $p_1(x_1) = p_1$.



A common and intuitive choice for this path is the **Gaussian probability path**, defined as:

$$
p_t(x_t) = \mathcal{N}(x \mid s(t) x_1, \sigma(t)^2I), \quad x_1 \sim p_{\text data}
$$

For any time $t$, since $x_t$ follows the distribution $p_t$, we can obtain the expression for $x_t$ as follows:


$$
x_t = s(t)x_1+\sigma(t)\epsilon = s(t)x_1+\sigma(t)x_0, \quad x_0 \sim \mathcal{N}(0,I)\label{eq:16}
$$ 

> **Takeaway.** It is noted that formula \ref{eq:16} is identical to the forward process of the diffusion model (apart from the notation being exactly the opposite). Therefore, the probability path defined in flow matching is equivalent to the forward noising schedules in the diffusion model.

It is not difficult to see that if we set $s(t) = t$ and $\sigma(t) = (1 - t)$, then this path collapses to the deterministic straight line of Rectified Flow, revealing RF as a specific instance of a broader class of paths.



---

## <a id="section4.2">4.2 Core objective: vector-field regression</a>

Associated with this probability path is a **marginal vector field**, $v_t(x_t)$, which represents the instantaneous velocity required for a particle at position $x_t$ at time $t$ to stay on the trajectory defined by the path. 

Following equation \ref{eq:14} and equation \ref{eq:16},  the corresponding marginal vector field is simply

$$v_t(x_t) = \frac{ds(t)}{dt}x_1 + \frac{d\sigma(t)}{dt}x_0$$

With a general path $p_t$ and its target vector field $v_t(x_t)$ defined, the learning problem becomes remarkably direct. The goal is to train a neural network $v_\theta(x, t)$ to match the target vector field $v_t$ at all times $t$ and positions $x$. This is formulated as the **Flow Matching objective**:

$$
\mathcal{L}_{\text{FM}}(\theta) = \mathbb{E}_{t \sim \mathcal{U}[0,1],\, x_t \sim p_t} \big[\, \| v_\theta(x_t, t) - v_t(x_t) \|^2 \,\big]\label{eq:18}
$$

---

## <a id="section4.3">4.3 Consistency with the Continuity Equation

Let $p_t$ be the marginal density of $x_t$. Then with the optimal field $v^\star$,

$$
\partial_t p_t(x) + \nabla\!\cdot\!\big(p_t(x_t)\,v^\star(x_t,t)\big) = 0,
$$

the **continuity equation** that guarantees mass is transported consistently along the path.

Thus, solving the learned ODE

$$
\frac{dx}{dt} = v_\theta(x,t), \qquad x(0)\sim p_1,
$$

yields a generator whose marginals approximate the designed path $p_t$, arriving at $p_1=p_{\text data}$ when $t=1$.

---

## <a id="section4.4">4.4 Translating Perspectives: connections to CNF and PF-ODE</a>

Flow Matching does not exist in a vacuum. It provides a new lens through which we can understand and unify existing paradigms.

- **Connection to Continuous Normalizing Flows (CNFs):** Both FM and CNFs learn a vector field $v_\theta$ to define a continuous-time generative model. However, CNFs are trained via maximum likelihood, which requires computing the computationally expensive **Jacobian trace** ($\text{Tr}(\nabla_x v_\theta)$). Flow Matching cleverly sidesteps this. The FM objective is a regression loss that is mathematically proven to guide the marginal probability path $p_t(x)$ to match the target data distribution, effectively achieving the same goal as CNFs but without the need for trace computations. It trades the exact likelihood objective for a more direct and scalable vector field regression.

- **Connection to Probability Flow ODEs (PF-ODEs):** Diffusion models also have a corresponding deterministic ODE for sampling, known as the Probability Flow ODE. This ODE's vector field is derived from the **score** of the noisy data distribution ($\nabla_x \log p_t(x)$). To train a model to predict this score, one must simulate the forward stochastic noising process. Flow Matching provides a more direct route. It bypasses the need for a stochastic process and score estimation entirely by directly defining the desired path and regressing its vector field. While both frameworks ultimately yield a generative ODE, FM constructs it from a "path-centric" perspective, whereas diffusion arrives at it from a "process-centric" one.

By establishing this simple, scalable regression objective, Flow Matching created a powerful new paradigm for generative modeling—one that retains the continuous-time elegance of CNFs while offering the practical efficiency and performance of diffusion models.

---

# <a id="section5">5. Extension: Conditional Flow Matching</a>

While Flow Matching provides a general regression framework, optimizing the target function shown in Equation \ref{eq:18} is very difficult, as directly estimating true target  $v_t(x_t)$ is expensive: **it requires integrating over all possible $z \sim p_{1}$ that generated by $x_t$**.

$$
v_t(x_t)=\int v_t(x_t|z)p(z|x_t)dz=\int v_t(x_t|z)\frac{p_t(x_t|z)p_{data}(z)}{p(x_t)}dz\label{eq:19}
$$


**Conditional Flow Matching (CFM)** solves this problem by introducing a conditioning variable $z$ that makes the velocity target explicit and unbiased, while avoiding marginalization over all possible samples.

CFM begins by defining **Conditional probability path** ([$2.2](#section2.2)), $p_t(x_t \mid z)$, Here, $z$ is deterministic data sampled from $p_{\text data}$, rather than arbitrary data. Similarly, associated with this probability path is a **Conditional vector field**, $v_t(x_t \mid z)$, the relationship between Conditional vector field and marginal vector field is as shown in \ref{eq:19}.

Now, with the conditional vector field as the true target, we construct the objective function for CFM.

$$
\mathcal L_{CFM}(\theta)=\big|\big| u_t^{\theta}(x_t) - u_t(x_t|z) \big|\big|^2\label{eq:9}
$$

Conditional velocity field is tractabl, which makes it feasible to minimize $\mathcal L_{CFM}(\theta)$ mathematically. The key point is that $\mathcal L_{FM} = \mathcal L_{CFM}+C$, where $C$ is a constant, which means that minimizing $\mathcal L_{FM}$ is equal to minimizing $\mathcal L_{CFM}$ with represpect to $\theta$.

$$
\begin{align}
\qquad & \mathcal L_{FM}(\theta)=\mathbb E_{t\sim U(0,1), x_t\sim p_t(x_t)}\| v_{\theta}(t, x_t) - v_t(x_t) \|^2\label{eq:23} \\[10pt]
\Longrightarrow \qquad 
& \mathcal L_{FM}(\theta)=\mathbb E_{t \sim U(0,1), x_t\sim p_t(x_t)}\big(\| v_{\theta}(t, x_t) \|^2 -2\| v_{\theta}(t, x_t) \| \| v_t(x_t) \| + \| v_t(x_t) \|^2 \big)\label{eq:24}
\end{align}
$$

Consider the second term:

$$
\begin{align}
\qquad & \mathbb E_{t\sim U(0,1), x_t\sim p_t(x_t)}\big(\| v_{\theta}(t, x_t) \| \times \| v_t(x_t) \| \big)\label{eq:25}  \\[10pt] 
\Longrightarrow  \qquad & \int_t\int_{x_t} p_t(x_t) \times \| v_{\theta}(t, x_t) \| \times \| v_t(x_t) \|dx_tdt\label{eq:26}  \\[10pt] 
\Longrightarrow \qquad & \int_t\int_{x_t} p_t(x_t) \times \| v_{\theta}(t, x_t) \| \times \big( \int_z\| v_t(x_t|z) \| \times \frac{p_t(x_t \mid z)p_{data}(z)}{p(x_t)}dz \big) dx_tdt\label{eq:27}   \\[10pt]
\Longrightarrow \qquad &  \int_t\int_{x_t}\int_z p_t(x_t) \times \frac{p_t(x_t \mid z)p_{data}(z)}{p(x_t)} \times \| v_{\theta}(t, x_t) \| \times \| u_t(x_t \mid z) \|dzdx_tdt\label{eq:28}   \\[10pt] 
\Longrightarrow \qquad & \int_t\int_{x_t}\int_z p_t(x_t \mid z) \times p_{data}(z) \times \| v_{\theta}(t, x_t) \| \times \| v_t(x_t \mid z) \|dzdx_tdt\label{eq:29}   
\\[10pt] 
\Longrightarrow \qquad &  \mathbb E_{t\sim U(0,1), z\sim p_{data}(z), x_t\sim p_t(x_t|z)}\big(\| v_{\theta}(t, x_t) \| \times \| u_t(x_t \mid z) \| \big)\label{eq:30}
\end{align}
$$

Now, Let's substitute the second item with the result above:


$$
\begin{align}
& \mathcal L_{FM}(\theta)=\mathbb E_{\scriptscriptstyle  t\sim U(0,1), x_t\sim p_t(x_t)}\| v_{\theta}(t, x_t) - v_t(x_t) \|^2 \\[10pt]
\Longrightarrow
& \mathcal L_{FM}(\theta)=\mathbb E_{\scriptscriptstyle  t\sim U(0,1), x_t\sim p_t(x_t)}\big(\| v_{\theta}(t, x_t) \|^2 -2\| v_{\theta}(t, x_t) \| \| v_t(x_t) \| + \| v_t(x_t) \|^2 \big)  \\[10pt] 
\Longrightarrow 
& \mathcal L_{FM}(\theta)=\mathbb E_{\scriptscriptstyle  t\sim U(0,1), z\sim p_{data}(z), x_t\sim p_t(x_t|z)}\big(\| v_{\theta}(t, x_t) \|^2 -2\| v_{\theta}(t, x_t) \| \| u_t(x_t) \| + \| u_t(x_t) \|^2 \big)  \\[10pt] 
\Longrightarrow & \mathcal L_{FM}(\theta)=\mathbb E_{\scriptscriptstyle  t\sim U(0,1), z\sim p_{data}(z), x_t\sim p_t(x_t|z)}\| v_{\theta}(t, x_t) - v_t(x_t|z) \|^2  +C_1+C_2  \\[10pt] 
\Longrightarrow 
& \mathcal L_{FM}(\theta) = \mathcal L_{CFM}(\theta)+C
\end{align}
$$

We can summarize the relationship between $\mathcal L_{FM}$ and \mathcal L_{CFM} with the following figure.

![Flow Matching Loss and Conditional Flow Matching Loss](/images/posts/2025-06-08-blog-post/4.jpg)



---

# <a id="section6">6. A Unified Narrative: Stochastic Interpolants</a>

## <a id="section6.1">6.1 From Deterministic to Stochastic Interpolations</a>

---

## <a id="section6.2">6.2 Diffusion, RF, and FM as Mutual Special Cases</a>

---


# <a id="section7">7. Conclusion & Bridge</a>

---

# <a id="section8">8. References</a>

[^Nice]: Dinh L, Krueger D, Bengio Y. Nice: Non-linear independent components estimation[J]. arXiv preprint arXiv:1410.8516, 2014.

[^nvp]: Dinh L, Sohl-Dickstein J, Bengio S. Density estimation using real nvp[J]. arXiv preprint arXiv:1605.08803, 2016.

[^Glow]: Kingma D P, Dhariwal P. Glow: Generative flow with invertible 1x1 convolutions[J]. Advances in neural information processing systems, 2018, 31.

[^Neural_ODE]: Chen R T Q, Rubanova Y, Bettencourt J, et al. Neural ordinary differential equations[J]. Advances in neural information processing systems, 2018, 31.

[^Ffjord]: Grathwohl W, Chen R T Q, Bettencourt J, et al. Ffjord: Free-form continuous dynamics for scalable reversible generative models[J]. arXiv preprint arXiv:1810.01367, 2018.

[^rectified_flow]: Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022.

[^FM]: Lipman Y, Chen R T Q, Ben-Hamu H, et al. Flow matching for generative modeling[J]. arXiv preprint arXiv:2210.02747, 2022.