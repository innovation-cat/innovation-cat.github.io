---
title: 'From Diffusion to Flow — Seeking the Elegant Path'
date: 2025-06-08
excerpt: "In this post, we uncovered the foundations of Flow Matching: the limitations of diffusion models, the constraints of continuous flows, and the transformative idea of directly learning the path between distributions. From the intuition of Rectified Flow to the unifying lens of Stochastic Interpolants, Flow Matching emerged as more than a method — it is a paradigm that reframes generation as learning currents of transformation. With this conceptual map in hand, we are now ready to move from theory to practice."
permalink: /posts/2025/06/flow-matching-1/
tags:
  - Generative Models
  - Discussion Models
  - Normalizing Flows
  - Flow Matching
  - Rectified Flow
  - Conditional Flow Matching
  - Stochastic Interpolants
---


<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>📚 Table of Contents</strong></span>
  </summary>
  <ul>
	<li><a href="#section1">1. Introduction</a>
		<ul>
			<li><a href="#section1.1">1.1 The Rise and Bottlenecks of Diffusion Models</a></li>
			<li><a href="#section1.2">1.2 The Elegance and Constraints of Continuous Normalizing Flows</a></li>
			<li><a href="#section1.3">1.3 The Breaking Point: Toward Direct Paths Between Distributions</a></li>
		</ul>
	</li>
	<li><a href="#section2">2. Preliminaries</a>
		<ul>
			<li><a href="#section2.1">2.1 Vector Field, Trajectory and Flow</a></li>
			<li><a href="#section2.2">2.2 Probability path</a></li>
		</ul>
	</li>
	<li><a href="#section3">3. Origins of the Idea: From Rectified Flow</a>
		<ul>
			<li><a href="#section3.1">3.1 Rectified Flow: The Intuition of the Shortest Path</a></li>
			<li><a href="#section3.2">3.2 Reflow: Straightening the Path Further</a></li>
		</ul>
	</li>
	<li><a href="#section4">4. Establishing the Flow Matching Paradigm</a>
		<ul>
			<li><a href="#section4.1">4.1 From RF to FM: a general probabilistic path</a></li>
			<li><a href="#section4.2">4.2 Core objective: vector-field regression</a></li>
			<li><a href="#section4.3">4.3 Translating Perspectives: connections to CNF and PF-ODE</a></li>
		</ul>
	</li>
	<li><a href="#section5">5. Extension: Conditional Flow Matching</a></li>
	<li><a href="#section6">6. A Unified Narrative: Stochastic Interpolants</a>
		<ul>
			<li><a href="#section6.1">6.1 From Deterministic to Stochastic Interpolations</a></li>
			<li><a href="#section6.2">6.2 Diffusion, RF, and FM as Mutual Special Cases</a></li>
		</ul>
	</li>
	<li><a href="#section7">7. Conclusion & Bridge</a></li>
	<li><a href="#section8">8. References</a></li>
  </ul>
</details>

This series, **the Flow Matching Trilogy**, is a journey through the past, present, and future of flow-based generative models. 

- [Part 1](https://innovation-cat.github.io/posts/2025/06/flow-matching-1/): introduces the theoretical foundations of Flow Matching, tracing its emergence from diffusion processes and normalizing flows toward a unified framework. 

- [Part 2](https://innovation-cat.github.io/posts/2025/07/flow-matching-2/): turns theory into practice, offering a systematic engineering guide on how to design, control, and accelerate Flow Matching systems. 

- [Part 3](https://innovation-cat.github.io/posts/2025/08/flow-matching-3/): looks outward, exploring the frontiers of research — from discrete and structured data to large-scale and multimodal models.

In the first article, we establishes the foundations of Flow Matching. It reviews the bottlenecks of diffusion models, the constraints of normalizing flows, and introduces the key idea of directly learning transformation paths between distributions. From Rectified Flow to Conditional Flow Matching and Stochastic Interpolants, it outlines how a simple intuition matured into a unified generative framework, paving the way for the practical design discussions in Part 2.

---

# <a id="section1">1. Introduction: Seeking the Third Road of Generation</a>

Before we arrive at Flow Matching itself, it is essential to understand the two dominant paradigms that shaped the landscape of modern generative modeling: diffusion models and normalizing flows. Each offers a distinct philosophy — diffusion relies on stochastic forward processes and iterative denoising, while flows construct invertible mappings grounded in probability theory. Both reveal valuable insights but also expose fundamental limitations. By examining their strengths and bottlenecks, we prepare the ground for a third road - Flow Matching.

## <a id="section1.1">1.1 The Rise and Bottlenecks of Diffusion Models</a>

Diffusion models have defined the state of the art in generative modeling. By gradually adding noise through a fixed forward process and then learning to reverse it, they can approximate extremely complex data distributions with remarkable fidelity. We have discussed many training and sampling techniques for diffusion models in the previous posts.

However, their reliance on **iterative denoising** is costly. Sampling often requires hundreds of steps, each invoking a deep network, which makes inference slow and error-prone. The method is powerful but **simulation-heavy**, as both training (via noisy forward diffusion) and inference (via numerical solvers) depend on repeatedly simulating stochastic dynamics.

---

## <a id="section1.2">1.2 The Elegance and Constraints of Continuous Normalizing Flows</a>


At the heart of Normalizing Flows (NFs) [^Nice] [^nvp] [^Glow] lies a simple but powerful principle: the **change-of-variables** formula. It connects the density of a data point $z$ in one space to the density of its transformed representation $x$ in another .

![Conditional probability path and Marginal probability path](/images/posts/2025-06-08-blog-post/1.jpg)

Suppose we have a random variable $z \sim p(z)$, where $p_z$ is a simple Gaussian distribution, we then construct a chain of $K$ invertible mappings:

$$
z_0 \sim p_z(z_0), \quad z_1 = f_1(z_0), \quad \dots, \quad x = z_K = f_K \circ \cdots \circ f_1(z_0).
$$

By repeatedly applying the change-of-variables formula, we get the final variable $x \sim p(x)$, the density of $x$ is given by

$$
\log p(x) = \log p(z_0) - \sum_{k=1}^K \log \left|\det J_{f_i}\right|.
$$

Where $J_{f_i}$ is Jacobian Matrix. Thus, the entire model is **exact likelihoods mathematically**, as long as each transformation is invertible and its Jacobian determinant is tractable. However, the computation the Jacobian determinant of $f_i$ is very expensive in high dimensions. 

To make Jacobian computation feasible, invertible mappings should be designed for special architecture to guarantee a tractable Jacobian determinant. For example, Nice [^Nice] using additive coupling layer to make Jacobian determinant is equal to 1.

![NICE](/images/posts/2025-06-08-blog-post/2.jpg)

Real NVP [^nvp] extends NICE’s additive coupling to an affine coupling layer, which guarantees that the Jacobian determinant is just the product of elementwise scales (a diagonal matrix). This makes log-likelihood tractable while keeping the model invertible and expressive.

![Real_NVP](/images/posts/2025-06-08-blog-post/3.jpg)

The move from **discrete transformations** to **continuous-time transformations** gave rise to **Continuous Normalizing Flows (CNFs)** [^Neural_ODE] [^Ffjord]. Instead of stacking finitely many mappings, we let the transformation evolve under an ODE:

$$
\frac{dz}{dt} = v_\theta(z, t), \quad z(0) \sim p_z, \quad z(1) \sim p_x.
$$

The corresponding density evolution is given by the **instantaneous change-of-variables formula** (Chen et al., 2018):

$$
\frac{d}{dt} \log p(z(t)) = - \text{Tr}\!\left(\frac{\partial v_\theta(z(t),t)}{\partial z(t)}\right).
$$

In theory, CNFs remove the architectural rigidity of discrete NFs, allowing any sufficiently expressive neural network to parameterize the vector field $v_\theta$. They provide a mathematically elegant and continuous-time perspective on generative modeling, bridging flows and differential equations.

However, the need to compute **Jacobian traces** is computationally expensive, and CNFs often demand architectures designed specifically to trade off expressivity and tractability. Training and sampling can be slow, and scaling to high-dimensional data remains difficult.

---

## <a id="section1.3">1.3 The Breaking Point: Toward Direct Paths Between Distributions</a>

Between diffusion’s slow stochastic wanderings and CNF’s constrained elegance lies a natural question: *must generative modeling always depend on heavy simulation or exact density tracking?*

Imagine instead that we could **directly learn the transformation path** between two distributions — for example, between noise and data. Rather than simulating a process step by step, or computing exact likelihoods, we could regress a **vector field** that prescribes how samples should flow from one distribution to another. This reframes generative modeling as the study of **currents of transformation**, where the emphasis shifts from *simulation* to *direct learning*.

This insight marks the turning point for **Flow Matching**. It signals the possibility of a third road: a paradigm that is at once intuitive, efficient, and theoretically unifying.

---

# <a id="section2">2. Preliminaries</a>

In this section, we first summarize the key terms and terminology needed for learning flow matching.

## <a id="section2.1">2.1 Vector Field, Trajectory and Flow</a>

**Vector Field**: vector field is a function that assigns to each spatial point $x_t \in \mathbb R^d$ and time $t \in [0, 1]$ an instantaneous velocity $v_{\theta}(t, x_t)$:

$$
v_{\theta}:\ \mathbb R^d \times [0,1] \to \mathbb R^d\label{eq:1}
$$

**ODE**: ODE (Ordinary Differential Equation) is the dynamical law you impose using that vector field:

$$
\frac{dx_t}{dt}=v_{\theta}(t, x_t)\label{eq:2}
$$

Solving this ODE from $t=0$ to $t=1$ is equivalent to sampling, whose goal is to transport an initial point $x_0$ to a target $x_1$ through space according to the learned velocities.

**Trajectory**: A trajectory $(x_0, \dots, x_{t}, \dots,x_1)$, is simply the solution of the above ODE for a given start point $x_0$.  It’s the continuous path that the “particle” traces out under the influence of the vector field:

$$
x(t)=x(0) + \int_0^tv_{\theta}(s, x(s))ds\label{eq:3}
$$

Or using the Euler method to  solve in a discrete time step:

$$
x_t=x_{t-1}+h*v_{\theta}(t-1, x_{t-1})\label{eq:4}
$$

**Flow**: a flow is essentially a collection of trajectories that follows the ODE, that means by solving the above ODE we gather a lot of solutions for different initial points

---


## <a id="section2.2">2.2 Probability path</a>

The first step of FM is to define a probability path, who specifies a gradual interpolation from initial distribution $p_{init}$ to target distribution $p_{data}$.

**Conditional probability path:**  given an end point $z \sim p_{data}$, a conditional probability path is the distribution of an intermediate sample conditioned on $z$, denoted as $p_t(x_t\|z)$ such that

$$
p_0(\cdot\|z)=p_{init},\ \ \ p_0(\cdot\|z)=\delta_z \ \ \ \ {\rm for\ all}\ z \in \mathbb R^d\label{eq:5}
$$

**Marginal probability path:** marginal probability path defined as the distribution that we obtain by first sampling a data point $z ∼ p_{data}$ from the data distribution and then sampling from $p_t(x_t \|z)$, we can formalize it as:

$$
p_t(x_t)=\int p_t(x_t|z) p_{data}(z)dz\label{eq:6}
$$

$p_t(x_t)$ satisfy $p_0(\cdot)=p_{init}$ and $p_1(\cdot)=p_{data}$. The difference between the marginal probability path and the conditional probability path can be illustrated as follows.

![Conditional probability path and Marginal probability path](/images/posts/post_5/2.gif)

One particularly popular probability path is the Gaussian probability path.

---





# <a id="section3">3. Origins of the Idea: From Rectified Flow</a>

The question raised at the end of the [$1](#section1) — *can we directly learn the path between distributions?* — found its first concrete answer in **Rectified Flow (RF)** [^rectified_flow]. Rather than simulating a noisy process step by step, RF proposed a remarkably simple intuition: what if we could connect the data distribution and the noise distribution by the **shortest, straightest path**?

---

## <a id="section3.1">3.1 Rectified Flow: The Intuition of the Shortest Path</a>

Suppose we have a data sample \$x\_0 \sim p\_{\text{data}}\$ and a noise sample \$x\_1 \sim p\_{\text{noise}}\$. The most direct connection between them is the **linear interpolation**

$$
x_t = (1 - t)\,x_0 + t\,x_1, \quad t \in [0,1].
$$

The velocity along this path is simply the constant vector

$$
v(x_t, t) = x_1 - x_0.
$$

Thus, instead of relying on stochastic simulation, RF frames generation as **learning a deterministic vector field** that drives samples along these straight trajectories. The learning objective is to regress a vector field \$v\_\theta(x,t)\$ that matches the ground-truth velocity:

$$
\mathcal{L}_{\text{RF}}(\theta) = \mathbb{E}_{x_0 \sim p_{\text{data}},\, x_1 \sim p_{\text{noise}},\, t \sim \mathcal{U}[0,1]} 
\big[\, \| v_\theta(x_t,t) - (x_1 - x_0) \|^2 \,\big].
$$

This turns training into standard **supervised regression problem**, with no need to simulate stochastic noise trajectories. At inference, RF generates by solving the learned ODE

$$
\frac{dx}{dt} = v_\theta(x,t), \quad x(0) \sim p_{\text{noise}},
$$



---

## <a id="section3.2">3.2 Reflow: Straightening the Path Further</a>

The idea was further extended by Albergo & Vanden-Eijnden (2023) in **Reflow**. Instead of committing to a fixed linear interpolation, Reflow iteratively *straightens* arbitrary generative paths: starting from a learned flow, one can re-train by using its generated trajectories as new interpolation baselines, thereby pushing the flow closer to a straight line.

Formally, given an existing flow \$v\_\theta\$, one defines new interpolation pairs \$(x\_0, x\_1)\$ where \$x\_1\$ is a generated sample from \$v\_\theta\$. Training again with the RF objective produces a refined vector field that yields shorter, straighter trajectories. This iterative straightening opens the door to **few-step or even one-step generation**, a long-standing goal in generative modeling.

---

# <a id="section4">Establishing the Flow Matching Paradigm</a>

## <a id="section4.1">4.1 From RF to FM: a general probabilistic path</a>

---

## <a id="section4.2">4.2 Core objective: vector-field regression</a>

---

## <a id="section4.3">4.3 Translating Perspectives: connections to CNF and PF-ODE</a>

---

# <a id="section5">5. Extension: Conditional Flow Matching</a>

---

# <a id="section6">6. A Unified Narrative: Stochastic Interpolants</a>

## <a id="section6.1">6.1 From Deterministic to Stochastic Interpolations</a>

---

## <a id="section6.2">6.2 Diffusion, RF, and FM as Mutual Special Cases</a>

---


# <a id="section7">7. Conclusion & Bridge</a>

---

# <a id="section8">8. References</a>

[^Nice]: Dinh L, Krueger D, Bengio Y. Nice: Non-linear independent components estimation[J]. arXiv preprint arXiv:1410.8516, 2014.

[^nvp]: Dinh L, Sohl-Dickstein J, Bengio S. Density estimation using real nvp[J]. arXiv preprint arXiv:1605.08803, 2016.

[^Glow]: Kingma D P, Dhariwal P. Glow: Generative flow with invertible 1x1 convolutions[J]. Advances in neural information processing systems, 2018, 31.

[^Neural_ODE]: Chen R T Q, Rubanova Y, Bettencourt J, et al. Neural ordinary differential equations[J]. Advances in neural information processing systems, 2018, 31.

[^Ffjord]: Grathwohl W, Chen R T Q, Bettencourt J, et al. Ffjord: Free-form continuous dynamics for scalable reversible generative models[J]. arXiv preprint arXiv:1810.01367, 2018.

[^rectified_flow]: Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022.