---
title: 'Fast Generation with Flow Matching'
date: 2025-07-02
excerpt: "Fast sampling has become a central goal in generative modeling, enabling the transition from high-fidelity but computationally intensive diffusion models to real-time generation systems. While diffusion models rely on tailored numerical solvers to mitigate the stiffness of their probability flow ODEs, flow matching defines dynamics through smooth interpolation paths, fundamentally altering the challenges of acceleration. This article provides a comprehensive overview of fast sampling in flow matching, with emphasis on path linearization strategies (e.g., Rectified Flow, ReFlow, SlimFlow, InstaFlow), the integration of consistency models, and emerging approaches such as flow generators."
permalink: /posts/2025/07/flow-matching-2/
tags:
  - Flow Matching
  - Acceleration
  - Consistency Models
---

<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>ðŸ“š Table of Contents</strong></span>
  </summary>
  <ul>
	<li><a href="#section1">1. From Diffusion to Flow Matching in Fast Sampling</a>
		<ul>
			<li><a href="#section1.1">1.1 The challenges of Diffusion Fast Sampling</a></li>
			<li><a href="#section1.2">1.2 Flow Matching Fast Sampling: Structural Differences and New Opportunities</a></li>
		</ul>
	</li>
	<li><a href="#section2">2. Straighten trajectories for Efficient Flow Matching</a>
		<ul>
			<li><a href="#section2.1">2.1 Motivation</a>
				<ul>
					<li><a href="#section2.1.1">2.1.1 LTE and the Role of Curvature</a></li>
					<li><a href="#section2.1.2">2.1.2 Why Path Linearization Helps</a></li>
				</ul>
			</li>
		</ul>
	</li>
  </ul>
</details>

Both Diffusion Models and Flow Matching are achieving state-of-the-art results across image, audio, and multimodal domains. Despite their success, a persistent limitation has been the large number of sampling steps required to produce high-quality outputs. This computational bottleneck has spurred intense research into few-step or even single-step generation methods, aiming to reconcile efficiency with sample quality.

In our previous articles ([Post1](https://innovation-cat.github.io/posts/2025/03/diffusion-model-3/) and [Post2](https://innovation-cat.github.io/posts/2025/03/ODE-Solver/)), we discussed the fast sampling issue of diffusion models. In this post, let's explore few-step or even single-step generation methods for flow matching.


# <a id="section1">1. From Diffusion to Flow Matching in Fast Sampling</a>

As introduced in our [previous article](https://innovation-cat.github.io/posts/2025/06/flow-matching-1/), DM and FM are two different generative paradigms, with DM being process-prioritized and FM being path-prioritized. This difference also leads to distinct optimization during fast sampling.


## <a id="section1.1">1.1 The challenges of Diffusion Fast Sampling</a>

Diffusion models sampling can be reduced to the problem of solving an PF-ODE

$$
\frac{dx_t}{dt} = f(t)x_t - \frac{1}{2}g^2(t)s_{\theta}(x_t, t)\label{eq:1}
$$

The structure of this ODE imposes severe challenges for numerical integration:

- **Stiffness**: The Stiffness of PFODE mainly comes from the nonlinear term. Specifically, the nonlinear score term $s_\theta(x_t, t)$ exhibits large magnitude variation across the time horizon. Together, these make the PF-ODE inherently stiff. Traditional high-order ODE solvers are often unstable or ineffective in this context (small step size), leading to the development of high-order pseudo-numerical methods such as DDIM, DEIS, DPM-Solver, and UniPC.

- **Approximation error**: Unlike classical ODEs where the vector field is analytically known, the drift depends on a neural score network, which itself is an approximation. This error accumulates continuously during the iteration process, so the sampling method needs to be able to reduce error accumulation or correct it.

- **Hight cost of NFE (Number of Function Evaluations)**: In traditional ODEs, the right hand side of Eq. \ref{eq:1} is a simple analytical expression, evaluation is essentially free. However, in Diffusion Models, this function is not explicit but parameterized by a large neural network with hundreds of millions of parameters, which is computationally expensive (tens or hundreds of GFLOPs). Diffusion model sampling needs to make a trade-off between high-quality output and minimizing the number of NFE.

As a result, the study of fast sampling in diffusion has been dominated by Two Major Categories.

- **Training-free approaches (numerical solverâ€“based)**: The goal is to speed up sampling without retraining the diffusion model, the key idea is to design specialized high-order solvers that exploit the semi-linear structure of the PF-ODE while tolerating approximation noise (such as DDIM, DEIS, DPM-Solver, and UniPC), the model weights remain unchanged, improvements come purely from the numerical side.

- **Training-coupled approaches (modelâ€“based)**: The goal is to modify training so that the model itself supports fast (few-step or even single-step) generation, The key idea is to incorporate distillation, consistency constraints, or new training paradigms. Such methods require retraining or fine-tuning the model, trading additional training cost for dramatic inference speedups.


## <a id="section1.2">1.2 Flow Matching Fast Sampling: Structural Differences and New Opportunities</a>

In flow matching, the generative process is described by

$$
\frac{d x_t}{d t} = v_\theta(x_t, t),
$$

where $v_\theta$ approximates a target velocity field defined by an interpolation path between the initial distribution $p_0$ and the data distribution $p_1$. approximation error and NFE costs are still exist in flow matching inference.

But a key difference is the flow matching has **No intrinsic stiffness**: The target velocity field is derived from a user-chosen interpolation path, which can be smooth by construction. Thus, the ODE is not structurally stiff.


These structural differences mean that FM behaves like a typical Neural ODE, and thus rarely needs specialized high-order solvers (the way diffusion does), most FM work simply uses off-the-shelf ODE solvers with modest step counts. The study of fast sampling in FM has been dominated by Two Major Categories.

- **Path Straightening (Trajectory Linearization)**: The core idea is that the straighter the learned transport path between data distribution and base distribution, the easier it is to approximate with fewer integration steps. The goal is to train a vector field to approximate a straight path.



- **Training-coupled approaches (modelâ€“based)**: Similar to diffusion acceleration, adjust training so that the model natively supports few-step or one-step sampling, Instead of relying on path geometry.


---

# <a id="section2">2. Straighten trajectories for Efficient Flow Matching</a>


The fundamental goal of path linearization in flow matching is to **reduce the geometric complexity** of the sample trajectories that connect the initial distribution $p_0$ to the data distribution $p_1$. When trajectories are highly curved, few-step ODE integration suffers from large global errors; conversely, nearly straight trajectories can be accurately captured with very few function evaluations. This phenomenon can be understood by examining the role of higher-order derivatives in numerical integration error.

---

## <a id="section2.1">2.1 Motivation</a>

Before introducing the concrete algorithms, letâ€™s first analyze why a straighter path can lead to faster sampling. Although this seems very intuitive, we still hope it can be explained mathematically.


### <a id="section2.1.1">2.1.1 LTE and the Role of Curvature</a>

Consider solving an ODE

$$
\frac{dx}{dt} = f(t, x)
$$

with step size $h$. The exact solution admits a Taylor expansion at $t$:

$$
x(t+h) = x(t) + h x'(t) + \tfrac{h^2}{2} x''(t) + \tfrac{h^3}{6} x^{(3)}(t) + \cdots .
$$

A first-order method such as Eulerâ€™s scheme only retains the first derivative term:

$$
x(t+h) \approx x(t) + h f(t, x(t)).
$$

The **local truncation error (LTE)** is therefore dominated by

$$
\text{LTE} \;\approx\; \tfrac{h^2}{2} x''(t) + \mathcal{O}(h^3).
$$

This shows explicitly that the second derivative $$x''(t)$$ â€” i.e., the **curvature of the trajectory** â€” acts as a multiplicative factor in the error constant. For higher-order integrators (Rungeâ€“Kutta of order $p$), the leading error terms similarly involve derivatives up to $x^{(p+1)}(t)$. Thus, trajectories with larger curvature inevitably incur larger numerical error for the same step size.

---


### <a id="section2.1.2">2.1.2 Why Path Linearization Helps</a>

Path linearization strategies aim to **design or refine the interpolation path** between $p_0$ and $p_1$ so that trajectories are as straight as possible, thereby reducing $$\|x''(t)\|$$ and higher-order derivatives. When curvature is small:

- The error constants in numerical integrators shrink,
- Few-step integration (even one-step) can approximate the full trajectory reliably,
- The sampling process is no longer limited by stiffness or oscillatory dynamics.

In other words, **straightening trajectories reduces the dependence of numerical error on higher-order derivatives**, making fast sampling feasible. This geometric perspective explains why methods such as Rectified Flow, ReFlow, and subsequent linearization techniques are effective in enabling one-step or few-step generation.



---
## <a id="section2.1">2.1 ReFlow â€” Iterative Rectification of Flow Matching</a>

Flow Matching (FM) defines a *straight path* between a base distribution $p_0$ (e.g., Gaussian noise) and the data distribution $p_1$. In training, the target velocity field corresponds to the tangent of this straight line:

$$
x_t = (1-t)x_0 + t x_1, \qquad \partial_t x_t = x_1 - x_0.
$$

However, the model is only conditioned on $(t, x_t)$ and does not observe the specific pair $(x_0, x_1)$. As a result, the learned vector field is the **marginal velocity field**, i.e. the conditional expectation of all possible straight-line velocities passing through $x_t$. 

$$
v_t(x) = \mathbb{E}\big[\partial_t x_t \,\big|\, x_t = x\big].
$$

Therefore, what the model learns is **not** the deterministic velocity of a specific straight-line path, but the conditional expectation of velocities across all possible paths at that point. This means even though the training objective enforces straight lines, the learned ODE trajectories $\tfrac{dx}{dt} = v_\theta(t,x)$ are generally curved.

![straight-line path and  ODE trajectories](/images/posts/2025-07-02-blog-post/1.jpg)

This discrepancy motivates **ReFlow (Rectified Flow)**: an iterative method to **straighten the learned trajectories** and accelerate sampling.

---

### Implementation: How ReFlow Works

ReFlow [^Reflow] is an iterative self-distillation procedure applied on top of FM:

1. **Warmup with real endpoints**

   * Train the model with real data samples $x_1 \sim p_{\text{data}}$.
   * This anchors the velocity field to the true data manifold.

2. **Synthetic endpoints generation**

   * Use the current (EMA) model as a teacher.
   * Sample from $p_0$ and integrate the learned ODE to obtain synthetic endpoints $\tilde{x}_1$.
   * These endpoints may be off-manifold or of imperfect quality.

3. **Mixing strategy**

   * Build a training buffer that contains both real endpoints and synthetic endpoints.
   * During training, each minibatch uses a mixture: with probability $1-p$ take a real endpoint, with probability $p$ take a synthetic endpoint.
   * The mixing ratio $p$ is gradually increased across rounds (e.g., from 0.4 â†’ 0.9).

4. **Rectification step**

   * Regardless of whether the endpoint is real or synthetic, the teacher velocity is again defined as the **straight-line velocity**:

     $$
     u^\*(x_t) = \frac{x_1 - x_0}{1-t} \quad \text{(or equivalently } x_1 - x_0 \text{ for straight paths)}.
     $$
   * Thus the model is always forced to interpret the path as a straight line between the chosen endpoints.

5. **Iteration**

   * Repeat: generate new synthetic endpoints with the updated model, refill the buffer, retrain with mixing.
   * After a few rounds, the flow field becomes increasingly straightened.

---
# <a id="section8">8. References</a>

[^Reflow]: Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022.

[^nvp]: Dinh L, Sohl-Dickstein J, Bengio S. Density estimation using real nvp[J]. arXiv preprint arXiv:1605.08803, 2016.

[^Glow]: Kingma D P, Dhariwal P. Glow: Generative flow with invertible 1x1 convolutions[J]. Advances in neural information processing systems, 2018, 31.

[^Neural_ODE]: Chen R T Q, Rubanova Y, Bettencourt J, et al. Neural ordinary differential equations[J]. Advances in neural information processing systems, 2018, 31.

[^Ffjord]: Grathwohl W, Chen R T Q, Bettencourt J, et al. Ffjord: Free-form continuous dynamics for scalable reversible generative models[J]. arXiv preprint arXiv:1810.01367, 2018.

[^rectified_flow]: Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022.

[^FM]: Lipman Y, Chen R T Q, Ben-Hamu H, et al. Flow matching for generative modeling[J]. arXiv preprint arXiv:2210.02747, 2022.

[^SI]: Albergo M S, Boffi N M, Vanden-Eijnden E. Stochastic interpolants: A unifying framework for flows and diffusions[J]. arXiv preprint arXiv:2303.08797, 2023.

[^SI_1]: Albergo M S, Vanden-Eijnden E. Building normalizing flows with stochastic interpolants[J]. arXiv preprint arXiv:2209.15571, 2022.

[^SI_2]: Albergo M S, Goldstein M, Boffi N M, et al. Stochastic interpolants with data-dependent couplings[J]. arXiv preprint arXiv:2310.03725, 2023.