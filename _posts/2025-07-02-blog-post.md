---
title: 'Fast Generation with Flow Matching'
date: 2025-07-02
excerpt: "Fast sampling has become a central goal in generative modeling, enabling the transition from high-fidelity but computationally intensive diffusion models to real-time generation systems. While diffusion models rely on tailored numerical solvers to mitigate the stiffness of their probability flow ODEs, flow matching defines dynamics through smooth interpolation paths, fundamentally altering the challenges of acceleration. This article provides a comprehensive overview of fast sampling in flow matching, with emphasis on path linearization strategies (e.g., Rectified Flow, ReFlow, SlimFlow, InstaFlow), the integration of consistency models, and emerging approaches such as flow generators."
permalink: /posts/2025/07/flow-matching-2/
tags:
  - Flow Matching
  - Acceleration
  - Consistency Models
---

<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>ðŸ“š Table of Contents</strong></span>
  </summary>
  <ul>
	<li><a href="#section1">1. Preliminaries</a>
		<ul>
			<li><a href="#section1.1">1.1 The challenges of Diffusion Fast Sampling</a></li>
			<li><a href="#section1.2">1.2 Flow Matching Fast Sampling: Structural Differences and New Opportunities</a></li>
		</ul>
	</li>
  </ul>
</details>

Both Diffusion Models and Flow Matching are achieving state-of-the-art results across image, audio, and multimodal domains. Despite their success, a persistent limitation has been the large number of sampling steps required to produce high-quality outputs. This computational bottleneck has spurred intense research into few-step or even single-step generation methods, aiming to reconcile efficiency with sample quality.

In the previous articles, we discussed the fast sampling issue of diffusion models. In this post, let's explore few-step or even single-step generation methods for flow matching.


# <a id="section1">1. Introduction: From Diffusion to Flow Matching in Fast Sampling</a>


# <a id="section1.1">1.1 The challenges of Diffusion Fast Sampling</a>

Diffusion models sampling can be reduced to the problem of solving an PF-ODE

$$
\frac{dx_t}{dt} = f(t)x_t - \frac{1}{2}g^2(t)s_{\theta}(x_t, t)\label{eq:1}
$$

The structure of this ODE imposes severe challenges for numerical integration:

- **Stiffness**: The linear drift term $-\tfrac{1}{2}\beta(t) x_t$ has coefficients that vary significantly with time, while the nonlinear score term $s_\theta(x_t, t)$ exhibits large magnitude variation across the time horizon. Together, these make the PF-ODE inherently stiff. Traditional high-order ODE solvers are often unstable or ineffective in this context, leading to the development of tailored pseudo-numerical methods such as DDIM, DEIS, DPM-Solver, and UniPC.

- **Approximation error**: Unlike classical ODEs where the vector field is analytically known, the drift depends on a neural score network, which itself is an approximation.

- **NFE (Number of Function Evaluations)**: In traditional ODEs, the right hand side of Eq. \ref{eq:1} is a simple analytical expression, evaluation is essentially free. However, in Diffusion Models, this function is not explicit but parameterized by a large neural network with hundreds of millions of parameters, which is computationally expensive (tens or hundreds of GFLOPs).

As a result, the study of fast sampling in diffusion has been dominated by Two Major Categories.

- **Training-free approaches (numerical solverâ€“based)**: The goal is to speed up sampling without retraining the diffusion model, the key idea is to design specialized high-order solvers that exploit the semi-linear structure of the PF-ODE while tolerating approximation noise (such as DDIM, DEIS, DPM-Solver, and UniPC), the model weights remain unchanged; improvements come purely from the numerical side.

- **Training-coupled approaches (modelâ€“based)**: The goal is to modify training so that the model itself supports fast (few-step or even single-step) generation, The key idea is to incorporate distillation, consistency constraints, or new training paradigms. Such methods require retraining or fine-tuning the model, trading additional training cost for dramatic inference speedups.


# <a id="section1.2">1.2 Flow Matching Fast Sampling: Structural Differences and New Opportunities</a>

In flow matching, the generative process is described by

$$
\frac{d x_t}{d t} = v_\theta(x_t, t),
$$

where $v_\theta$ approximates a target velocity field defined by an interpolation path between the initial distribution $p_0$ and the data distribution $p_1$. approximation error and NFE costs are still exist in flow matching inference.

But a key difference is the flow matching has **No intrinsic stiffness**: The target velocity field is derived from a user-chosen interpolation path, which can be smooth by construction. Thus, the ODE is not structurally stiff.


These structural differences mean that FM behaves like a typical Neural ODE, and  rarely needs specialized high-order solvers (the way diffusion does), most FM work simply uses off-the-shelf ODE solvers with modest step counts. The study of fast sampling in FM has been dominated by Two Major Categories.

- **Path Straightening (Trajectory Linearization)**: The core idea is that the straighter the learned transport path between data distribution and base distribution, the easier it is to approximate with fewer integration steps. The goal is to train a vector field to approximate a straight path.



- **Training-coupled approaches (modelâ€“based)**: Similar to diffusion acceleration, adjust training so that the model natively supports few-step or one-step sampling, Instead of relying on path geometry.


