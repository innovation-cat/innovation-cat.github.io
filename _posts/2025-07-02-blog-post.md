---
title: 'Fast Generation with Flow Matching'
date: 2025-07-02
excerpt: "Fast sampling has become a central goal in generative modeling, enabling the transition from high-fidelity but computationally intensive diffusion models to real-time generation systems. While diffusion models rely on tailored numerical solvers to mitigate the stiffness of their probability flow ODEs, flow matching defines dynamics through smooth interpolation paths, fundamentally altering the challenges of acceleration. This article provides a comprehensive overview of fast sampling in flow matching, with emphasis on path linearization strategies (e.g., Rectified Flow, ReFlow, SlimFlow, InstaFlow), the integration of consistency models, and emerging approaches such as flow generators."
permalink: /posts/2025/07/flow-matching-2/
tags:
  - Flow Matching
  - Acceleration
  - Consistency Models
---

<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>ðŸ“š Table of Contents</strong></span>
  </summary>
  <ul>
	<li><a href="#section1">1. From Diffusion to Flow Matching in Fast Sampling</a>
		<ul>
			<li><a href="#section1.1">1.1 The challenges of Diffusion Fast Sampling</a></li>
			<li><a href="#section1.2">1.2 Flow Matching Fast Sampling: Structural Differences and New Opportunities</a></li>
		</ul>
	</li>
	<li><a href="#section2">2. From Diffusion to Flow Matching in Fast Sampling</a>
		<ul>
			<li><a href="#section2.1">1.1 Motivation</a></li>
			<li><a href="#section2.2">1.2 Flow Matching Fast Sampling: Structural Differences and New Opportunities</a></li>
		</ul>
	</li>
  </ul>
</details>

Both Diffusion Models and Flow Matching are achieving state-of-the-art results across image, audio, and multimodal domains. Despite their success, a persistent limitation has been the large number of sampling steps required to produce high-quality outputs. This computational bottleneck has spurred intense research into few-step or even single-step generation methods, aiming to reconcile efficiency with sample quality.

In our previous articles ([Post1](https://innovation-cat.github.io/posts/2025/03/diffusion-model-3/) and [Post2](https://innovation-cat.github.io/posts/2025/03/ODE-Solver/)), we discussed the fast sampling issue of diffusion models. In this post, let's explore few-step or even single-step generation methods for flow matching.


# <a id="section1">1. From Diffusion to Flow Matching in Fast Sampling</a>

As introduced in our [previous article](https://innovation-cat.github.io/posts/2025/06/flow-matching-1/), DM and FM are two different generative paradigms, with DM being process-prioritized and FM being path-prioritized. This difference also leads to distinct optimization during fast sampling.


## <a id="section1.1">1.1 The challenges of Diffusion Fast Sampling</a>

Diffusion models sampling can be reduced to the problem of solving an PF-ODE

$$
\frac{dx_t}{dt} = f(t)x_t - \frac{1}{2}g^2(t)s_{\theta}(x_t, t)\label{eq:1}
$$

The structure of this ODE imposes severe challenges for numerical integration:

- **Stiffness**: The linear drift term $f(t)x_t$ has coefficients that vary significantly with time ($f(t)=-1/2\beta(t)$ in DDPM), while the nonlinear score term $s_\theta(x_t, t)$ exhibits large magnitude variation across the time horizon. Together, these make the PF-ODE inherently stiff. Traditional high-order ODE solvers are often unstable or ineffective in this context (small step size), leading to the development of high-order pseudo-numerical methods such as DDIM, DEIS, DPM-Solver, and UniPC.

- **Approximation error**: Unlike classical ODEs where the vector field is analytically known, the drift depends on a neural score network, which itself is an approximation. This error accumulates continuously during the iteration process, so the sampling method needs to be able to reduce error accumulation or correct it.

- **Hight cost of NFE (Number of Function Evaluations)**: In traditional ODEs, the right hand side of Eq. \ref{eq:1} is a simple analytical expression, evaluation is essentially free. However, in Diffusion Models, this function is not explicit but parameterized by a large neural network with hundreds of millions of parameters, which is computationally expensive (tens or hundreds of GFLOPs). Diffusion model sampling needs to make a trade-off between high-quality output and minimizing the number of NFE.

As a result, the study of fast sampling in diffusion has been dominated by Two Major Categories.

- **Training-free approaches (numerical solverâ€“based)**: The goal is to speed up sampling without retraining the diffusion model, the key idea is to design specialized high-order solvers that exploit the semi-linear structure of the PF-ODE while tolerating approximation noise (such as DDIM, DEIS, DPM-Solver, and UniPC), the model weights remain unchanged, improvements come purely from the numerical side.

- **Training-coupled approaches (modelâ€“based)**: The goal is to modify training so that the model itself supports fast (few-step or even single-step) generation, The key idea is to incorporate distillation, consistency constraints, or new training paradigms. Such methods require retraining or fine-tuning the model, trading additional training cost for dramatic inference speedups.


## <a id="section1.2">1.2 Flow Matching Fast Sampling: Structural Differences and New Opportunities</a>

In flow matching, the generative process is described by

$$
\frac{d x_t}{d t} = v_\theta(x_t, t),
$$

where $v_\theta$ approximates a target velocity field defined by an interpolation path between the initial distribution $p_0$ and the data distribution $p_1$. approximation error and NFE costs are still exist in flow matching inference.

But a key difference is the flow matching has **No intrinsic stiffness**: The target velocity field is derived from a user-chosen interpolation path, which can be smooth by construction. Thus, the ODE is not structurally stiff.


These structural differences mean that FM behaves like a typical Neural ODE, and  rarely needs specialized high-order solvers (the way diffusion does), most FM work simply uses off-the-shelf ODE solvers with modest step counts. The study of fast sampling in FM has been dominated by Two Major Categories.

- **Path Straightening (Trajectory Linearization)**: The core idea is that the straighter the learned transport path between data distribution and base distribution, the easier it is to approximate with fewer integration steps. The goal is to train a vector field to approximate a straight path.



- **Training-coupled approaches (modelâ€“based)**: Similar to diffusion acceleration, adjust training so that the model natively supports few-step or one-step sampling, Instead of relying on path geometry.


---

# <a id="section2">2. Straighten trajectories for Efficient Flow Matching</a>



## <a id="section2.1">2.1 Motivation</a>

The fundamental goal of path linearization in flow matching is to **reduce the geometric complexity** of the sample trajectories that connect the initial distribution $p_0$ to the data distribution $p_1$. When trajectories are highly curved, few-step ODE integration suffers from large global errors; conversely, nearly straight trajectories can be accurately captured with very few function evaluations. This phenomenon can be understood by examining the role of higher-order derivatives in numerical integration error.

---

### <a id="section2.1.1">LTE and the Role of Curvature</a>

Consider solving an ODE

$$
\frac{dx}{dt} = f(t, x)
$$

with step size $h$. The exact solution admits a Taylor expansion at $t$:

$$
x(t+h) = x(t) + h x'(t) + \tfrac{h^2}{2} x''(t) + \tfrac{h^3}{6} x^{(3)}(t) + \cdots .
$$

A first-order method such as Eulerâ€™s scheme only retains the first derivative term:

$$
x(t+h) \approx x(t) + h f(t, x(t)).
$$

The **local truncation error (LTE)** is therefore dominated by

$$
\text{LTE} \;\approx\; \tfrac{h^2}{2} x''(t) + \mathcal{O}(h^3).
$$

This shows explicitly that the second derivative $x''(t)$â€”i.e., the **curvature of the trajectory**â€”acts as a multiplicative factor in the error constant. For higher-order integrators (Rungeâ€“Kutta of order $p$), the leading error terms similarly involve derivatives up to $x^{(p+1)}(t)$. Thus, trajectories with larger curvature inevitably incur larger numerical error for the same step size.

---

### <a id="section2.1.2">Curvature in Flow Matching Trajectories</a>

In flow matching, dynamics are governed by a learned velocity field:

$$
\frac{dx_t}{dt} = v_\theta(x_t, t).
$$

The second derivative is

$$
x''(t) = \partial_t v_\theta(x_t, t) + (Dv_\theta(x_t,t)) \, v_\theta(x_t,t).
$$

If $v_\theta$ varies sharply with respect to time or state, the magnitude of $x''(t)$ becomes large, leading to trajectories that bend and oscillate. These high-curvature trajectories are intrinsically difficult to approximate with few integration steps, since the local error scales with $\|x''(t)\|$.

---

### <a id="section2.1.3">Why Path Linearization Helps</a>

Path linearization strategies aim to **design or refine the interpolation path** between $p_0$ and $p_1$ so that trajectories are as straight as possible, thereby reducing $$\|x''(t)\|$$ and higher-order derivatives. When curvature is small:

* The error constants in numerical integrators shrink,
* Few-step integration (even one-step) can approximate the full trajectory reliably,
* The sampling process is no longer limited by stiffness or oscillatory dynamics.

In other words, **straightening trajectories reduces the dependence of numerical error on higher-order derivatives**, making fast sampling feasible. This geometric perspective explains why methods such as Rectified Flow, ReFlow, and subsequent linearization techniques are effective in enabling one-step or few-step generation.



---

# <a id="section8">8. References</a>

[^Nice]: Dinh L, Krueger D, Bengio Y. Nice: Non-linear independent components estimation[J]. arXiv preprint arXiv:1410.8516, 2014.

[^nvp]: Dinh L, Sohl-Dickstein J, Bengio S. Density estimation using real nvp[J]. arXiv preprint arXiv:1605.08803, 2016.

[^Glow]: Kingma D P, Dhariwal P. Glow: Generative flow with invertible 1x1 convolutions[J]. Advances in neural information processing systems, 2018, 31.

[^Neural_ODE]: Chen R T Q, Rubanova Y, Bettencourt J, et al. Neural ordinary differential equations[J]. Advances in neural information processing systems, 2018, 31.

[^Ffjord]: Grathwohl W, Chen R T Q, Bettencourt J, et al. Ffjord: Free-form continuous dynamics for scalable reversible generative models[J]. arXiv preprint arXiv:1810.01367, 2018.

[^rectified_flow]: Liu X, Gong C, Liu Q. Flow straight and fast: Learning to generate and transfer data with rectified flow[J]. arXiv preprint arXiv:2209.03003, 2022.

[^FM]: Lipman Y, Chen R T Q, Ben-Hamu H, et al. Flow matching for generative modeling[J]. arXiv preprint arXiv:2210.02747, 2022.

[^SI]: Albergo M S, Boffi N M, Vanden-Eijnden E. Stochastic interpolants: A unifying framework for flows and diffusions[J]. arXiv preprint arXiv:2303.08797, 2023.

[^SI_1]: Albergo M S, Vanden-Eijnden E. Building normalizing flows with stochastic interpolants[J]. arXiv preprint arXiv:2209.15571, 2022.

[^SI_2]: Albergo M S, Goldstein M, Boffi N M, et al. Stochastic interpolants with data-dependent couplings[J]. arXiv preprint arXiv:2310.03725, 2023.