---
title: 'PF-ODE Sampling in Diffusion Models'
date: 2025-03-20
excerpt: "Diffusion sampling can be cast as integrating the probability flow ODE (PF-ODE), but dropping it into a generic ODE toolbox rarely delivers the best speed‚Äìquality trade-off. This post first revisits core numerical-analysis ideas. It then explains why vanilla integrators underperform on the semi-linear, sometimes stiff PF-ODE in low-NFE regimes, and surveys families that exploit diffusion-specific structure: pseudo-numerical samplers (PLMS/PNDM) and semi-analytic/high-order solvers (DEIS, DPM-Solver/++/UniPC). The goal is a practical, unified view of when and why these PF-ODE samplers work beyond ‚Äújust use RK4.‚Äù"
permalink: /posts/2025/03/ODE-Solver/
tags:
  - ODE
  - Diffusion Model
  - Numerical Computation
  - Sampling
---

<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>üìö Table of Contents</strong></span>
  </summary>
  <ul>
	<li><a href="#section1">1. Introduction</a></li>
	<li><a href="#section2">2. Numerical ODE Solver</a>
		<ul>
			<li><a href="#section2.1">2.1 One-step vs Multi-step</a>
				<ul>
				  <li><a href="#section2.1.1">2.1.1 One-step</a></li>
				  <li><a href="#section2.1.2">2.1.2 Multi-step</a></li>
				</ul>
			</li>
			<li><a href="#section2.2">2.2 Implicit vs Explicit</a>
				<ul>
				  <li><a href="#section2.2.1">2.2.1 Explicit</a></li>
				  <li><a href="#section2.2.2">2.2.2 Implicit</a></li>
				</ul>
			</li>
			<li><a href="#section2.3">2.3 Local Truncation Error and Global Truncation Error</a>
				<ul>
				  <li><a href="#section2.3.1">2.3.1 Example with Euler Method</a></li>
				  <li><a href="#section2.3.2">2.3.2 Example with Henu Method</a></li>
				</ul>
			</li>
			<li><a href="#section2.4">2.4 The Limitation of Numerical ODE Solvers for diffusion Sampling</a></li>
		</ul>
	</li>
	<li><a href="#section3">3. Pseudo-Numerical Methods</a>
		<ul>
			<li><a href="#section3.1">3.1 PLMS</a></li>
			<li><a href="#section3.2">3.2 PNDM</a></li>
		</ul>
	</li>
	<li><a href="#section4">4. High-Order and Semi-Analytic Solvers</a>
		<ul>
			<li><a href="#section4.1">4.1 DEIS</a></li>
			<li><a href="#section4.2">4.2 DPM-Solvers</a></li>
		</ul>
	</li>
	<li><a href="#section5">5. References</a></li>
  </ul>
</details>



Diffusion sampling via the probability flow ODE (PF-ODE) looks like a standard initial-value problem, but off-the-shelf integrators rarely hit the best speed‚Äìquality trade-off. This article grounds PF-ODE in core numerical-analysis notions‚Äîone-step vs. multi-step, explicit vs. implicit, and the roles of LTE and GTE‚Äîthen explains why low-NFE regimes expose stiffness, error accumulation, and training‚Äìinference mismatch for generic solvers. It organizes the landscape into pseudo-numerical samplers (PLMS, PNDM) and high-order/semi-analytic methods (DEIS, DPM-Solver/++, UniPC), highlighting the structure they exploit in the semi-linear PF-ODE. The aim is a practitioner‚Äôs map: when each family works, why ‚Äújust use RK4‚Äù leaves performance on the table, and how to pick a solver for fast, stable sampling. 


# <a id="section1">1. Introduction</a>






Diffusion models involves a forward diffusion process that gradually adds Gaussian noise to data samples until they become pure noise, and a reverse process that learns to denoise this noise back to generate new samples. 

Mathematically, the forward process can be modeled as a stochastic differential equation (SDE): 

$$dx = f(x, t) \, dt + g(t) \, dw$$ 

where $x$ is the state variable (e.g., an image), $t$ is the time step, $f(x, t)$ is the drift term, $g(t)$ is the diffusion coefficient, and $dw$ represents Wiener process. The reverse process, used for sampling, is another SDE that approximates the time-reversal of the forward SDE, incorporating a score function $\nabla_x \log p_t(x)$ (the gradient of the log-probability density), which is estimated by a neural network trained via score matching.

A crucial insight is that this reverse SDE has an equivalent deterministic representation through the probability flow ordinary differential equation (PF ODE): 

$$\frac{dx}{dt} = f(x, t) - \frac{1}{2} g(t)^2 \nabla_x \log p_t(x)$$

This equivalence stems from the fact that the PF ODE is derived to match the Fokker-Planck equation (which describes the evolution of probability densities) of the original SDE, ensuring that trajectories generated by solving the ODE backward (from $t = T$ at pure noise to $t = 0$ at clean data) produce the same marginal probability distributions as the stochastic SDE paths, but without injecting additional randomness. Thus, sampling reduces to numerically integrating this ODE, making the process deterministic and potentially more efficient, as it avoids the variance introduced by stochastic sampling while preserving the generative quality.

In this post, we first review the numerical methods for solving ODEs. Then, we analyze why we do not directly use ODE numerical solvers for sampling in diffusion models. Finally, we explore how to construct an efficient sampler based on the properties of PF-ODE for sampling.


# <a id="section2">2. Numerical ODE Solver</a>

Numerical methods convert a continuous‚Äêtime initial-value problem into a sequence of discrete algebraic updates that march the solution forward in small time steps.

Numerical ODE solvers work by discretizing the continuous time domain into a sequence of time points: $t_0, t_1, ..., t_{n-1}, t_n$, the interval between any two adjacent time steps is $h$, i,e,. $t_i=t_{i-1}+h$. Given an initial-value problem:

$$
\frac{dx}{dt}=f(t,x),\ \ \ \ x(t_0)=x_0
$$

the Fundamental Theorem of Calculus rewrites the update over one step $h$:

$$
x_{t_{i+1}}=x_{t_i}+\int_{t_i}^{t_{i+1}}f(t,x)dt
$$

Because the exact integrand $f(t,x)$ is unknown (it involves the unknown path $x$), numerical schemes replace that integral with a tractable quadrature formula  built from sample slopes. The essential difference between different numerical methods lies in the different strategies they use to approximate this integral. 

## <a id="section2.1">2.1 One-step vs Multi-step</a>

This research dimension answers the question: **"How much historical information is needed to computer $x_{t_{n+1}}$?"**

### <a id="section2.1.1">2.1.1 One-step Methods</a>

A one-step method uses only the information from the single previous point $(t_n, x_{t_n})$ to compute $x_{t_{n+1}}$, it does not care about earlier points like $x_{t_{n-1}}$, $x_{t_{n-2}}$, etc. That is to say, The information for $x_{t_{n+1}}$ is determined entirely by $x_{t_n}$ and some estimates of the slope within the interval $[t_n, t_{n+1}]$, we formalize as general form $x_{t_{n+1}}=\Phi(t_n, x_{t_n}, h)$   

Some classic numerical one-step methods are listed as follows:

|    Methods     | Order   |  NFE | Sampling Points| Update (explicit form) |
| :--------:  | :-----:  | :----:  | :--------:  | :-----:  | :----:  |
| Euler | 1 |1| $t_n $ | $$x_{t_{n+1}} = x_{t_n} + h*f(t_n, x_{t_n})$$ |
|  Heun (RK2)  | 2 |2| $$t_n \\ t_n+h$$ |  $$k_1=f(t_n, x_{t_n}) \\ k_2=f(t_n+{h}, x_{t_n}+{h}*k_1)  \\  x_{t_{n+1}}=x_{t_n}+\frac{h}{2}*(k_1+k_2)$$ |
| RK3 | 3 |3 | $$t_n \\ t_n+\frac{h}{2} \\ t_n+h$$ | $$k_1=f(t_n, x_{t_n}) \\ k_2=f(t_n+\frac{h}{2}, x_{t_n}+\frac{h}{2}k_1) \\  k_3=f(t_n+h, x_{t_n}-hk_1+2hk_2)  \\  x_{t_{n+1}}=x_{t_n}+\frac{h}{6}(k_1+4k_2+k_3)$$ |
| RK4|4 |4|  $$t_n \\ (t_n+\frac{h}{2})(2\times) \\ t_n+h$$ | $$k_1=f(t_n, x_{t_n}) \\ k_2=f(t_n+\frac{h}{2}, x_{t_n}+\frac{h}{2}k_1) \\ k_3=f(t_n+\frac{h}{2}, x_{t_n}+\frac{h}{2}k_2) \\  k_4=f(t_n+h, x_{t_n}+hk_3)  \\  x_{t_{n+1}}=x_{t_n}+\frac{h}{6}(k_1+2k_2+2k_3+k_4)$$ |

Geometrically, The integral on the right equals the signed area enclosed by the curve $f(t,x)$, the 
$t$-axis, and the vertical lines $t=t_i$ and $t=t_{i+1}$. Higher order numerical methods guarantee a better asymptotic error bound when all other factors (step size, stability, constants, arithmetic) are favourable.

![Area Approximations of 4 Common Explicit ODE Methods](/images/posts/post_4/1.png)

However, in real problems those factors often dominate, so a lower-order method can outperform a higher-order one.

### <a id="section2.1.2">2.1.2 Multi-step Methods</a>

A multi-step method uses not only the information from the single previous point $(t_n, x_{t_n})$, but also from previous points, such as $(t_{n-1}, x_{t_{n-1}}), (t_{n-2}, x_{t_{n-2}})$, etc. We formalize this as a general form $x_{t_{n+1}}=\Phi(h, f_{n}, f_{n-1}, f_{n-2},\dots)$, where $f_i = f(t_i, x_{t_i})$. By leveraging historical slope evaluations ($f$ values from prior steps), these methods can achieve higher-order accuracy with fewer new function evaluations per step, making them more efficient for problems where $f$ is expensive to compute (like neural network calls in diffusion models). However, they require an initial "bootstrap" phase (often using one-step methods to generate the first few points) and can be less stable for non-smooth or stiff problems due to error propagation from history.

Traditional multi-step methods include the Adams family (Adams-Bashforth for explicit, Adams-Moulton for implicit) and Backward Differentiation Formulas (BDF) for stiff systems.
Some classic numerical multi-step methods are listed as follows (focusing on explicit Adams-Bashforth variants for comparison with one-step methods; note that after bootstrapping, they typically require only 1 NFE per step as they reuse past $f$ values):




|    Methods     | Order   |  NFE | Sampling Points| Update (explicit form) |
| :--------:  | :-----:  | :----:  | :--------:  | :-----:  | :----:  |
|Adams-Bashforth 1 (AB1, equivalent to Euler)| 1 | 1 | $t_n$ | $x_{t_{n+1}}=x_{t_n}+hf(t_n,x_{t_n})$ |
|Adams-Bashforth 2 (AB2)| 2 | 1 | $t_n, t_{n-1}$ | $x_{t_{n+1}}=x_{t_n}+\frac{h}{2}[3f(t_n,x_{t_n})‚àíf(t_{n‚àí1},x_{t_{n‚àí1}})]$ |
|Adams-Bashforth 3 (AB3) | 3 |1|$t_n, t_{n-1}, t_{n-2}$| $x_{t_{n+1}}=x_{t_n}+\frac{h}{12}[23f(t_n,x_{t_n})‚àí16f(t_{n‚àí1},x_{t_{n‚àí1}})+5f(t_{n‚àí2},x_{t_{n‚àí2}})]$ |
|Adams-Bashforth 4 (AB4)|4|1| $t_n, t_{n-1}, t_{n-2}, t_{n-3}$|$x_{t_{n+1}}=x_{t_n}+\frac{h}{24}[55f(t_n,x_{t_n})‚àí59f(t_{n‚àí1},x_{t_{n‚àí1}})+37f(t_{n‚àí2},x_{t_{n‚àí2}})‚àí9f(t_{n‚àí3},x_{t_{n‚àí3}})]$ |

Geometrically, multi-step methods approximate the integral by fitting a polynomial interpolant through multiple past slope points, extrapolating forward. This can provide a more accurate area estimate under the curve $f(t,x)$ compared to one-step methods, especially for smooth functions, but it assumes the history is reliable‚Äîerrors in early steps can compound over time.

The following figures Approximate $\int_{t_n}^{t_{n+1}}f(s, x(s))ds$ by integrating the Lagrange interpolant. The shaded region is the predicted area that advances the solution.

![Area Approximations of 4 ABK Methods](/images/posts/post_4/2.png)


However, in practice, multi-step methods shine in scenarios with consistent step sizes and non-stiff ODEs, as they reduce computational overhead by recycling evaluations. For diffusion PF-ODEs, their efficiency inspires pseudo-multi-step samplers (e.g., PLMS), but stiffness often necessitates hybrid approaches with correctors or adaptive stepping.

## <a id="section2.2">2.2 Implicit vs Explicit</a>

This research dimension aims to answer the question: "Is the formula for $x_{t_{n+1}}$ is a direct calculation or an equation to be solved?", that is to say, whether $x_{t_{n+1}}$ appears on both sides of the equation simultaneously.

### <a id="section2.2.1">2.2.1 Explicit</a>

Explicit methods refers to the formula for $x_{t_{n+1}}$ is an explicit expression, where the unknown  $x_{t_{n+1}}$ appears only on the left-hand side. We can directly plug in known values on the right-hand side to compute $x_{t_{n+1}}$.

$$
x_{t_{n+1}}=x_{t_n}+h\Phi(t_n, x_{t_n}, f_n, f_{n-1}, \dots)
$$

Where $f_n=f(t_n, x_{t_n})$. Examples include Forward Euler, explicit Runge‚ÄìKutta (Heun/RK3/RK4), and Adams‚ÄìBashforth (AB) multi-step schemes


### <a id="section2.2.2">2.2.2 Implicit</a>

Implicit methods refers to the formula for  $x_{t_{n+1}}$ is an equation, where the unknown $x_{t_{n+1}}$  appears on both sides of the equals sign. Typical examples including backward euler method, Adams-Moulton families.

$$
x_{t_{n+1}}=x_{t_n}+h\Phi(t_{n+1}, x_{t_{n+1}}; t_n, x_{t_n}, f_n, f_{n-1}, \dots)
$$


An implicit method requires solving a (potentially nonlinear) equation at each step to find $x_{t_{n+1}}$, often using iterative techniques like Newton-Raphson. This increases computational cost per step but enhances stability, making them suitable for stiff ODEs common in diffusion models.

Some classic implicit numerical methods are listed as follows, focusing on Runge-Kutta family examples (which can be one-step; multi-step implicit methods like Adams-Moulton are also common):


|Methods |Order |NFE (per iteration)|Sampling Points|Update (implicit form)|
|Backward Euler | 1 | 1 (plus solver iterations)| $t_{n+1}$|$x_{t_{n+1}} = x_{t_n} + h \cdot f(t_{n+1}, x_{t_{n+1}})$|
|Trapezoidal (Implicit RK2)|2|1 (plus solver iterations)|$t_n \\ t_{n+1}$|$x_{t_{n+1}} = x_{t_n} + \frac{h}{2} \left[ f(t_n, x_{t_n}) + f(t_{n+1}, x_{t_{n+1}}) \right]$|
|Implicit Midpoint (Gauss-Legendre RK2)| 2 | 1 (plus solver iterations) | $t_n + \frac{h}{2}$ | $k = f\left(t_n + \frac{h}{2}, x_{t_n} + \frac{h}{2} k\right) \\ x_{t_{n+1}} = x_{t_n} + h k$|
|Radau IIA (Implicit RK3)|3|2 (plus solver iterations)|$t_n + \frac{h}{3} \\ t_n + h$|Solve for $k_1, k_2$: \\ $k_1 = f\left(t_n + \frac{h}{3}, x_{t_n} + \frac{5h}{12} k_1 - \frac{h}{12} k_2\right) \\ k_2 = f\left(t_n + h, x_{t_n} + h k_1 + h k_2 - h k_1\right)$ (simplified Butcher tableau form; full details in literature) \\ $x_{t_{n+1}} = x_{t_n} + \frac{3h}{4} k_1 + \frac{h}{4} k_2$|


--- 


## <a id="section2.3">2.3 Local Truncation Error and Global Truncation Error</a>

In numerical methods for solving ordinary differential equations (ODEs), errors arise because we approximate the continuous solution with discrete steps. Two key concepts are the Local Truncation Error (LTE) and the Global Truncation Error (GTE), which are the common indicators used to measure errors.

-	**Local Truncation Error (LTE):** This is the error introduced in a single step of the numerical method, assuming the solution at the previous step is exact. It measures how well the method approximates the true solution over one step, based on the Taylor series expansion of the exact solution. 
	
	Mathematically, if the exact solution at $t_n$ is $x(t_n)$, the LTE at step $n+1$ is:
	
	$$
	\tau_{n+1} = x(t_{n+1}) - x_{n+1}^{\text{approx}}
	$$
	
	where $x_{n+1}^{\text{approx}}$ is computed using the method with exact input $x(t_n)$. 

- **Global Truncation Error (GTE):** This is the total accumulated error at the end of the integration interval (e.g., from $t_0$ to $T$), considering that errors from previous steps propagate forward. It depends on the number of steps $N = (T - t_0)/h$, and for stable methods, GTE is typically $O(h^p)$ if LTE is $O(h^{p+1})$. The relationship is roughly GTE $\approx$ (number of steps) $\times$ LTE, but propagation can amplify or dampen it.

Below, we will use the Euler method and the Henu method as examples to demonstrate how to obtain its LTE and GTE.


---

### <a id="section2.3.1">2.3.1 Example with Euler Method</a>
The forward Euler method is a first-order method:

$$x_{n+1} = x_n + h f(t_n, x_n)$$

#### Deriving LTE
- Assume exact input: Start with $x_n = x(t_n)$.

- The method approximates: $x_{n+1}^{\text{approx}} = x(t_n) + h f(t_n, x(t_n))$.

- From Taylor: $x(t_n + h) = x(t_n) + hf(t_n, x(t_n)) + \frac{h^2}{2} x''(t_n) + O(h^3)$.

- Subtract: LTE $\tau_{n+1} = x(t_n + h) - x_{n+1}^{\text{approx}} = \frac{h^2}{2} x''(t_n) + O(h^3)$.

- Thus, **LTE = $O(h^2)$**. (The leading term is quadratic in $h$.)

This shows how to arrive: The Euler update matches the first two Taylor terms (constant + linear), so the error starts from the quadratic term.

#### Deriving GTE

- Let $e_n = x(t_n) - x_n$ be the global error at step $n$.

- The error recurrence: $e_{n+1} = e_n + h [f(t_n, x(t_n)) - f(t_n, x_n)] + \tau_{n+1}$.

- For Lipschitz-continuous $f$ (with constant $L$), $\|f(x(t_n)) - f(x_n)\| \leq L \|e_n\|$, so:
$$\|e_{n+1}\| \leq \|e_n\| (1 + h L) + \|\tau_{n+1}\|$$

- Since $\tau_{n+1} = O(h^2)$, over $N$ steps ($N \approx 1/h$), the accumulated error bounds to $\|e_N\| \leq C h$ for some constant $C$ (from summing the geometric series of propagated local errors).

- Thus, GTE = $O(h)$. (Errors accumulate linearly with $1/h$ steps, reducing the order by 1.)

Cause: Each local error adds up, and propagation (via $1 + hL$) amplifies like compound interest, but stability ensures it's bounded by $O(h)$.

### <a id="section2.3.2">2.3.2 Example with Heun Method</a>

The Heun method (a second-order Runge-Kutta) improves on Euler with a predictor-corrector:

$$k_1 = f(t_n, x_n), \quad k_2 = f(t_n + h, x_n + h k_1), \quad x_{n+1} = x_n + \frac{h}{2} (k_1 + k_2)$$

#### Deriving LTE

- Assume exact input: $x_n = x(t_n)$.

- Expand $k_1 = f(t_n, x(t_n)) = x'(t_n)$.

- Predictor: $x_n + h k_1 = x(t_n) + h x'(t_n)$.

-  $k_2 = f(t_n + h, x(t_n) + h x'(t_n))$. Taylor-expand $f$ in two variables:

   $$
   \begin{align}
   f(t_n + h, x(t_n) + h x'(t_n)) = & f(t_n, x(t_n)) + h \left( f_t + x'(t_n) f_x \right) \\[10pt] & + \frac{h^2}{2} \left( f_{tt} + 2 x' f_{tx} + (x')^2 f_{xx} \right) + O(h^3)
   \end{align}
   $$ 
   
   But since $x'' = \frac{\partial f}{\partial t} + x' \frac{\partial f}{\partial x}$, $k_2 = x' + h x'' + \frac{h^2}{2} x''' + O(h^3)$.

- Then, 
 $$
 \begin{align}
 x_{n+1}^{\text{approx}} & = x(t_n) + \frac{h}{2} (x' + x' + h x'' + \frac{h^2}{2} x''' + O(h^3)) \\[10pt] & = x(t_n) + h x' + \frac{h^2}{2} x'' + \frac{h^3}{4} x''' + O(h^4)
 \end{align}
 $$

- True: $x(t_n + h) = x(t_n) + h x' + \frac{h^2}{2} x'' + \frac{h^3}{6} x''' + O(h^4)$.

- Subtract: LTE $\tau_{n+1} = \left( \frac{h^3}{6} - \frac{h^3}{4} \right) x''' + O(h^4) = -\frac{h^3}{12} x''' + O(h^4)$.
Thus, LTE = $O(h^3)$. (Matches up to quadratic term, error from cubic.)

#### Deriving GTE

-  Similar recurrence: 

   $$
   \begin{align}
   e_{n+1} =\  & e_n + \frac{h}{2} [ (f(t_n, y(t_n)) - f(t_n, y_n))] \\[10pt] &+ \frac{h}{2}[(f(t_{n+1}, y(t_{n+1})) - f(t_{n+1}, y_n + h k_1)) ] + \tau_{n+1}
   \end{align}
   $$

- Using Lipschitz $L$, the perturbation terms are $O(e_n)$ and $O(h e_n + e_n)$, leading to $\|e_{n+1}\| \leq (1 + h L + O(h^2)) \|e_n\| + \|\tau_{n+1}\|$.

- With $\tau_{n+1} = O(h^3)$, over $N \approx 1/h$ steps, the bound sums to $\|e_N\| \leq C h^2$ (accumulated as $N \times O(h^3) = O(h^2)$).

- Thus, GTE = $O(h^2)$. (Higher local order leads to better global convergence.)

Cause: Fewer accumulations needed for the same accuracy, and the method's stability (A-stable for some cases) prevents excessive propagation.

In summary, LTE is per-step (higher order means smaller), while GTE is overall (reduced by 1 order due to accumulation). For Euler (order 1), halving $h$ halves GTE; for Heun (order 2), it quarters GTE‚Äîmaking higher-order methods more efficient for accuracy.


## <a id="section2.4">2.4 The Limitation of Numerical ODE Solvers for diffusion Sampling</a>

Although the PF ODE formulation enables deterministic sampling, directly applying standard numerical ODE solvers (e.g., explicit Runge-Kutta methods like RK4, Euler methods, or adaptive solvers like Dormand-Prince) is often suboptimal or problematic for diffusion models. These general-purpose solvers do not fully exploit the specific structure of diffusion ODEs, leading to inefficiencies and quality issues, particularly in high-dimensional spaces like image generation. Below, I list the key reasons, drawn from analyses of diffusion sampling challenges:

- Numerical Instability and Stiffness Issues: Diffusion PF ODEs are semi-linear and can be stiff, especially in high dimensions, causing standard explicit solvers to become unstable with large step sizes. This results in exploding gradients or divergence, requiring tiny steps that increase computational cost and negate speed advantages.

- High Discretization Errors in Few-Step Regimes: With fewer integration steps (e.g., 10‚Äì20 instead of 1000), traditional solvers accumulate significant truncation errors, causing the approximated trajectory to deviate from the true ODE path. This leads to degraded sample quality, such as artifacts or lower fidelity, as the solver fails to accurately track the probability flow.

- Mismatch with Model Training Objectives: Diffusion models are trained to optimize score-matching losses, not to minimize ODE integration errors. Pursuing better ODE solving can paradoxically worsen perceptual quality, as seen in consistency models where tighter approximations to the PF ODE reduce sample fidelity due to inconsistencies between training and inference.

- Inefficiency for Guided or Conditional Sampling: Standard solvers do not inherently handle constraints like classifier guidance or conditional generation efficiently, often requiring additional modifications that increase function evaluations (NFEs) or fail to maintain distribution matching.

- Lack of Exploitation of Semi-Linear Structure: Diffusion ODEs have a specific semi-linear form (linear drift plus nonlinear score term), which general solvers ignore, leading to suboptimal performance. Without tailored approximations, they require more NFEs for convergence, making them slower than specialized methods.

These problems motivate the development of specialized solvers that incorporate higher-order approximations, exploit the ODE's structure, and provide convergence guarantees for fast, high-quality sampling in low-NFE settings.



# <a id="section3">3. Pseudo-Numerical Methods for Diffusion Models</a>

PF-ODEs in diffusion have a semi-linear form

$$
\frac{dx}{dt}=a(t)\,x + b(t)\,\underbrace{\text{model\_out}(x,t)}_{\epsilon_\theta,\ s_\theta,\ \text{or } v_\theta}.
$$

The linear drift $a(t)\,x$ is handled **analytically** via the training schedule (the $\alpha,\bar\alpha$ coefficients), so the step-to-step state update can be written as a **closed one-step map** (DDIM/‚Äúprobability-flow‚Äù style) that is *linear* in the model output for the current time. What we improve, then, is **only the model output at the current step**‚Äîusing multistep extrapolation/correction‚Äîbefore plugging it into that one-step map.

A convenient way to read this is: we apply classical multistep ideas **not to the whole RHS $f$**, but to the **neural part**; the linear part is already folded into the schedule-dependent coefficients. Mathematically this matches an **integrating-factor / exponential-integrator** view of Sec. 2: treat the linear term exactly, raise the order on the remaining nonlinearity.

**One-line template (Œµ-parameterization):**

$$
\hat\epsilon_t \;\leftarrow\; \text{(multistep combination of past }\epsilon_{\theta}) ,
\qquad
x_{t-\Delta}
= \Psi\!\left(x_t,\ \hat\epsilon_t;\ \bar\alpha_t,\bar\alpha_{t-\Delta}\right)
$$

where $\Psi$ is the standard deterministic DDIM-style update (a linear map of $(x_t,\hat\epsilon_t)$ with schedule coefficients). The two samplers below differ only in **how $\hat\epsilon_t$ is formed**. ([arXiv][2])


> Mental model: *treat the linear part exactly, raise the order on the neural part*. This is the unifying principle behind the **PNDM family**.

## <a id="section3.1">3.1 PLMS</a>

## <a id="section3.2">3.2 PNDM</a>


# <a id="section4">4. High-Order and Semi-Analytic Solvers</a>


## <a id="section4.1">4.1 DEIS</a>

## <a id="section4.2">4.2 DPM-Solvers</a>

## <a id="section4.3">4.3 DPM-Solver++</a>

## <a id="section4.4">4.4 UniPC</a>


---

# <a id="section5">References</a>

[^Parisi]: Parisi G. Correlation functions and computer simulations[J]. Nuclear Physics B, 1981, 180(3): 378-384.

[^Grenander]: Grenander U, Miller M I. Representations of knowledge in complex systems[J]. Journal of the Royal Statistical Society: Series B (Methodological), 1994, 56(4): 549-581.

[^Aapo]: Aapo Hyv√§rinen, ‚ÄúEstimation of non-normalized statistical models by score matching‚Äù, JMLR, 2005.

[^Song_2019]: Yang Song and Stefano Ermon. "Generative Modeling by Estimating Gradients of the Data Distribution". NeurIPS 2019.

[^ddpm]: Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[J]. Advances in neural information processing systems, 2020, 33: 6840-6851.

[^ddim]: Song J, Meng C, Ermon S. Denoising diffusion implicit models[J]. arXiv preprint arXiv:2010.02502, 2020.

[^Vincent]: Vincent P. A connection between score matching and denoising autoencoders[J]. Neural computation, 2011, 23(7): 1661-1674.