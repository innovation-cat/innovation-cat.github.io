---
title: 'Analysis of the Stability of Diffusion Model Training'
date: 2025-01-23
permalink: /posts/2025/01/diffusion-model-2/
tags:
  - cool posts
  - category1
  - category2
---

Diffusion models have achieved unprecedented success in the field of generative modeling, producing incredibly high-fidelity images, audio, and other data. However, the training process of these models presents several unique challenges like divergence, vanishing gradients, or unstable training behavior during the learning process. A stable training process ensures that the model produces good quality samples and converges efficiently over time without suffering from numerical instabilities.

Training diffusion models can be challenging and unstable due to several factors:

  - **Noise scheduling**, which controls how noise is injected during the forward process and removed during the reverse process. The choice of noise schedule (linear, cosine, or others) directly impacts training stability. In particular, schedules that introduce large or small variations in noise at different time steps can lead to vanishing or exploding gradients. 
  
  - **Prediction Target**, 

---

# Brief recap

For generative models, we expect our model $p_{\theta}$ (parameterized by $\theta$) to be as close as possible to the true distribution $p_{data}$. Based on the KL divergence, we derive that

$$
\mathbb{KL}(p_{data}(x) \parallel p_{\theta}(x)) = \int p_{data}(x)\log (p_{data}(x))dx - \int p_{data}(x)\log(p_{\theta}(x))dx
$$

The first term, $\int p_{data}(x) \log (p_{data}(x))dx$, is the entropy of the true distribution
$p_{data}$, it is constant with respect to the model parameters $\theta$. The second term, $\int p_{data}(x)\log(p_{\theta}(x))dx$, is the expected log-likelihood of the model under the true distribution. Thus, minimizing KL divergence is equal to log-likelihood $p_{\theta}(x)$, where $x  \sim p_{data}$.

## From Maximum likelihood to ELBO

Let $x_0$ be the original image, and $x_i (i=1,2,...,T)$ be the image with noise added to $x_0$. We wish to maximise 

$$
\log p_{\theta}(x_0)=\log \int p_{\theta}(x_{0:T}) dx_{1:T}
$$

Introduce the forward process $q_{\phi}(x_{1:T} \mid x_0)$(a Markov chain with fixed noise‑schedule). Using Jensen’s inequality gives the evidence lower bound:

$$
\begin{align}
\log p_\theta(x_0) \geq \mathcal{L}_\text{ELBO} & = \mathbb{E}_q \left[ \log p_\theta(x_0 \mid x_1) - \log \frac{q(x_{T} \mid x_0)}{p_\theta(x_{T})} - \sum_{t=2}^T \log \frac{q(x_{t-1} \mid x_t, x_0)}{p_\theta(x_{t-1} \mid x_t)} \right]  
\end{align}
$$

The first term is reconstruction loss, the second term is prior matching, Therefore, what we are truly concerned about is the third item, which also known as denoising term.


## From KL collapses to a mean MSE

For each denoising step, both forward posterior $q_{\phi}(x_{t-1} \mid x_t, x_0)$ and backward posterior 
$p_{\theta}(x_{t-1} \mid x_t)$ are gaussian distributions, For two Gaussians with identical covariance:

$$
KL = \frac{1}{2\beta_t} \|\tilde{\mu}_t - \mu_\theta(x_t, t)\|_2^2 + \text{const}.
$$

Hence, for each denoising step, the loss function equals to the 
$$
\mathcal{L}_{\text{denoise}} =  \mathbb{E}_q \left[ \frac{1}{2\beta_t} \|\tilde{\mu}_t - \mu_\theta(x_t, t)\|^2 \right].
$$

Hence, for each denoising step. $q_{\phi}(x_{t-1} \mid x_t, x_0)=\frac{q_{\phi}(x_{t} \mid x_{t-1})q_{\phi}(x_{t-1} \mid x_{0})}{q_{\phi}(x_{t} \mid x_{0})} \propto q_{\phi}(x_{t} \mid x_{t-1})q_{\phi}(x_{t-1} \mid x_{0})$. For $q_{\phi}(x_{t} \mid x_{t-1})$, we have:

$$
q_{\phi}(x_{t} \mid x_{t-1}) = \mathcal{N}(\mu_q, \sigma_q^2I)
$$

For $q_{\phi}(x_{t-1} \mid x_{0})$, we have:






---

# Training Objective


All prediction can be rewrited as: 

$$
\mathcal{L}=\mathbb{E}\parallel M_{\theta}(x_t, t) - target \parallel ^2
$$

The three targets are linearly related,

## $\epsilon$-prediction


## $x_0$-prediction


## $v$-prediction

## score-prediction 

## what is the different


 
---

# Noise Schedule and SNR 

---

# Conclusion

---

# References


