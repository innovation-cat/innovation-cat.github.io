---
title: 'Analysis of the Stability of Diffusion Model Training'
excerpt: "while diffusion models have revolutionized generative AI, their training challenges stem from a combination of resource intensity, optimization intricacies, and deployment hurdles. A stable training process ensures that the model produces good quality samples and converges efficiently over time without suffering from numerical instabilities."
date: 2025-01-23
permalink: /posts/2025/01/diffusion-model-2/
tags:
  - Diffusion Model
  - SNR
  - Model Training
---

<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>üìö Table of Contents</strong></span>
  </summary>
  <ul style="margin:0; padding-left:1.1rem;">
    <li><a href="#section1">1. Preliminary</a>
		<ul>
		  <li><a href="#section1.1">1.1 From Maximum likelihood to ELBO</a></li>
		  <li><a href="#section1.2">1.2 From KL collapses to a mean MSE</a></li>
		</ul>
	</li>
    <li><a href="#section2">2. Re-parameterising the mean with different target predictor</a>
		<ul>
		  <li><a href="#section2.1">2.1 x<sub>0</sub>-prediction</a></li>
		  <li><a href="#section2.2">2.2 &epsilon;-prediction</a></li>
		  <li><a href="#section2.3">2.3 v-prediction</a></li>
		  <li><a href="#section2.4">2.4 Score-prediction</a></li>
		</ul>
	</li>
	<li><a href="#section3">3. A Unified Analysis: Why Targets Differ in Practice</a>
		<ul>
			<li><a href="#section3.1">3.1 Correlation: Measuring Task Difficulty</a></li>
			<li><a href="#section3.2">3.2 Conditional Mean: Measuring Signal Strength</a></li>
			<li><a href="#section3.3">3.3 Conditional Variance: Measuring Gradient Stability</a></li>
			<li><a href="#section3.4">3.4 Summary and Comparative Table</a></li>
		</ul>
	</li>	
	<li><a href="#section4">Amplification of Network Prediction Errors Over Timesteps</a></li>
	<li><a href="#section5">Conclusion</a></li>
	<li><a href="#section6">References</a></li>
  </ul>
</details>


Diffusion models have achieved remarkable success in generative modeling, producing state‚Äëof‚Äëthe‚Äëart results in image, audio, and multimodal generation. Yet training them remains notoriously difficult. Instabilities such as vanishing gradients, exploding loss values, and imbalanced learning across timesteps often hinder efficient convergence.

At the core of these issues lies the choice of **training objective**. Although the canonical derivation from maximum likelihood leads naturally to an evidence lower bound (ELBO) and finally to a mean‚Äësquared error (MSE) objective, how we reparameterize the regression target strongly affects optimization dynamics. Four parameterizations are commonly used in practice‚Äî$x_0$, $\epsilon$, $v$, and score prediction. They are mathematically equivalent but exhibit different levels of stability, gradient flow, and ease of optimization.

This post focus on diffusion models training - why diffusion training is unstable, how the four objectives differ, and how modern solutions re‚Äëbalance training dynamics.

---

# <a id="section1">1. Preliminary</a>

For generative models, we expect our model $p_{\theta}$ (parameterized by $\theta$) to be as close as possible to the true distribution $p_{data}$. Based on the KL divergence, we derive that

$$
\mathbb{KL}(p_{data}(x) \parallel p_{\theta}(x)) = \int p_{data}(x)\log (p_{data}(x))dx - \int p_{data}(x)\log(p_{\theta}(x))dx\label{eq:1}
$$

The first term, $\int p_{data}(x) \log (p_{data}(x))dx$, is the entropy of the true distribution
$p_{data}$, it is a constant with respect to the model parameters $\theta$. The second term, $\int p_{data}(x)\log(p_{\theta}(x))dx$, is the expected log-likelihood of the model under the true distribution. Thus, minimizing KL divergence is equal to maximize log-likelihood $p_{\theta}(x)$, where $x \sim p_{data}$.

---

## <a id="section1.1">1.1 From Maximum likelihood to ELBO</a>

Let $x_0$ be the original image, and $x_i (i=1,2,...,T)$ be the image with noise added to $x_0$. We wish to maximise 

$$
\log p_{\theta}(x_0)=\log \int p_{\theta}(x_{0:T}) dx_{1:T} \label{eq:2}
$$

Introduce the forward process $q(x_{1:T} \mid x_0)$ (a Markov chain with fixed noise‚Äëschedule). Using Jensen‚Äôs inequality gives the evidence lower bound:

$$
\begin{align}
\log p_\theta(x_0) \geq \mathcal{L}_\text{ELBO} & = \mathbb{E}_q \left[ \log p_\theta(x_0 \mid x_1) - \log \frac{q(x_{T} \mid x_0)}{p_\theta(x_{T})} - \sum_{t=2}^T \log \frac{q(x_{t-1} \mid x_t, x_0)}{p_\theta(x_{t-1} \mid x_t)} \right]\label{eq:3}
\end{align}
$$

The first term is reconstruction loss, the second term is prior matching, both of them are extremely small and can be ignored. Therefore, what we are truly concerned about is the third item, which also known as denoising term.

---

## <a id="section1.2">1.2 From KL collapses to a mean MSE</a>

For each denoising step, both forward posterior $q(x_{t-1} \mid x_t, x_0) \sim \mathcal{N}(\mu_{q}, \sigma_{q}^2I)$ and backward posterior
$p_{\theta}(x_{t-1} \mid x_t) \sim \mathcal{N}(\mu_{\theta}, \sigma_{\theta}^2I)$ are gaussian distributions. For two Gaussians with identical covariance, if we fix the two variances are equal to $\sigma_{q}^2$, then the KL divergence is equal to:

$$
\mathbb{KL}\left(q(x_{t-1} \mid x_t, x_0) \parallel p_{\theta}(x_{t-1} \mid x_t) \right) = \frac{1}{2\sigma_q^2} \|{\mu}_{q} - \mu_{\theta}(x_t, t)\|_2^2 + \text{const}\label{eq:4}
$$

Hence, for each denoising step, the loss function equals to

$$
\mathcal{L}_{\text{denoise}} =  \mathbb{E}_q \left[ \|\mu_q - \mu_{\theta}(x_t, t)\|^2 \right]\label{eq:5} 
$$

$\mu_q$ is the true target we want to predict, How do we calculate the value of 
$\mu_q$? Let's first decompose forward posterior $q(x_{t-1} \mid x_t, x_0)$ :

$$
q(x_{t-1} \mid x_t, x_0)=\frac{q(x_{t} \mid x_{t-1})q(x_{t-1} \mid x_{0})}{q(x_{t} \mid x_{0})} \propto q(x_{t} \mid x_{t-1})q(x_{t-1} \mid x_{0})\label{eq:6}
$$

where 

$$
q(x_{t} \mid x_{t-1}) \sim \mathcal{N}(x_{t-1};\mu_1, \sigma_1^2I),\ \ \mu_1=\frac{1}{\sqrt{\alpha_t}}x_{t},\ \ \sigma_1^2=\frac{1-\alpha_t}{\alpha_t} \\[10pt] q(x_{t-1} \mid x_{0})  \sim \mathcal{N}(x_{t-1};\mu_2, \sigma_2^2I), \ \ \mu_2=\sqrt{\bar \alpha_{t-1}}x_{0},\ \  \sigma_2^2=1-\bar \alpha_{t-1}\label{eq:7}
$$

The product of two Gaussian distributions is itself a Gaussian distribution, 




$$
\mu_{q} = \frac{\mu_1\sigma_2^2+\mu_2\sigma_1^2}{\sigma_1^2+\sigma_2^2}  = \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1}) x_t + \sqrt{\bar{\alpha}_{t-1}} (1 - \alpha_t) {x}_0}{1 - \bar{\alpha}_t}\label{eq:8}
$$

Combining equations \ref{eq:5} and \ref{eq:8}, Our goal is to construct a neural network $\mu_{\theta}$, which takes $x_t$ and $t$ as inputs, such that the output of the network is as close as possible to $\mu_q$.


--- 

# <a id="section2">2. Re-parameterising the mean with different target predictor</a>

Following equation \ref{eq:8}, we can dirrectly build a network $\mu_{\theta}$ to output $\mu_{q}$. However, in practice, we usually do not directly fit the value of $\mu_{q}$, mainly due to the following reasons.

- $\mu_{q}$ is an affine function of $x_t$, which is known at training and test time, there is no need for the network to ‚Äúreproduce‚Äù it. If we regress $\mu_{q}$ directly, the network wastes capacity relearning a large known term and must also learn the residual that actually depends on the unknown clean content. 

- The mean target value is highly time-dependent scaling across $t$, which means that the output of the network is unstable, it is usually extremely difficult for a network to output results with a large variance range.


Instead of asking the network to output $\mu_{q}$ directly, the community typically uses four common prediction targets to train diffusion models: $\epsilon$-prediction, $x_0$-prediction, $v$-prediction, score-prediction. If we regard the original image $x_0$ and noise $\epsilon$ as two orthogonal dimensions, then All the common targets are linear in $(x_0, \epsilon)$


## <a id="section2.1">2.1 $x_0$-prediction (aka sample-prediction in Diffusers)</a>

In $x_0$-prediction, the neural network is trained to directly estimate the clean original data $x_0$ from the noisy input $x_t$ at timestep $t$. Denoted the network as $ x_{\theta}(x_t, t)$, and the predicted output is $$\hat{x}_0$$, this approach reparameterizes the predicted mean $\mu_{\theta}$ using the estimated $$\hat{x}_0$$. Substituting into Equation \ref{eq:8}, the mean becomes:

$$
\mu_{\theta}(x_t, t) = \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1}) x_t + \sqrt{\bar{\alpha}_{t-1}} (1 - \alpha_t) \hat{x}_0}{1 - \bar{\alpha}_t}\label{eq:9}
$$

The loss function then simplifies to minimizing the MSE between the true $x_0$ and the predicted $\hat{x}_0$:

$$
\mathcal{L} = \mathbb{E}_{x_0, t, \epsilon} \left[ \| x_0 - x_{\theta}(x_t, t) \|^2 \right]\label{eq:10}
$$

**Pros**

This parameterization is advantageous because the target $x_0$ has a consistent scale across timesteps, reducing the variance in network outputs and improving training stability. 

**Cons**

The primary drawback of $x_0$-prediction lies in the uneven learning difficulty across signal-to-noise ratio (SNR) regimes, which induces heterogeneous gradient behaviors and ultimately hinders training convergence. 


## <a id="section2.2">2.2 $\epsilon$-prediction</a>

The $\epsilon$-prediction paradigm tasks the network with predicting the noise $\epsilon$ added during the forward process. This parameterization leverages the forward noising equation to express the clean data $x_0$ in terms of the noise $\epsilon$ via a simple linear transformation (equation \ref{eq:11}). 

$$
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon \  \Longrightarrow \  x_0=\frac{x_t-\sqrt{1 - \bar{\alpha}_t} \epsilon}{\sqrt{\bar{\alpha}_t}}\label{eq:11}
$$ 

Substituting this reparameterized form of $x_0$ (with $\hat{\epsilon}$ in place of $\epsilon$) into Equation \ref{eq:8} for the mean gives:

$$
\mu_{\theta}(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \hat{\epsilon} \right)\label{eq:12}
$$

The loss function then simplifies to minimizing the MSE between the true noise $\epsilon$ and the predicted $\hat{\epsilon}$:

$$
\mathcal{L} = \mathbb{E}_{x_0, t, \epsilon} \left[ \| \epsilon - \epsilon_{\theta}(x_t, t) \|^2 \right]\label{eq:13}
$$

**Pros**

This parameterization is advantageous because the target $\epsilon$ is timestep-independent target distribution since $\epsilon \sim \mathcal{N}(0, I)$.

**Cons**

Like $x_0$-prediction, $\epsilon$-prediction also suffers from the uneven learning difficulty problems, which needs re-weighting $ w(t) $ to promote balanced learning across the noise spectrum. We will conduct more in-depth analysis in the following sections.



## <a id="section2.3">2.3 $v$-prediction</a>

Velocity ($v$)-prediction combines elements of both $x_0$ and $\epsilon$ predictions by forecasting a velocity term $v$ that interpolates between them. Defined velocity as $$v = \sqrt{\bar{\alpha}_t} \epsilon - \sqrt{1 - \bar{\alpha}_t} x_0$$ (or its normalized variant), the network predicts $$\hat{v} = v_{\theta}(x_t, t)$$. The mean $\mu_{\theta}$ can be expressed in terms of $$\hat{v}$$:

$$
\mu_{\theta}(x_t, t) = \sqrt{\alpha_t}x_t- \frac{(1-\alpha_t)\sqrt{\bar \alpha_{t-1}}}{\sqrt{1-\bar \alpha_t}}{\hat v}\label{eq:14}
$$

The loss function then simplifies to minimizing the MSE between the true velocity $v$ and the predicted $$\hat{v}$$:

$$
\mathcal{L} = \mathbb{E}_{x_0, t, \epsilon} \left[ \| v - v_{\theta}(x_t, t) \|^2 \right]\label{eq:15}
$$

This hybrid approach adapts dynamically to timestep: in high SNR regimes, it behaves like $\epsilon$-prediction, while in low SNR regimes, it resembles $x_0$-prediction. This adaptability enhances stability by balancing gradient flows across SNR levels, reducing divergence risks, but requires precise calibration of the velocity formulation to avoid numerical instabilities during backpropagation.


## <a id="section2.4">2.4 Score-prediction</a>

This parameterization draws from the score-based generative modeling framework, where the neural network estimates the score function $s_{\theta}(x_t, t) = \nabla_{x_t} \log p_t(x_t)$, representing the gradient of the log-probability density at the noisy state $x_t$. In Gaussian diffusion models, the score is directly related to the noise via $$s(x_t, t) = -\frac{\epsilon}{\sqrt{1 - \bar \alpha_t}}$$, allowing a re-expression of the noise $\epsilon$ in terms of the score. Starting from the forward noising equation and substituting the equivalent form $$\epsilon = -\sqrt{1 - \bar{\alpha}_t} \, s(x_t, t)$$, the predicted mean $$\mu_{\theta}$$ is derived by inserting this into Equation \ref{eq:8}:

$$
\mu_{\theta}(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t + (1 - \alpha_t) \, s_{\theta}(x_t, t) \right)
$$

The loss function simplifies to minimizing the MSE between the true score $s(x_t, t)$ and the predicted $$\hat{s} = s_{\theta}(x_t, t)$$:

$$\mathcal{L} = \mathbb{E}_{x_0, t, \epsilon} \left[ \| s(x_t, t) - s_{\theta}(x_t, t) \|^2 \right]$$

**Pros**
Integrated in models like Score-Based Generative Models (SGMs) and aligned with continuous-time formulations, this approach facilitates theoretical connections to stochastic differential equations (SDEs) and enhances generalization across noise schedules. 

**Cons**
However, it may introduce scaling sensitivities in discrete timesteps, potentially leading to instabilities if the score magnitudes are not properly normalized, necessitating adaptive weighting or variance adjustments during training. In general, it has more theoretical value but is rarely used in practice.


---

# <a id="section3">3. A Unified Analysis: Why Targets Differ in Practice</a>

Although $x_0$, $\epsilon$, $v$, and score prediction are mathematically equivalent (all linear reparameterizations of the $(x_0,\epsilon)$ basis), their optimization behaviors differ dramatically. To explain this, we analyze them through **three quantitative lenses**:

- Correlation with $x_t$: Measuring Task Difficulty

- Conditional Mean: Measuring Signal Strength

- Conditional Variance: Measuring gradient stability

---

## <a id="section3.1">3.1 Correlation: Measuring Task Difficulty</a>

Correlation measures how predictable a target \$y\_t\$ is from the noisy input \$x\_t\$ via a *linear* relationship:

$$
\rho_t = \frac{\mathrm{Cov}(y_t, x_t)}{\sqrt{\mathrm{Var}(y_t)\,\mathrm{Var}(x_t)}}\label{eq:18}
$$

- **High correlation:** the target is strongly aligned with \$x\_t\$, making the regression problem easier.
- **Low correlation:** the target is weakly coupled to \$x\_t\$, making training harder.

We now analyze this quantity under two scenarios.

### <a id="section3.1.1">3.1.1 Case A: When $x_0$ is Gaussian (simplified setting)</a>

Assume $x_0 \sim \mathcal N(0,I)$, $\epsilon \sim \mathcal N(0,I)$, independent. Then $(x_0,\epsilon,x_t)$ is jointly Gaussian, so correlations are determined entirely by covariances. With

$$
x_t = a_t x_0 + b_t \epsilon, 
\quad \text{SNR} = \frac{a_t^2}{b_t^2},
$$

and $\mathrm{Var}(x_t)=(a_t^2+b_t^2)I$, we obtain closed-form correlations:

- $\epsilon$-prediction: Since $\mathrm{Cov}(\epsilon,x_t)=\mathrm{Cov}(\epsilon,a_t x_0 + b_t \epsilon)=b_t$, Substitute into Eq. \ref{eq:18}

  $$
  \rho_{\epsilon, x_t} = \frac{\mathrm{Cov}(\epsilon,x_t)}{\sqrt{\mathrm{Var}(\epsilon)\,\mathrm{Var}(x_t)}} = \frac{b_t}{\sqrt{a_t^2+b_t^2}} = \frac{1}{\sqrt{1+\text{SNR}}}.
  $$
  
  **strong at low SNR, weak at high SNR**, easiest to learn in low-SNR regions but harder in high-SNR regions.

- $x_0$-prediction: Since $\mathrm{Cov}(x_0,x_t)=\mathrm{Cov}(x_0,a_t x_0 + b_t \epsilon)=a_t$, Substitute into Eq. \ref{eq:18}
  
  $$
  \rho_{x_0, x_t} = \frac{\mathrm{Cov}(x_0,x_t)}{\sqrt{\mathrm{Var}(x_0)\,\mathrm{Var}(x_t)}} = \frac{a_t}{\sqrt{a_t^2+b_t^2}} = \frac{\sqrt{SNR}}{\sqrt{1+\text{SNR}}}.
  $$
  
  **strong at high SNR, weak at low SNR**, easiest to learn in high-SNR regions but harder in low-SNR regions.
  
- $v$-prediction: Since $\mathrm{Cov}(v,x_t)=\mathrm{Cov}(a_t \epsilon - b_t x_0,a_t x_0 + b_t \epsilon)=0$, Substitute into Eq. \ref{eq:18}

  $$
  \rho_{v, x_t} = \frac{\mathrm{Cov}(v,x_t)}{\sqrt{\mathrm{Var}(v)\,\mathrm{Var}(x_t)}} = 0.
  $$
  
  orthogonal to $x_t$ for all SNR, **tends to maintain a more balanced correlation across SNR, indicating more uniform difficulty.**
  
- score-prediction: Since $\mathrm{Cov}(s,x_t)=\mathrm{Cov}(-\dfrac{1}{b_t}\epsilon,a_t x_0 + b_t \epsilon)=-1$, and $\sqrt{\mathrm{Var}(s)}=\dfrac{1}{b_t}$, Substitute into Eq. \ref{eq:18}

  $$
  \rho_{s, x_t} = \frac{\mathrm{Cov}(s,x_t)}{\sqrt{\mathrm{Var}(s)\,\mathrm{Var}(x_t)}} = \frac{-b_t}{\sqrt{a_t^2+b_t^2}} = -\frac{1}{\sqrt{1+\text{SNR}}}.
  $$
  
  same magnitude as $\epsilon$ but opposite sign.

The following figure shows Pearson correlation between four target values and $x_t$, with the horizontal axis corresponding to SNR and T respectively.

![Correlation vs SNR and t](/images/posts/post_2/correlation.png)

---

### <a id="section3.1.2">3.1.2 When $x_0$ is non-Gaussian (realistic case)</a>

For real data, $x_0$ is non-Gaussian. Then: 
- The joint distribution $(x_0, x_t)$ is no longer Gaussian.
- The correlation coefficient $\rho(y_t, x_t)$ cannot be reduced to a simple function of SNR.
- Exact values depend on higher-order moments of the data distribution.

**Nevertheless, qualitative trends remain unchanged** because the corruption channel is Gaussian:

- **$\epsilon$-prediction:** correlation stronger in **low-SNR regime** (since $x_t \approx b_t\epsilon$ when SNR is low), weaker in high-SNR (since $x_t \approx a_tx_0$ when SNR is high).

- **$x_0$-prediction:** correlation stronger in **high-SNR regime**, weaker in low-SNR (since $x_t \approx a_t x_0$ when SNR is high).

- **$v$-prediction:** correlation remains small, though not exactly zero; still lacks strong SNR bias.

- **Score-prediction:** correlation trend mirrors Œµ (stronger in low-SNR, weaker in high-SNR).


---

## <a id="section3.2">3.2 Conditional Mean: Measuring Signal Strength</a>

A fundamental principle of statistics states that to minimize the loss function, the optimal model $f_{\theta}^{\star}$ must predict the conditional expectation of the target y given the input $x_t$, i,e., 

$$
f_{\theta}^{\star} = \mathbb{E}(y \mid x_t)
$$

- **If the conditional mean is large**: the network receives strong, coherent gradients ‚Üí fast convergence, efficient learning.

- **If the conditional mean is small**: the signal is weak, updates become noise-dominated, leading to slow progress or gradient starvation unless reweighted.

We now analyze this quantity under two scenarios.


---

### <a id="section3.2.1">3.2.1 Case A: When $x_0$ is Gaussian (simplified setting)</a>

If we assume both $x_0\sim \mathcal N(0,I)$ and $\epsilon\sim\mathcal N(0,I)$ (independent), then the joint distribution $(x_0,\epsilon,x_t)$ is Gaussian. In this case, the conditional expectations are **linear in $x_t$** and admit closed-form formulas:

$$
\mathbb{E}(y_t \mid x_t) = \mathbb{E}(y_t) + \frac{\mathrm{Cov}(y_t, x_t)}{\mathrm{Var}(x_t)}\left( x_t - \mathbb{E}(x_t) \right)\label{eq:20}
$$


-  **$ \epsilon $-prediction:** Substitute $y_t=\epsilon$ into Eq. \ref{eq:20}

   $$
   \begin{align}
   \mathbb{E}[\epsilon \mid x_t] & = \mathbb{E}(\epsilon) + \frac{\mathrm{Cov}(\epsilon, x_t)}{\mathrm{Var}(x_t)}(x_t - \mathbb{E}(x_t)) = b_tx_t
   \end{align}
   $$
   
   Strong in low-SNR ($b_t$ is large) regime, weak in high-SNR ($b_t$ is small) regime. Learns heavily noised states well, but struggles when data is clean.

-  **$ x_0 $-prediction:** Substitute $y_t=x_0$ into Eq. \ref{eq:20}

   $$
   \begin{align}
   \mathbb{E}[\epsilon \mid x_t] & = \mathbb{E}(x_0) + \frac{\mathrm{Cov}(x_0, x_t)}{\mathrm{Var}(x_t)}(x_t - \mathbb{E}(x_t)) = a_tx_t
   \end{align}
   $$
   
   Strong in high-SNR ($a_t$ is large) regime, weak in low-SNR ($a_t$ is small) regime. Captures coarse structure early, but struggles with heavily noised states.

-  **$ v $-prediction:**
   With $ v = a_t \epsilon - b_t x_0 $, substituting the above conditional means yields:
   
   $$
   \begin{align}
   \mathbb{E}[v \mid x_t] & = a_t \, \mathbb{E}[\epsilon \mid x_t] - b_t \, \mathbb{E}[x_0 \mid x_t] \\[10pt] 
   & =  a_t b_t x_t - b_t a_t x_t = 0
   \end{align}
   $$
   
   More uniform but weaker signal (zero when $x_0$ follows Gaussian distribution). Stable but slower unless boosted.
   

-  **Score-prediction:** From the relationship between the score and $ \epsilon $
   
   $$ 
   \mathbb{E}[s \mid x_t] = -\frac{\mathbb{E}[\epsilon \mid x_t]}{b_t} = -x_t
   $$  
   
   
   Roughly uniform mean signal (\$-x\_t\$ under Gaussian proxy). Even distribution, but scale imbalance remains (see ¬ß5.3).


The following figure shows conditional mean between four target values, with the horizontal axis corresponding to SNR and T respectively.

![Correlation vs SNR and t](/images/posts/post_2/condmean.png)

---

### <a id="section3.2.2">3.2.2 Case B: When $x_0$ is non-Gaussian (realistic case)</a>

Similarly, for real datasets, $x_0$ is far from Gaussian, the neat linear forms from Case A **do not hold exactly**. Instead, we can only describe **qualitative trends** based on information-theoretic arguments (I‚ÄìMMSE identity) and intuitive limits:

- **Œµ-prediction:**

  - Low SNR: $x_t \approx b_t\epsilon$, so $\epsilon$ is recoverable ‚Üí large conditional mean.
  - High SNR: $x_t \approx a_t x_0$, noise is hidden ‚Üí $\epsilon$ is unidentifiable ‚Üí conditional mean near zero.

- **$x_0$-prediction:**

  - High SNR: $x_t$ almost equals $x_0$, so posterior concentrates ‚Üí large conditional mean.
  - Low SNR: $x_t$ mostly noise, posterior diffuse ‚Üí conditional mean close to prior mean (‚âà0 if whitened).

- **$v$-prediction:**
  By construction, $v=a_t\epsilon - b_t x_0$. In non-Gaussian settings its conditional mean need not vanish, but it remains relatively small and without strong SNR bias.

- **Score-prediction:**
  Training target is $s=-\epsilon/b_t$. Its conditional mean is
  $\mathbb E[s \mid x_t] = -\mathbb E[\epsilon\mid x_t]/b_t$.
  Qualitative behavior mirrors Œµ-prediction: stronger at low-SNR, weaker at high-SNR.
  
---

## <a id="section3.3">3.3 Conditional Variance: Measuring Gradient Stability</a>

The **Law of Total Variance** provides the key identity linking signal and noise:

$$
\mathrm{Var}(Y) \;=\; \underbrace{\mathbb E[\mathrm{Var}(Y \mid X)]}_{\text{irreducible noise / conditional variance}}
+ \underbrace{\mathrm{Var}(\mathbb E[Y \mid X])}_{\text{explainable signal}}.
$$

Here: $Y$ is the training target ($\epsilon, x_0, v, \tilde s$), $X\equiv x_t$ is the noisy input.

- $\mathbb E[Y \mid X]$ is the **signal** (learnable part).
- $\mathrm{Var}(Y \mid X)$ is the **irreducible noise** (SGD variance floor).

This decomposition is why **conditional variance** is a central diagnostic of gradient stability. Near the optimum, the per-sample MSE gradient variance is proportional to: $$\mathrm{Var}(y_t \mid x_t)$$

This represents the irreducible noise of prediction.

- **Large conditional variance:** noisy gradients ‚Üí requires smaller LR, larger batch/EMA, or down-weighting; otherwise unstable.
- **Small conditional variance:** clean gradients ‚Üí stable, efficient learning; but if paired with small conditional mean, learning still stagnates.

---

### <a id="section3.3.1">3.3.1 Case A: When $x_0$ is Gaussian (simplified setting)</a>

Assume $x_0 \sim \mathcal N(0,I)$, $\epsilon\sim \mathcal N(0,I)$, independent, with

$$
x_t = a_t x_0 + b_t \epsilon, \qquad \text{SNR} = \frac{a_t^2}{b_t^2}.
$$

Then all conditional distributions are Gaussian, so exact formulas are available:

- **$\epsilon$-prediction**

$$
\mathrm{Var}(\epsilon \mid x_t) = \frac{a_t^2}{a_t^2+b_t^2}\,I = \frac{\text{SNR}}{1+\text{SNR}}\,I.
$$

- **$x_0$-prediction**

$$
\mathrm{Var}(x_0 \mid x_t) = \frac{b_t^2}{a_t^2+b_t^2}\,I = \frac{1}{1+\text{SNR}}\,I.
$$

- **$v$-prediction**

$$
\mathrm{Var}(v \mid x_t) = I, \quad \text{independent of SNR}.
$$

- **Score-prediction** ($\tilde s = -\epsilon/b_t$)

$$
\mathrm{Var}(\tilde s \mid x_t) = \frac{1}{b_t^2}\,\mathrm{Var}(\epsilon \mid x_t) = \text{SNR}\cdot I.
$$

These satisfy the total variance identity, e.g. for $\epsilon$:

$$
\mathrm{Var}(\epsilon) = I = \underbrace{\mathbb E[\mathrm{Var}(\epsilon\mid x_t)]}_{\frac{\text{SNR}}{1+\text{SNR}}I}
+ \underbrace{\mathrm{Var}(\mathbb E[\epsilon\mid x_t])}_{\frac{1}{1+\text{SNR}}I}.
$$

---

### <a id="section3.3.2">3.3.2 Case B: When $x_0$ is non-Gaussian (realistic case)</a>

For real-world data, $x_0$ is highly non-Gaussian (e.g., images). However, the **qualitative SNR trends** remain the same, since the corruption channel is Gaussian.


- **$\epsilon$-prediction:** low-SNR ‚áí small variance (stable); high-SNR ‚áí large variance (unstable).
- **$x_0$-prediction:** high-SNR ‚áí small variance (stable); low-SNR ‚áí large variance (unstable).
- **$v$-prediction:** variance not exactly constant, but relatively flat; no strong SNR bias.
- **Score-prediction:** inherits Œµ‚Äôs trend but amplified by $1/b_t$; variance grows with SNR ‚áí high-SNR instability.


---

## <a id="section3.4">3.4 Summary and Comparative Table</a>

We had notice that no mather the data prior $p(x_0)$ is Gaussian or not, the qualitative trends is consistent with each other. That's because the channel is Gaussian ($x_t=a_tx_0+b_t\epsilon$ is a Gaussian-corrupted version of $x_0$), the **SNR regimes** have universal behavior, independent of the prior:

- **High-SNR ($a_t\!\gg\! b_t$)**: $x_t\approx a_t x_0$. The observation is dominated by the clean signal; the injected noise is nearly invisible.

- **Low-SNR ($b_t\!\gg\! a_t$)**: $x_t\approx b_t \epsilon$. The observation is dominated by noise.


These **qualitative trends** for the three metrics (correlation, conditional mean, conditional variance) therefore **do not depend** on assuming a Gaussian data prior, only the **closed-form expressions** require a Gaussian prior.



| Target               | Correlation (difficulty)         | Conditional Mean (signal) | Conditional Variance (stability)     | Net Effect                      |
| -------------------- | -------------------------------- | ------------------------- | ------------------------------------ | ------------------------------- |
| \$\epsilon\$         | High at low-SNR, low at high-SNR | Strong in low-SNR         | Stable in low-SNR, noisy in high-SNR | **Biased toward low-SNR**       |
| \$x\_0\$             | High at high-SNR, low at low-SNR | Strong in high-SNR        | Stable in high-SNR, noisy in low-SNR | **Biased toward high-SNR**      |
| \$v\$                | Zero (uniform)                   | Weak but uniform          | Constant (stable)                    | **Balanced but weaker**         |
| Score (\$\tilde s\$) | Mirrors \$\epsilon\$             | Roughly uniform           | Noisy in high-SNR, stable in low-SNR | **Unstable in high-SNR regime** |



---

# <a id="section4">Amplification of Network Prediction Errors Over Timesteps</a>



---

# <a id="section5">Conclusion</a>

---

# <a id="section6">References</a>

[^iedm]: Karras T, Aittala M, Lehtinen J, et al. Analyzing and improving the training dynamics of diffusion models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 24174-24184.

[^edm]: Karras T, Aittala M, Aila T, et al. Elucidating the design space of diffusion-based generative models[J]. Advances in neural information processing systems, 2022, 35: 26565-26577.

[^Kingma]: Kingma D, Gao R. Understanding diffusion objectives as the elbo with simple data augmentation[J]. Advances in Neural Information Processing Systems, 2023, 36: 65484-65516.

[^Salimans]: Salimans T, Ho J. Progressive distillation for fast sampling of diffusion models[J]. arXiv preprint arXiv:2202.00512, 2022.
