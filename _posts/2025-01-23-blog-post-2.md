---
title: 'Analysis of the Stability and Efficiency of Diffusion Model Training'
excerpt: "while diffusion models have revolutionized generative AI, their training challenges stem from a combination of resource intensity, optimization intricacies, and deployment hurdles. A stable training process ensures that the model produces good quality samples and converges efficiently over time without suffering from numerical instabilities."
date: 2025-01-23
permalink: /posts/2025/01/diffusion-model-2/
tags:
  - Diffusion Model
  - SNR
  - Model Training
---

<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>üìö Table of Contents</strong></span>
  </summary>
  <ul style="margin:0; padding-left:1.1rem;">
    <li><a href="#section1">1. Preliminary</a>
		<ul>
		  <li><a href="#section1.1">1.1 From Maximum likelihood to ELBO</a></li>
		  <li><a href="#section1.2">1.2 From KL collapses to a mean MSE</a></li>
		</ul>
	</li>
    <li><a href="#section2">2. Re-parameterising the mean with different target predictor</a>
		<ul>
		  <li><a href="#section2.1">2.1 x<sub>0</sub>-prediction</a></li>
		  <li><a href="#section2.2">2.2 &epsilon;-prediction</a></li>
		  <li><a href="#section2.3">2.3 v-prediction</a></li>
		  <li><a href="#section2.4">2.4 Score-prediction</a></li>
		</ul>
	</li>
	<li><a href="#section3">3. A Unified Analysis: Why Targets Differ in Practice</a>
		<ul>
			<li><a href="#section3.1">3.1 Correlation: Measuring Task Difficulty</a></li>
			<li><a href="#section3.2">3.2 Conditional Mean: Measuring Signal Strength</a></li>
			<li><a href="#section3.3">3.3 Conditional Variance: Measuring Gradient Stability</a></li>
			<li><a href="#section3.4">3.4 Summary and Comparative Table</a></li>
		</ul>
	</li>	
	<li><a href="#section4">4. Stabilizing Diffusion Training via Optimization & Objective Design</a>
		<ul>
			<li><a href="#section4.1">4.1 The root causes of instability and inefficiency in diffusion model training</a>
				<ul>
					<li><a href="#section4.1.1">4.1.1 The universal perspective of instability and inefficiency </a></li>
					<li><a href="#section4.1.2">4.1.2 Why Different Spaces matters</a></li>
				</ul>
			</li>	
			<li><a href="#section4.2">4.2 Stabilization in t-space (time-step parameterization)</a>
				<ul>
					<li><a href="#section4.2.1">4.2.1 Weighted loss strategies</a></li>
					<li><a href="#section4.2.2">4.2.2 Noise schedules</a></li>
				</ul>
			</li>
			<li><a href="#section4.3">4.3 Stabilization in œÉ-space (EDM and successors)</a>
				<ul>
					<li><a href="#section4.3.1">4.3.1 EDM-style preconditioning</a></li>
					<li><a href="#section4.3.2">4.3.2 Sigma sampling distributions</a></li>
					<li><a href="#section4.3.3">4.3.3 Loss weighting in œÉ-space</a></li>
				</ul>
			</li>
			<li><a href="#section4.4">4.4 General Engineering Stabilization Techniques</a>
				<ul>
					<li><a href="#section4.4.1">4.4.1 Optimizer & Numerical Stability</a></li>
					<li><a href="#section4.4.2">4.4.2 EMA and Post-hoc EMA</a></li>
				</ul>
			</li>
			<li><a href="#section4.5">4.5 General Engineering Stabilization Techniques</a>
				<ul>
					<li><a href="#section4.4.1">4.4.1 Optimizer & Numerical Stability</a></li>
					<li><a href="#section4.4.2">4.4.2 EMA and Post-hoc EMA</a></li>
				</ul>
			</li>
		</ul>
	</li>
	<li><a href="#section5">5. Stabilizing Diffusion Training via Network Architecture</a></li>
	<li><a href="#section6">6. References</a></li>
  </ul>
</details>


Diffusion models have achieved remarkable success in generative modeling, producing state‚Äëof‚Äëthe‚Äëart results in image, audio, and multimodal generation. Yet training them remains notoriously difficult. Instabilities such as vanishing gradients, exploding loss values, and imbalanced learning across timesteps often hinder efficient convergence.

At the core of these issues lies the choice of **training objective**. Although the canonical derivation from maximum likelihood leads naturally to an evidence lower bound (ELBO) and finally to a mean‚Äësquared error (MSE) objective, how we reparameterize the regression target strongly affects optimization dynamics. Four parameterizations are commonly used in practice‚Äî$x_0$, $\epsilon$, $v$, and score prediction. They are mathematically equivalent but exhibit different levels of stability, gradient flow, and ease of optimization.

This post focus on **diffusion models training** - why diffusion training is unstable, how the four objectives differ, and how modern solutions re‚Äëbalance training dynamics.

---

<h1 id="section1" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">1. Preliminary</h1>


For generative models, we expect our model $p_{\theta}$ (parameterized by $\theta$) to be as close as possible to the true distribution $p_{\text {data}}$. Based on the KL divergence, we derive that

$$
\mathbb{KL}\left(p_{\text {data}}(x) \parallel p_{\theta}(x) \right) = \int p_{\text {data}}(x)\log (p_{\text {data}}(x))dx - \int p_{\text {data}}(x)\log(p_{\theta}(x))dx\label{eq:1}
$$

The first term, $\int p_{\text {data}}(x) \log (p_{\text {data}}(x))dx$, is the **entropy** of the true distribution $p_{\text {data}}$, it is a constant with respect to the model parameters $\theta$. The second term, $\int p_{\text {data}}(x)\log(p_{\theta}(x))dx$, is the expected log-likelihood of the model under the true distribution. Thus, **minimizing KL divergence is equal to maximize log-likelihood $p_{\theta}(x)$, where $x \sim p_{\text {data}}$**.

---

<h1 id="section1.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">1.1 From Maximum likelihood to ELBO</h1>


Let $x_0$ be the original image, and $x_i (i=1,2,...,T)$ be the image with noise added to $x_0$. Our goal is to maximise 

$$
\log p_{\theta}(x_0)=\log \int p_{\theta}(x_{0:T}) dx_{1:T} \label{eq:2}
$$

Introduce the forward process $q(x_{1:T} \mid x_0)$ (a Markov chain with fixed noise‚Äëschedule). Using Jensen‚Äôs inequality gives the evidence lower bound:

$$
\begin{align}
\log p_\theta(x_0) \geq \mathcal{L}_\text{ELBO} & = \mathbb{E}_q \left[ \underbrace{\log p_\theta(x_0 \mid x_1)}_{\text{reconstruction loss}} - \underbrace{\log \frac{q(x_{T} \mid x_0)}{p_\theta(x_{T})}}_{\text{prior matching}} - \underbrace{\sum_{t=2}^T \log \frac{q(x_{t-1} \mid x_t, x_0)}{p_\theta(x_{t-1} \mid x_t)}}_{\text{denoising matching}} \right]\label{eq:3}
\end{align}
$$

The first term is reconstruction loss, the second term is prior matching, both of them are extremely small and can be ignored. Therefore, what we are truly concerned about is the third item, which also known as denoising matching term.

---

<h1 id="section1.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">1.2 From KL divergence to a mean MSE</h1>


For each denoising step, both forward posterior $q(x_{t-1} \mid x_t, x_0) \sim \mathcal{N}(\mu_{q}, \sigma_{q}^2I)$ and backward posterior
$p_{\theta}(x_{t-1} \mid x_t) \sim \mathcal{N}(\mu_{\theta}, \sigma_{\theta}^2I)$ are gaussian distributions. For two Gaussians with identical covariance, if we fix the two variances are equal to $\sigma_{q}^2$, then the KL divergence is equal to:

$$
\mathbb{KL}\left(q(x_{t-1} \mid x_t, x_0) \parallel p_{\theta}(x_{t-1} \mid x_t) \right) = \frac{1}{2\sigma_q^2} \|{\mu}_{q} - \mu_{\theta}(x_t, t)\|_2^2 + \text{const}\label{eq:4}
$$

Hence, for each denoising step, the loss function equals to

$$
\mathcal{L}_{\text{denoise}} =  \mathbb{E}_q \left[ \|\mu_q - \mu_{\theta}(x_t, t)\|^2 \right]\label{eq:5} 
$$

$\mu_q$ is the true target we want to predict, How do we calculate the value of 
$\mu_q$? Let's first decompose forward posterior $q(x_{t-1} \mid x_t, x_0)$ :

$$
q(x_{t-1} \mid x_t, x_0)=\frac{q(x_{t} \mid x_{t-1})q(x_{t-1} \mid x_{0})}{q(x_{t} \mid x_{0})} \propto q(x_{t} \mid x_{t-1})q(x_{t-1} \mid x_{0})\label{eq:6}
$$

where 

$$
\begin{align}
& q(x_{t} \mid x_{t-1}) \sim \mathcal{N}(x_{t-1};\mu_1, \sigma_1^2I),\ \ \mu_1=\frac{1}{\sqrt{\alpha_t}}x_{t},\ \ \sigma_1^2=\frac{1-\alpha_t}{\alpha_t} \\[10pt] 
& q(x_{t-1} \mid x_{0})  \sim \mathcal{N}(x_{t-1};\mu_2, \sigma_2^2I), \ \ \mu_2=\sqrt{\bar \alpha_{t-1}}x_{0},\ \  \sigma_2^2=1-\bar \alpha_{t-1}\label{eq:7}
\end{align}
$$

The product of two Gaussian distributions is also a Gaussian distribution, with mean gives.




$$
\mu_{q} = \frac{\mu_1\sigma_2^2+\mu_2\sigma_1^2}{\sigma_1^2+\sigma_2^2}  = \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1}) x_t + \sqrt{\bar{\alpha}_{t-1}} (1 - \alpha_t) {x}_0}{1 - \bar{\alpha}_t}\label{eq:8}
$$

Combining equations \ref{eq:5} and \ref{eq:8}, Our goal is to construct a neural network $\mu_{\theta}$, which takes $x_t$ and $t$ as inputs, such that the output of the network is as close as possible to $\mu_q$.


--- 

<h1 id="section2" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">2. Re-parameterising the mean with different target predictor</h1>

Following equation \ref{eq:8}, we can dirrectly build a network $\mu_{\theta}$ to output $\mu_{q}$. However, in practice, we usually do not directly fit the value of $\mu_{q}$, mainly due to the following reasons.

- $\mu_{q}$ is an affine function of $x_t$, which is known at training and test time, there is no need for the network to ‚Äúreproduce‚Äù it. If we regress $\mu_{q}$ directly, the network wastes capacity relearning a large known term and must also learn the residual that actually depends on the unknown clean content. 

- The mean target value is highly time-dependent scaling across $t$, which means that the output of the network is unstable, it is usually extremely difficult for a network to output results with a large variance range.


Instead of asking the network to output $\mu_{q}$ directly, the community typically uses four common prediction targets to train diffusion models: $\epsilon$-prediction, $x_0$-prediction, $v$-prediction, score-prediction. If we regard the original image $x_0$ and noise $\epsilon$ as two orthogonal dimensions, then All the common targets are linear in $(x_0, \epsilon)$

---

<h1 id="section2.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.1 $x_0$-prediction (aka sample-prediction in Diffusers)</h1>

In $x_0$-prediction, the neural network is trained to directly estimate the clean original data $x_0$ from the noisy input $x_t$ at timestep $t$. Denoted the network as $ x_{\theta}(x_t, t)$, and the predicted output is $$\hat{x}_0$$, this approach reparameterizes the predicted mean $\mu_{\theta}$ using the estimated $$\hat{x}_0$$. Substituting into Equation \ref{eq:8}, the mean becomes:

$$
\mu_{\theta}(x_t, t) = \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1}) x_t + \sqrt{\bar{\alpha}_{t-1}} (1 - \alpha_t)\,x_{\theta}(x_t, t)}{1 - \bar{\alpha}_t}\label{eq:9}
$$

The loss function then simplifies to minimizing the MSE between the true $x_0$ and the predicted $\hat{x}_0$:

$$
\mathcal{L}_{x_0} = \mathbb{E}_{x_0, t, \epsilon} \left[ \| x_0 - x_{\theta}(x_t, t) \|^2 \right]\label{eq:10}
$$

- **Pros**: This parameterization is **the most intuitive** since all DM's final goal is to recover the original image. If the sample data $x_0$ is normalized, the network's predicted output will have stable variance for any input timestep $t$.


- **Cons**: The primary drawback of $x_0$-prediction lies in the uneven learning difficulty across signal-to-noise ratio (SNR) regimes, which induces heterogeneous gradient behaviors and ultimately hinders training convergence. 

---

<h1 id="section2.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.2 $\epsilon$-prediction</h1>


The $\epsilon$-prediction paradigm tasks the network with predicting the noise $\epsilon$ added during the forward process. Denoted the network as $ \epsilon_{\theta}(x_t, t)$, and the predicted output is $$\hat{\epsilon}$$, this parameterization leverages the forward noising equation to express the clean data $x_0$ in terms of the noise $\epsilon$ via a simple linear transformation . 

$$
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon \  \Longrightarrow \  x_0=\frac{x_t-\sqrt{1 - \bar{\alpha}_t} \epsilon}{\sqrt{\bar{\alpha}_t}}\label{eq:11}
$$ 

Substituting  $x_{\theta}$ with ${\epsilon}_{\theta}$ into equation \ref{eq:8} for the mean gives:

$$
\mu_{\theta}(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} {\epsilon}_{\theta}(x_t, t) \right)\label{eq:12}
$$

The loss function then simplifies to minimizing the MSE between the true noise $\epsilon$ and the predicted $\hat{\epsilon}$:

$$
\mathcal{L}_{\epsilon} = \mathbb{E}_{x_0, t, \epsilon} \left[ \| \epsilon - \epsilon_{\theta}(x_t, t) \|^2 \right]\label{eq:13}
$$

- **Pros**: This parameterization is **the most widely adopted**, Since it was proposed by DDPM, and DDPM is one of the earliest and most influential articles in the field of diffusion models. Besides, the target $\epsilon$ is timestep-independent target distribution since $\epsilon \sim \mathcal{N}(0, I)$, the training process is relatively stable.


- **Cons**: Like $x_0$-prediction, $\epsilon$-prediction also suffers from the uneven learning difficulty problems, which needs re-weighting $ w(t) $ to promote balanced learning across the noise spectrum. We will conduct more in-depth analysis in the following sections.

---

<h1 id="section2.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.3 $v$-prediction</h1>


Velocity ($v$)-prediction combines elements of both $x_0$ and $\epsilon$ predictions by forecasting a velocity term $v$ that interpolates between them. Defined velocity as $$v = \sqrt{\bar{\alpha}_t} \epsilon - \sqrt{1 - \bar{\alpha}_t} x_0$$ (or its normalized variant), the network predicts $$\hat{v} = v_{\theta}(x_t, t)$$. Now, the mean $\mu_{\theta}$ can be expressed in terms of $${v}_{\theta}$$:

$$
\mu_{\theta}(x_t, t) = \sqrt{\alpha_t}x_t- \frac{(1-\alpha_t)\sqrt{\bar \alpha_{t-1}}}{\sqrt{1-\bar \alpha_t}}{v_{\theta}}\label{eq:14}
$$

The loss function then simplifies to minimizing the MSE between the true velocity $v$ and the predicted $$\hat{v}$$:

$$
\mathcal{L}_v = \mathbb{E}_{x_0, t, \epsilon} \left[ \| v - v_{\theta}(x_t, t) \|^2 \right]\label{eq:15}
$$

- **Pros**: This parameterization is **the most stable**, Provides more uniform learning difficulty across all noise levels. Currently, $v$-prediction is being used by most advanced models, such as ImageGen, Stable Diffusion, etc.

- **Cons**: $v$ is slightly less intuitive compared to $\epsilon$ and $x_0$.

---

<h1 id="section2.4" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">2.4 Score-prediction</h1>

This parameterization draws from the score-based generative modeling framework, where the neural network estimates the score function $s_{\theta}(x_t, t) = \nabla_{x_t} \log p_t(x_t)$, representing the gradient of the log-probability density at the noisy state $x_t$. In Gaussian diffusion models, the score is directly related to the noise via 

$$s_{\theta}(x_t, t) = -\frac{\epsilon_{\theta}(x_t, t)}{\sqrt{1 - \bar \alpha_t}}$$

Starting from the forward noising equation and substituting the equivalent form $$\epsilon = -\sqrt{1 - \bar{\alpha}_t} \, s(x_t, t)$$, the predicted mean $$\mu_{\theta}$$ is derived by inserting this into Equation \ref{eq:8}:

$$
\mu_{\theta}(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t + (1 - \alpha_t) \, s_{\theta}(x_t, t) \right)
$$

The loss function simplifies to minimizing the MSE between the true score $s(x_t, t)$ and the predicted $$\hat{s} = s_{\theta}(x_t, t)$$:

$$\mathcal{L}_s = \mathbb{E}_{x_0, t, \epsilon} \left[ \| s - s_{\theta}(x_t, t) \|^2 \right]$$

- **Pros**: This parameterization is **the most perfectly matched with the reverse SDE theory**, as our discussion in last [posts](https://innovation-cat.github.io/posts/2024/11/diffusion-model-1/), the only unknown needed for sampling is score.


- **Cons**: However, it may introduce scaling sensitivities in discrete timesteps, potentially leading to instabilities if the score magnitudes are not properly normalized, necessitating adaptive weighting or variance adjustments during training. In general, it has more theoretical value but is rarely used in practice.


---

<h1 id="section3" style="color: #1E3A8A; font-size: 27px; font-weight: bold; text-decoration: underline;">3. A Unified Analysis: Why Targets Differ in Practice</h1>

Regardless of what target is being predicted, the loss function can be uniformly written in the form of MSE.

$$\mathcal{L} = \mathbb{E}_{x_0, t, \epsilon} \left[ \| y_t - f_{\theta}(x_t, t) \|^2 \right]$$

Where $y_t$ is target ($x_0,\,v,\,\epsilon,\text{or score}$), $f_{\theta}$ is neural network with $x_t$ and $t$ as inputs. **Our subsequent discussions are all based on the unified loss function form above.**

Although four different targets are mathematically equivalent and can be converted into each other, their optimization behaviors differ dramatically. To explain this, we analyze them through **three quantitative lenses**:

- **Correlation with $x_t$**: Measuring Task Difficulty

- **Conditional Mean**: Measuring Signal Strength

- **Conditional Variance**: Measuring gradient stability

---

<h1 id="section3.1" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.1 Correlation: Measuring Task Difficulty</h1>


Correlation measures how predictable a target \$y\_t\$ is from the noisy input \$x\_t\$ via a *linear* relationship:

$$
\rho_t = \frac{\mathrm{Cov}(y_t, x_t)}{\sqrt{\mathrm{Var}(y_t)\,\mathrm{Var}(x_t)}}\label{eq:18}
$$

- **High correlation:** the target is strongly aligned with \$x\_t\$, making the regression problem easier.
- **Low correlation:** the target is weakly coupled to \$x\_t\$, making training harder.

We now analyze this quantity under two scenarios.

---

<h1 id="section3.1.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">3.1.1 Case A: When $x_0$ is Gaussian (simplified setting)</h1>



Assume $x_0 \sim \mathcal N(0,I)$, $\epsilon \sim \mathcal N(0,I)$, independent. Then $(x_0,\epsilon,x_t)$ is jointly Gaussian, so correlations are determined entirely by covariances. With

$$
x_t = a_t x_0 + b_t \epsilon, 
\quad \text{SNR} = \frac{a_t^2}{b_t^2},
$$

and $\mathrm{Var}(x_t)=(a_t^2+b_t^2)I$, we obtain closed-form correlations:

- **$\epsilon$-prediction**: Since $\mathrm{Cov}(\epsilon,x_t)=\mathrm{Cov}(\epsilon,a_t x_0 + b_t \epsilon)=b_t$, Substitute into Eq. \ref{eq:18}

  $$
  \rho_{\epsilon, x_t} = \frac{\mathrm{Cov}(\epsilon,x_t)}{\sqrt{\mathrm{Var}(\epsilon)\,\mathrm{Var}(x_t)}} = \frac{b_t}{\sqrt{a_t^2+b_t^2}} = \frac{1}{\sqrt{1+\text{SNR}}}.
  $$
  
  **strong at low SNR, weak at high SNR**, easiest to learn in low-SNR regions but harder in high-SNR regions.

- **$x_0$-prediction**: Since $\mathrm{Cov}(x_0,x_t)=\mathrm{Cov}(x_0,a_t x_0 + b_t \epsilon)=a_t$, Substitute into Eq. \ref{eq:18}
  
  $$
  \rho_{x_0, x_t} = \frac{\mathrm{Cov}(x_0,x_t)}{\sqrt{\mathrm{Var}(x_0)\,\mathrm{Var}(x_t)}} = \frac{a_t}{\sqrt{a_t^2+b_t^2}} = \frac{\sqrt{SNR}}{\sqrt{1+\text{SNR}}}.
  $$
  
  **strong at high SNR, weak at low SNR**, easiest to learn in high-SNR regions but harder in low-SNR regions.
  
- **$v$-prediction**: Since $\mathrm{Cov}(v,x_t)=\mathrm{Cov}(a_t \epsilon - b_t x_0,a_t x_0 + b_t \epsilon)=0$, Substitute into Eq. \ref{eq:18}

  $$
  \rho_{v, x_t} = \frac{\mathrm{Cov}(v,x_t)}{\sqrt{\mathrm{Var}(v)\,\mathrm{Var}(x_t)}} = 0.
  $$
  
  orthogonal to $x_t$ for all SNR, **tends to maintain a more balanced correlation across SNR, indicating more uniform difficulty.**
  
- **score-prediction**: Since $\mathrm{Cov}(s,x_t)=\mathrm{Cov}(-\dfrac{1}{b_t}\epsilon,a_t x_0 + b_t \epsilon)=-1$, and $\sqrt{\mathrm{Var}(s)}=\dfrac{1}{b_t}$, Substitute into Eq. \ref{eq:18}

  $$
  \rho_{s, x_t} = \frac{\mathrm{Cov}(s,x_t)}{\sqrt{\mathrm{Var}(s)\,\mathrm{Var}(x_t)}} = \frac{-b_t}{\sqrt{a_t^2+b_t^2}} = -\frac{1}{\sqrt{1+\text{SNR}}}.
  $$
  
  same magnitude as $\epsilon$ but opposite sign.

The following figure shows Pearson correlation between four target values and $x_t$, with the horizontal axis corresponding to SNR and T respectively.

![Correlation vs SNR and t](/images/posts/post_2/correlation.png)

---

<h1 id="section3.1.2" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">3.1.2 Case B: When $x_0$ is non-Gaussian (realistic case)</h1>

For real data, $x_0$ is non-Gaussian. Then: 
- The joint distribution $(x_0, x_t)$ is no longer Gaussian.
- The correlation coefficient $\rho(y_t, x_t)$ cannot be reduced to a simple function of SNR.
- Exact values depend on higher-order moments of the data distribution.

**Nevertheless, qualitative trends remain unchanged** because the corruption channel is Gaussian:

- **$\epsilon$-prediction:** correlation stronger in **low-SNR regime** (since $x_t \approx b_t\epsilon$ when SNR is low), weaker in high-SNR (since $x_t \approx a_tx_0$ when SNR is high).

- **$x_0$-prediction:** correlation stronger in **high-SNR regime**, weaker in low-SNR (since $x_t \approx a_t x_0$ when SNR is high).

- **$v$-prediction:** correlation remains small, though not exactly zero; still lacks strong SNR bias.

- **Score-prediction:** correlation trend mirrors Œµ (stronger in low-SNR, weaker in high-SNR).


---

<h1 id="section3.2" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.2 Conditional Mean: Measuring Signal Strength</h1>


A fundamental principle of statistics states that to minimize the loss function, the optimal model $f_{\theta}^{\star}$ must predict the conditional expectation of the target y given the input $x_t$, i,e., 

$$
f_{\theta}^{\star} = \mathbb{E}(y \mid x_t)
$$

- **If the conditional mean is large**: the network receives strong, coherent gradients ‚Üí fast convergence, efficient learning.

- **If the conditional mean is small**: the signal is weak, updates become noise-dominated, leading to slow progress or gradient starvation unless reweighted.

We now analyze this quantity under two scenarios.


---

<h1 id="section3.2.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">3.2.1 Case A: When $x_0$ is Gaussian (simplified setting)</h1>


If we assume both $x_0\sim \mathcal N(0,I)$ and $\epsilon\sim\mathcal N(0,I)$ (independent), then the joint distribution $(x_0,\epsilon,x_t)$ is Gaussian. In this case, the conditional expectations are **linear in $x_t$** and admit closed-form formulas:

$$
\mathbb{E}(y_t \mid x_t) = \mathbb{E}(y_t) + \frac{\mathrm{Cov}(y_t, x_t)}{\mathrm{Var}(x_t)}\left( x_t - \mathbb{E}(x_t) \right)\label{eq:25}
$$


-  **$ \epsilon $-prediction:** Substitute $y_t=\epsilon$ into Eq. \ref{eq:25}

   $$
   \begin{align}
   \mathbb{E}[\epsilon \mid x_t] & = \mathbb{E}(\epsilon) + \frac{\mathrm{Cov}(\epsilon, x_t)}{\mathrm{Var}(x_t)}(x_t - \mathbb{E}(x_t)) = b_tx_t
   \end{align}
   $$
   
   Strong in low-SNR ($b_t$ is large) regime, weak in high-SNR ($b_t$ is small) regime. Learns heavily noised states well, but struggles when data is clean.

-  **$ x_0 $-prediction:** Substitute $y_t=x_0$ into Eq. \ref{eq:25}

   $$
   \begin{align}
   \mathbb{E}[\epsilon \mid x_t] & = \mathbb{E}(x_0) + \frac{\mathrm{Cov}(x_0, x_t)}{\mathrm{Var}(x_t)}(x_t - \mathbb{E}(x_t)) = a_tx_t
   \end{align}
   $$
   
   Strong in high-SNR ($a_t$ is large) regime, weak in low-SNR ($a_t$ is small) regime. Captures coarse structure early, but struggles with heavily noised states.

-  **$ v $-prediction:**
   With $ v = a_t \epsilon - b_t x_0 $, substituting the above conditional means yields:
   
   $$
   \begin{align}
   \mathbb{E}[v \mid x_t] & = a_t \, \mathbb{E}[\epsilon \mid x_t] - b_t \, \mathbb{E}[x_0 \mid x_t] \\[10pt] 
   & =  a_t b_t x_t - b_t a_t x_t = 0
   \end{align}
   $$
   
   More uniform but weaker signal (zero when $x_0$ follows Gaussian distribution). Stable but slower unless boosted.
   

-  **Score-prediction:** From the relationship between the score and $ \epsilon $
   
   $$ 
   \mathbb{E}[s \mid x_t] = -\frac{\mathbb{E}[\epsilon \mid x_t]}{b_t} = -x_t
   $$  
   
   
   Roughly uniform mean signal (\$-x\_t\$ under Gaussian proxy). Even distribution, but scale imbalance remains (see ¬ß5.3).


The following figure shows conditional mean between four target values, with the horizontal axis corresponding to SNR and T respectively.

![Correlation vs SNR and t](/images/posts/post_2/condmean.png)

---

<h1 id="section3.2.2" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">3.2.2 Case B: When $x_0$ is non-Gaussian (realistic case)</h1>


Similarly, for real datasets, $x_0$ is far from Gaussian, the neat linear forms from Case A **do not hold exactly**. Instead, we can only describe **qualitative trends** based on information-theoretic arguments (I‚ÄìMMSE identity) and intuitive limits:

- **Œµ-prediction:**

  - Low SNR: $x_t \approx b_t\epsilon$, so $\epsilon$ is recoverable ‚Üí large conditional mean.
  - High SNR: $x_t \approx a_t x_0$, noise is hidden ‚Üí $\epsilon$ is unidentifiable ‚Üí conditional mean near zero.

- **$x_0$-prediction:**

  - High SNR: $x_t$ almost equals $x_0$, so posterior concentrates ‚Üí large conditional mean.
  - Low SNR: $x_t$ mostly noise, posterior diffuse ‚Üí conditional mean close to prior mean (‚âà0 if whitened).

- **$v$-prediction:**
  By construction, $v=a_t\epsilon - b_t x_0$. In non-Gaussian settings its conditional mean need not vanish, but it remains relatively small and without strong SNR bias.

- **Score-prediction:**
  Training target is $s=-\epsilon/b_t$. Its conditional mean is
  $\mathbb E[s \mid x_t] = -\mathbb E[\epsilon\mid x_t]/b_t$.
  Qualitative behavior mirrors Œµ-prediction: stronger at low-SNR, weaker at high-SNR.
  
---



<h1 id="section3.3" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.3 Conditional Variance: Measuring Gradient Stability</h1>


The **Law of Total Variance** provides the key identity linking signal and noise:

$$
\mathrm{Var}(y_t) \;=\; \underbrace{\mathbb E[\mathrm{Var}(y_t \mid x_t)]}_{\text{irreducible noise / conditional variance}}
+ \underbrace{\mathrm{Var}(\mathbb E[y_t \mid x_t])}_{\text{explainable signal}}\label{eq:31}
$$

Here: $y_t$ is the training target ($\epsilon, x_0, v, \tilde s$), $x_t$ is the noisy input.

- $\mathbb E[y_t \mid x_t]$ is the **signal** (learnable part).
- $\mathrm{Var}(y_t \mid x_t)$ is the **irreducible noise** (SGD variance floor).

This decomposition is why **conditional variance** is a central diagnostic of gradient stability. Near the optimum, the per-sample MSE gradient variance is proportional to: $$\mathrm{Var}(y_t \mid x_t)$$

This represents the irreducible noise of prediction.

- **Large conditional variance:** noisy gradients ‚Üí requires smaller LR, larger batch/EMA, or down-weighting; otherwise unstable.
- **Small conditional variance:** clean gradients ‚Üí stable, efficient learning; but if paired with small conditional mean, learning still stagnates.

---

<h1 id="section3.3.1" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">3.3.1 Case A: When $x_0$ is Gaussian (simplified setting)</h1>


Assume $x_0 \sim \mathcal N(0,I)$, $\epsilon\sim \mathcal N(0,I)$, independent, with

$$
x_t = a_t x_0 + b_t \epsilon, \qquad \text{SNR} = \frac{a_t^2}{b_t^2}.
$$

Then all conditional distributions are Gaussian, so exact formulas are available:

- **$\epsilon$-prediction:** Since $\mathrm{Var}(\mathbb E[\epsilon \mid x_t])=\mathrm{Var}(b_tx_t)=b_t^2\,I$, substitute into Eq. \ref{eq:31}

  $$
  \begin{align}
  \mathrm{Var}(\epsilon \mid x_t) & = \mathrm{Var}(\epsilon) - \mathrm{Var}(\mathbb E[\epsilon \mid x_t]) \\[10pt]
  & = I - \frac{b_t^2}{a_t^2+b_t^2}\,I = \frac{a_t^2}{a_t^2+b_t^2}\,I = \frac{\text{SNR}}{1+\text{SNR}}\,I
  \end{align}
  $$
  
  small variance in low-SNR regime, large variance in high-SNR regime. The range varies between 0 and 1.

- **$x_0$-prediction:** Since $\mathrm{Var}(\mathbb E[x_0 \mid x_t])=\mathrm{Var}(a_tx_t)=a_t^2\,I$, substitute into Eq. \ref{eq:31}

  $$
  \begin{align}
  \mathrm{Var}(x_0 \mid x_t) & = \mathrm{Var}(x_0) - \mathrm{Var}(\mathbb E[x_0 \mid x_t]) \\[10pt]
  & = I - \frac{a_t^2}{a_t^2+b_t^2}\,I = \frac{b_t^2}{a_t^2+b_t^2}\,I = \frac{1}{1+\text{SNR}}\,I
  \end{align}
  $$
  
  large variance in low-SNR regime, small variance in high-SNR regime. The range varies between 0 and 1.

- **$v$-prediction:** Since $\mathrm{Var}(\mathbb E[v\mid x_t])=0$, substitute into Eq. \ref{eq:31}

  $$
  \mathrm{Var}(v \mid x_t) = \mathrm{Var}(v) - \mathrm{Var}(\mathbb E[v \mid x_t]) = I, \quad \text{}.
  $$
  
  independent of SNR, the variance is stable in all SNR regime.

- **Score-prediction:** Since $s = -\epsilon/b_t$,  substitute into Eq. \ref{eq:31}

  $$
  \mathrm{Var}(s \mid x_t) = \frac{1}{b_t^2}\,\mathrm{Var}(\epsilon \mid x_t) = \text{SNR}\cdot I.
  $$
  
  Variance is directly proportional to SNR. In the high-SNR region, the variance is extremely large.


The following figure shows conditional mean between four target values, with the horizontal axis corresponding to SNR and T respectively.

![Correlation vs SNR and t](/images/posts/post_2/condvar.png)

---

<h1 id="section3.3.2" style="color: #1D4ED8; font-size: 23px; font-weight: bold; text-decoration: underline;">3.3.2 Case B: When $x_0$ is non-Gaussian (realistic case)</h1>

For real-world data, $x_0$ is highly non-Gaussian (e.g., images). However, the **qualitative SNR trends** remain the same, since the corruption channel is Gaussian.


- **$\epsilon$-prediction:** low-SNR ‚áí small variance (stable); high-SNR ‚áí large variance (unstable).
- **$x_0$-prediction:** high-SNR ‚áí small variance (stable); low-SNR ‚áí large variance (unstable).
- **$v$-prediction:** variance not exactly constant, but relatively flat; no strong SNR bias.
- **Score-prediction:** inherits Œµ‚Äôs trend but amplified by $1/b_t$; variance grows with SNR ‚áí high-SNR instability.


---

<h1 id="section3.4" style="color: #1E40AF; font-size: 25px; font-weight: bold; text-decoration: underline;">3.4 Summary and Comparative Table</h1>


We had notice that no mather the data prior $p(x_0)$ is Gaussian or not, the qualitative trends is consistent with each other. That's because the channel is Gaussian ($x_t=a_tx_0+b_t\epsilon$ is a Gaussian-corrupted version of $x_0$), the **SNR regimes** have universal behavior, independent of the prior:

- **High-SNR ($a_t \gg b_t$)**: $x_t\approx a_t x_0$. The observation is dominated by the clean signal; the injected noise is nearly invisible.

- **Low-SNR ($b_t \gg a_t$)**: $x_t\approx b_t \epsilon$. The observation is dominated by noise.


These **qualitative trends** for the three metrics (correlation, conditional mean, conditional variance) therefore **do not depend** on assuming a Gaussian data prior, only the **closed-form expressions** require a Gaussian prior.



| Target               | Correlation (difficulty)         | Conditional Mean (signal) | Conditional Variance (stability)     | Net Effect                      |
| -------------------- | -------------------------------- | ------------------------- | ------------------------------------ | ------------------------------- |
| \$\epsilon\$         | High at low-SNR, low at high-SNR | Strong in low-SNR         | Stable in low-SNR, noisy in high-SNR | **Biased toward low-SNR**       |
| \$x\_0\$             | High at high-SNR, low at low-SNR | Strong in high-SNR        | Stable in high-SNR, noisy in low-SNR | **Biased toward high-SNR**      |
| \$v\$                | Zero (uniform)                   | Weak but uniform          | Constant (stable)                    | **Balanced but weaker**         |
| Score  | Mirrors \$\epsilon\$             | Roughly uniform           | Noisy in high-SNR, stable in low-SNR | **Unstable in high-SNR regime** |


---

<h1 id="section4" style="color: #1E3A8A; font-size: 28px; font-weight: bold; text-decoration: underline;">4. Stabilizing Diffusion Training via Optimization & Objective Design</h1>




The analysis in Section [3](#section3) makes it clear that a naive MSE loss, regardless of the chosen target, is suboptimal. The inherent bias of $\epsilon$ and $x_0$-prediction towards opposite ends of the SNR spectrum, and the weak signal of $v$-prediction, demand more sophisticated training strategies. 

In this section, we will explore how to make the training of diffusion models more stable, efficient, and accurate.

---

## <a id="section4.1">4.1 The root causes of instability and inefficiency in diffusion model training</a>



### <a id="section4.1.1">4.1.1 The universal perspective of instability and inefficiency</a>


The central challenge of training diffusion models is **not a single problem, but a multi-scale one**. A single neural network is asked to address a multi-scale problem spanning multiple orders of magnitude, whether viewed through the lens of discrete timesteps (t), physical signal-to-noise ratios (SNR), or continuous noise levels ($\sigma$). This inherent tension has led the research community down two distinct philosophical paths to achieve stability.

---

### <a id="section4.1.2">4.1.2 From t/SNR-space perspective</a>

When the process is parameterized by discrete timesteps $t$ (or equivalently by SNR), the aboved problem can be reformulated as: different timesteps correspond to highly imbalanced contributions, because the SNR curve induced by the schedule is extremely uneven.

From this perspective, the solution is therefore focus on **re-balancing the contribution of timesteps**, The goal is to externally adjust the training process to ensure a more uniform and effective contribution from all parts of the denoising chain, we will discuss in section [4.2](#section4.2).

---

### <a id="section4.1.1">4.1.3 From œÉ-space perspective</a>


When the process is parameterized by continuous noise scale $\sigma$, as in EDM [^edm], the aboved problem can be  reformulated as: the target itself changes scale drastically with different noise level, leading to poorly conditioned inputs and outputs unless explicitly normalized.

From this perspective, The solution is therefore focus on **reparameterizing and normalizing the task across noise levels**, the goal is to re-design the model framework itself to be inherently stable and scale-aware from the ground up, we will discuss in section [4.3](#section4.3).

---

### <a id="section4.1.2">4.1.4 Why Different Spaces matters</a>

Distinguishing these two space is critical. It is more than a matter of simple categorization; it reveals the evolution in our understanding of the problem itself. By separating our discussion along these lines, we can clearly understand the motivation, strengths, and limitations of each technique, moving from treating the symptoms to re-engineering the system from first principles.			

Although both views tackle the same root cause, they reinterpret the instability differently:

- t/SNR-space: imbalance arises from which timesteps are emphasized.

- $\sigma$-space: imbalance arises from how the denoiser is conditioned across noise levels.

Because the optimization levers are fundamentally different ‚Äî scheduling and weighting vs. preconditioning and reparameterization ‚Äî we will discuss them in separate subsections.

---

## <a id="section4.2">4.2 Stabilization in t-space (time-step parameterization)</a>

In this section, we discuss the t/SNR-space approach who treats instability as a scheduling and weighting imbalance problem.

---

### <a id="section4.2.1">4.2.1 Strategic Loss Weighting</a>

The most direct way to counteract imbalanced learning is to apply a weighting term $w(t)$ to the loss function, making the objective:

$$
\mathcal{L}_{w} =  \mathbb{E}_{t, x_t} \left[ w(t)\|y_t - f_{\theta}(x_t, t)\|^2 \right]\label{eq:39}
$$

Where **$w(t)=1$** in our previous discussion (vanilla diffusion training). The function $w(t)$ acts as a precision-engineered "equalizer" for our training process. It amplifies the learning signal where it's naturally weak and  suppress the loss when the gradient variance is large to prevent instability. The goal is to design $w(t)$ such that the effective learning task is normalized across all timesteps (or all SNR regions), we summarizes several mainstream weighting strategies as follows:


- **Perception Prioritized (P2) Weighting**: This innovative approach, introduced by Choi et al. [^p2], shifts the optimization goal from minimizing raw MSE to minimizing human-perceived error, often measured by metrics like LPIPS, where

  $$
  w(t) = \frac{1}{(\kappa + \text{SNR}(t))^\gamma}
  $$
  
  The paper recommends hyperparameters $\kappa=1$ and $\gamma=0.5$. This strategy is born from the observation that MSE is a poor proxy for perceptual quality. The authors found that most of the perceptual error occurs in the mid-to-high SNR range, even when the MSE is low. The P2 weighting scheme is therefore counter-intuitively designed to **down-weight the high-SNR regime** ($w(t) \to 0$ as $\text{SNR}(t) \to \infty$) and **up-weight the mid-to-low SNR regime** ($w(t) \approx 1$ as $\text{SNR}(t) \to 0$).
  
  The logic is to force the model to first learn the semantically crucial, large-scale structures of the image (governed by mid-to-low SNR steps) correctly. A well-formed global structure provides a better foundation for generating perceptually pleasing details, even if the MSE in the high-SNR details is not minimized as aggressively.


- **SNR-Based weighting**: This is a simple, elegant, and effective strategy that provides a balanced solution by weighting the loss directly by the SNR: $$ w(t) = \text{SNR}(t) $$ [^snr_based], Or, more generally, it can be formalized

  $$ 
  w(t) = (\text{SNR}(t))^{\gamma},\quad \gamma > 0 
  $$


  This approach cleverly addresses both primary difficulties simultaneously:
  
  1.  **At High SNR**: The weight is large, amplifying the weak signal of $\epsilon$-prediction and forcing the model to learn details.
  2.  **At Low SNR**: The weight is small, suppressing the large and noisy gradients common to $x_0$-prediction, thus promoting stability.
  
  It provides a balanced trade-off, ensuring that neither end of the SNR spectrum dominates the training process. However, due to the potentially large variation range of SNR, using logarithmic scaling ($w(t)=\log(\text{SNR}(t))$) can make the weight distribution across different SNR regions more balanced.
  
- **Min-SNR Weighting**: min-snr [^min_snr] sets an upper bound to suppress the high-SNR region.

  $$
  w(t) = \min(\text{SNR}(t), \tau)
  $$
  
  In both the official implementation and the Diffusers library, $\tau$ is set to 5 by default.
  1. **When $SNR(t) < \tau$ (Low-to-Mid SNR regime)**: The weight is $w(t) = \text{SNR}(t)$. The behavior is identical to standard SNR weighting.
  
  2. **When $SNR(t) \geq \tau$ (High SNR regime)**: The weight is "capped" at a constant upper bound, $w(t) = \tau$. It no longer increases as the SNR grows. 
  
  A massive weight in high-SNR area can cause the model to focus excessively on minuscule errors in high-frequency details. This can result in perceptually unpleasant artifacts, such as "over-sharpened" or "fried" textures, which harm the naturalness of the generated image. Min-SNR strategy acts as a "limiter" on the signal amplifier. It ensures that the model learns details sufficiently without the detrimental side effects of extreme weights.
  
  This balancing prioritizes **better perceptual quality and training stability** by preventing the model from becoming pathologically focused on details.

- **Max-SNR Weighting**: max-snr [^max_snr] sets an lower bound to ensure a minimum learning signal.
  
  $$
  w(t) = \max(\text{SNR}(t), \tau)
  $$
  
  1. **When $SNR(t) > \tau$ (Mid-to-High SNR regime)**: The weight is $w(t) = \text{SNR}(t)$. In this range, the behavior is identical to standard SNR weighting, leveraging a high weight to amplify the weak signal in the high-SNR region.
  
  2. **When $SNR(t) \leq \tau$ (Very Low SNR regime)**: The weight is "frozen" at a constant lower bound, $w(t) = \tau$. It no longer approaches zero as the SNR decreases. 
  
  
  When the weight is approach to 0 (in low-snr area), the model might learn that it receives no penalty for errors in these initial steps and therefore fails to learn how to meaningfully begin the denoising process. This can hinder the start of the generation or lead to artifacts in the final sample. Max-SNR strategy enforces a minimum level of supervision even in the noisiest regimes, and compels the model to learn a meaningful "first step" out of the noise distribution.
	
  
  Overall, This balancing prevents a "**blind spot**" in the learning process, ensuring the integrity and effectiveness of the entire denoising chain from pure noise to clean image.

- **SNR clipping**: This strategy is a robust combination of the previous two.
  
  $$
  w(t) = \min(\max(\text{SNR}(t), \tau_{min}), \tau_{max})
  $$
  
  1. **When $\text{SNR}(t) < \tau_{min}$ (Very Low SNR)**: $w(t) = \tau_{min}$ (the floor is active).
  2. **When $\tau_{min} \leq \text{SNR}(t) \leq \tau_{max}$ (Core SNR range)**: $w(t) = \text{SNR}(t)$ (standard behavior).
  3. **When $\text{SNR}(t) > \tau_{max}$ (Very High SNR)**: $w(t) = \tau_{max}$ (the ceiling is active).

  This strategy **boosts** the weights in the lowest SNR region and **suppresses** them in the highest SNR region, effectively creating a trapezoidal weighting curve. Theoretically speaking, it simultaneously optimize the learning signal, gradient stability, and perceptual quality. It is one of the most robust heuristic weighting schemes in practice.

---

### <a id="section4.2.2">4.2.2 Noise schedules</a>

In t-space parameterization, stability is tightly linked to **how the noise schedule is designed**. The forward process is defined through a sequence of variance increments $\{\beta_t\}_{t=1}^T$, which accumulate into $\alpha_t$ and $\bar\alpha_t$. Different choices of $\beta_t$ correspond to different trajectories of signal-to-noise ratio (SNR) as training progresses.


- **Linear schedule:** 
  This is the original implementation of DDPM [^ddpm], $\beta_t$ increases linearly from a small value $\beta_{min}$(e.g. $10^{-4}$) to a larger value $\beta_{max}$ (e.g. $0.02$).
  
  $$
  \beta_t = \beta_{\text{min}} + (t - 1) \cdot \frac{\beta_{\text{max}} - \beta_{\text{min}}}{T - 1}, \quad t = 1, 2, \dots, T
  $$

  

- **Scaled Linear Schedule:**  A variant of the linear schedule, tailored for latent diffusion models (e.g., Stable Diffusion). It scales the betas by taking the square root of the range, linearly interpolating, and then squaring the result. This creates a non-linear growth curve where betas start smaller and increase more gradually initially, helping with stability in latent spaces where noise scales differently. It's particularly useful for models trained on compressed representations to avoid over-noising early timesteps.

  $$
  \beta_t = \left( \sqrt{\beta_{\text{min}}} + (t - 1) \cdot \frac{\sqrt{\beta_{\text{max}}} - \sqrt{\beta_{\text{min}}}}{T - 1} \right)^2, \quad t = 1, 2, \dots, T
  $$
  
  In latent diffusion models (LDMs), the scaled linear noise schedule is preferred over the original linear schedule for several important reasons:
  - In latent space, because the representation has lower variance, using the same $\beta$ range (e.g., 0.0001 $\to$ 0.02 in linear schedule) breaks the SNR dynamics, noise is injected too aggressively, and the signal disappears too early, that means the model sees nearly pure noise for a large fraction of training, which is inefficient.
  
  - Scaled linear slows down noise growth with a narrower range of $\beta$ values (e.g., 0.00085 $\to$ 0.012 in SD), so that the SNR curve decays more smoothly, balances the learning signal, stabilizes training, and yields higher-quality outputs.
  
- **Squaredcos Cap V2 Schedule (Cosine Schedule)Ôºö** This strategy uses a cosine-based function to create a smoother, more gradual increase in noise. Rather than direct definition $\beta$, this relies on an intermediate function for the cumulative alpha $\bar{\alpha}_t$ [^iddpm].

  $$
  \bar{\alpha}(t) = \cos^2 \left( \frac{t / T + s}{1 + s} \cdot \frac{\pi}{2} \right), \quad s = 0.008 
  $$
  
  And then,

  $$
  \beta_t = \min \left( 1 - \frac{\bar{\alpha}(t)}{\bar{\alpha}(t-1)}, \ \beta_{\max} \right), \quad \beta_{\max} = 0.999
  $$
  
- **Exponential Schedule:** As the name suggests, the exponential noise schedule defines the rate of noise change in a manner that follows an exponential growth pattern. This schedule is designed to ensure that $\beta$ increases exponentially as the timestep t progresses. The function governing the exponential noise schedule is given by:

  $$ 
  \beta_t = \beta_{\text{start}} \cdot \left( \frac{\beta_{\text{end}}}{\beta_{\text{start}}} \right)^{\frac{t-1}{T-1}}, \quad t = 1, 2, \dots, T
  $$

- **Sigmoid Schedule:** This strategy uses a sigmoid function to create an S-shaped curve for $\beta_t$, starting near $\beta_{\text{min}}$, rising steeply in the middle, and plateauing near $\beta_{\text{max}}$. It‚Äôs particularly useful for tasks requiring rapid noise increase in mid-timesteps (e.g., GeoDiff or inpainting models), offering better control over multi-scale noise addition and improving training stability. 

  $$
  \beta_t = \sigma \left( -6 + 12 \cdot \frac{t-1}{T-1} \right) \cdot (\beta_{\text{end}} - \beta_{\text{start}}) + \beta_{\text{start}}, \quad t = 1, 2, \dots, T
  $$
  
  where $\sigma(x) = \frac{1}{1 + e^{-x}}$ is the logistic sigmoid function, scaled from -6 to 6 to cover the full range.

We plot a direct comparison of the SNR curves for these noise schedulers.

![Correlation vs SNR and t](/images/posts/post_2/SNR.jpg)

Many common Œ≤-schedules (linear, cosine, etc.) leave a small non-zero SNR at the last step. During training the model learns to exploit this residual signal, but during inference we usually start from pure noise. This **mismatch** causes artifacts like limited brightness or reduced dynamic range.

**Zero Terminal SNR (ZTSNR)** [^ZTSNR] is an effective solution to this problem, which enforces the SNR at the final timestep $SNR(T)=0$ to be exactly zero ‚Äî i.e., the sample at the terminal step should be pure Gaussian noise without any residual data signal.

Implementing ZTSNR in diffusion models involves modifying the noise schedule to ensure that the cumulative product of alphas ($\bar{\alpha}_T$) reaches zero at the terminal timestep $T$, effectively setting the SNR to zero. 

$$
\bar{\alpha}_T=0\,\qquad\,SNR(T)=\frac{\bar{\alpha}_T}{1- \bar{\alpha}_T}=0
$$

One effective method is to **rescale** the $\beta_t$ values to adjust the cumulative alpha product.

$$\beta_t' = \beta_t \cdot \frac{1 - \bar{\alpha}_1}{1 - \bar{\alpha}_T}$$

Recalculate $\alpha_t' = 1 - \beta_t'$ and $$\bar{\alpha}_t' = \prod_{i=1}^t \alpha_i'$$, verifying that $$\bar{\alpha}_T' \approx 0$$. This scales the original betas to ensure $\bar{\alpha}_T$ approaches zero while preserving the relative shape of the schedule.

---

## <a id="section4.3">4.3 Stabilization in œÉ-space (EDM and successors)</a>

In this section, we discuss the $\sigma$-space approach who treats instability as a conditioning and reparameterization problem.

---

### <a id="section4.3.1">4.3.1 EDM-Style Preconditioning</a>

In early diffusion models such as DDPM or DDIM, both training and sampling were defined in terms of a **discrete timestep $t \in \{1,\dots,T\}$**. A noise schedule $\{\beta_t\}$ was chosen, then transformed into $\alpha_t$ and $\bar{\alpha}_t$, and the network was trained with $t$ as its input.

The limitation of this viewpoint is that the **relationship between SNR and timestep $t$** depends entirely on the chosen schedule (linear, cosine, exponential, etc.). As a result, the effective learning task is **schedule-dependent**: the same $t$ can correspond to vastly different SNR levels and hence very different learning difficulties.

---

### <a id="section4.3.1.1">4.3.1.1 From $t$-space to $\sigma$-space</a>

EDM introduces a critical shift: **instead of modeling in the discrete timestep space, all training objectives and parameterizations are expressed directly in terms of the continuous noise scale $\sigma$.** Consequently, a noisy data point at any noise level is elegantly expressed as:

$$
x(\sigma) = x_0 + \sigma \,\epsilon, \quad \epsilon \sim \mathcal N(0,I).
$$
  
Here $\sigma$ is the standard deviation of the added Gaussian noise. It is directly linked to the signal-to-noise ratio (SNR):

  $$
  \text{SNR} = \frac{1}{\sigma^2}.
  $$
  
> **Important: All subsequent design choices‚Äîpreconditioning coefficients, loss weights, training distributions‚Äîare defined as functions of $\sigma$ rather than $t$**.

---


### <a id="section4.3.1.2">4.3.1.2 EDM Preconditioning Formulation</a>

The previous section on Strategic Loss Weighting aimed to correct the training objective, balancing the model's focus across different SNR regimes. However, the root of instability lies not just in the objective function but is also deeply embedded in how the network architecture handles inputs and outputs of dramatically varying scales.

From our "$\sigma$-space perspective", the instability of network training becomes exceptionally clear: as $\sigma$ varies from very small to very large values, the variance of the network input $x_t$, given by ${\text Var}(x_t) = {\text Var}(x_0) + \sigma_t^2$, and the scales of various potential prediction targets (like $x_0$ or $\epsilon$) change dramatically, often across several orders of magnitude.

Demanding a single, fixed neural network $f_Œ∏$ to effectively process inputs with a magnitude of $0.1$ in one forward pass and $100$ in another‚Äîwhile its output target undergoes similar wild fluctuations‚Äîposes a tremendous challenge for the optimizer and the network weights. This can lead to exploding or vanishing gradients at different timesteps, severely undermining training stability.

To remove scale inconsistencies across different noise levels, Karras et al. [^edm] introduced a pivotal technique: **Network Pre-conditioning**.  The core idea is to use simple, analytically-defined scaling functions to "normalize" the network's inputs and outputs. This ensures the core network $f_{\theta}$ always operates in a **well-behaved "comfort zone"** where its inputs and outputs have roughly constant, near-unit variance.. EDM expresses the pre-conditioned network (also known as denoiser, the target is $x_0$) as:

$$
F_{\theta}(x(\sigma), \sigma) = c_{\text{out}}(\sigma) \cdot f_{\theta}(c_{\text{in}}(\sigma) \cdot x(\sigma), c_{\text{noise}}(\sigma)) + c_{\text{skip}}(\sigma) \cdot x(\sigma)
$$



Here, $f_Œ∏$ is our core U-Net architecture, and the $c_...$ terms are simple scalar functions that depend only on the noise level $\sigma$. Let's precisely break down this formula in $\sigma$-space:

- **$c_{in}(\sigma)$ (Input Scaling)**: This term aims to counteract the scale variation of the input $x(œÉ)$. To give $f_{\theta}$'s input a constant variance, $c_{in}(\sigma)$ must cancel out the variance of $x(\sigma)$. Letting the clean data variance be $\sigma_{\text data}^2 = \mathrm{Var}(x_0)$, we have $\mathrm{Var}(x(\sigma)) = \sigma_{\text data}^2 + \sigma^2$. EDM therefore chooses:
  
  $$
  c_{in}(\sigma) = \frac{1}{\sqrt{\sigma^2 + \sigma_{\text data}^2}}
  $$
  
  which ensures that the U-Net sees inputs with similar statistical properties regardless of $\sigma$.


- **$c_{noise}(\sigma)$ (Noise Level Encoding)**: The continuous value of $\sigma$ (typically a transformation of its logarithm, like 
  
  $$
  c_{noise}(\sigma) = 0.25 \log(\sigma)
  $$ 
  
  which is fed directly as the conditioning, replacing discrete $t$ embeddings. This allows the network to reason over a continuum of noise levels.

- **$c_{skip}(\sigma)$ (Skip Scaling)**: This is the most ingenious part of the pre-conditioning framework. Let's consider the expression of pre-conditioned network $F_{\theta}$. We notice that it consists of two components: one is the linear part, and the other is the nonlinear part. **This semi-linear structure is crucial in the training and sampling of diffusion models**.

  $$
  F_{\theta}(x(\sigma), \sigma) = \underbrace{c_{\text{skip}}(\sigma) \cdot x(\sigma)}_{\text{Linear Baseline Prediction}} + \underbrace{c_{\text{out}}(\sigma) \cdot f_{\theta}(c_{\text{in}}(\sigma) \cdot x(\sigma), c_{\text{noise}}(\sigma))}_{\text{Non-linear Residual Correction}}
  $$
  
  Now, the meaning of $c_{\text{skip}}(\sigma) \cdot x(\sigma)$ is very clear: $c_{\text{skip}}(\sigma) \cdot x(\sigma)$ is the **best linear estimate** of the target signal ($x_0$) given the noisy input $x(\sigma)$. And as we learned in Section [3.2](#section3.2), in the sense of minimizing the mean squared error, this best linear estimate is the conditional expectation $\mathbb{E}[x_0 \mid x(\sigma)]$. The solution for $\mathbb{E}[x_0 \mid x(\sigma)]$ can be obtained through Equation \ref{eq:25}.
  
  $$
  \begin{align}
  \mathbb{E}(x_0 \mid x(\sigma)) & = \mathbb{E}(x_0) + \frac{\mathrm{Cov}(x_0, x(\sigma))}{\mathrm{Var}(x(\sigma))}\left( x(\sigma) - \mathbb{E}(x(\sigma)) \right) \\[10pt]
  & = 0 + \frac{\mathrm{Cov}(x_0, x_0+\sigma\,\epsilon)}{\mathrm{Var}(x(\sigma))}\left( x(\sigma) - 0 \right) \\[10pt]
  & = \frac{\sigma_{\text data}^2}{\sigma_{\text data}^2 + \sigma^2}\,x(\sigma)
  \end{align}
  $$
  
  this implies the best choice of $c_{skip}(\sigma)$.
  
  $$
  c_{skip}(\sigma) = \frac{\sigma_{\text data}^2}{\sigma_{\text data}^2 + \sigma^2}
  $$


- **$c_{out}(\sigma)$ (output Scaling)**: Since $c_{skip}$ has handled the linear part, what is left for the expensive U-Net $f_{\theta}$ to learn? It must learn the difference: the **non-linear residual**. By rearranging the pre-conditioning formula, we can see the learning target for $f_{\theta}$: 

  $$
  f_{\theta} \approx \frac{x_0 - c_{\text{skip}}(\sigma) \cdot x(\sigma)}{c_{\text{out}}(\sigma)}
  $$
  
  The numerator, $x_0 - c_{skip}(\sigma)\,x(\sigma)$, represents the error between the true signal $x_0$ and its best linear estimate. However, the variance of this error still changes dramatically with $\sigma$. If $f_{\theta}$ were to learn this error directly, its output scale would remain unstable. Specifically, we want the variance of $f_{\theta}$ is 1, this implies the choice of $c_{out}(\sigma)$ is.
  
  $$
  \begin{align}
  c_{skip}(\sigma) & = \sqrt{ \mathrm{Var}(x_0) -  c_{skip}^2(\sigma)\mathrm{Var}(x(\sigma))} \\[10pt]
  & = \frac{\sigma\,\sigma_{\text data}}{\sqrt{\sigma_{\text data}^2 + \sigma^2}}
  \end{align}
  $$


---

### <a id="section4.3.1.3">4.3.1.3 EDM as a Unified Interface for different targets</a>

An important perspective is that the preconditioned network $F_\theta$ acts as a **unified interface** (or container). Regardless of which prediction target we prefer ‚Äî $x_0$, $\epsilon$, $v$, or the score ‚Äî all can be obtained from $D_\theta$ by a **fixed linear transformation**.

| Prediction target                   | Expression in terms of $D_\theta(x;\sigma)$                   | Notes                                                                                                               |
| ----------------------------------- | ------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------- |
| Clean data ($x_0$)                  | $\hat x_0 = D_\theta(x;\sigma)$                               | $D_\theta$ is default designed as an denoiser, i.e. the optimal estimate of $x_0$.                                     |
| Noise ($\epsilon$)                  | $\hat\epsilon = \tfrac{x - D_\theta(x;\sigma)}{\sigma}$       | Direct inversion of the corruption process $x = x_0 + \sigma\epsilon$.                                              |
| Score ($\nabla_x \log p(x;\sigma)$) | $s(x;\sigma) = \tfrac{D_\theta(x;\sigma) - x}{\sigma^2}$ | EDM‚Äôs denoising‚Äìscore matching identity.                                                                            |
| $v$-prediction                      | $\hat v = \hat\epsilon - \sigma\,\hat x_0$      | $v$ is a linear combination of $(x_0,\epsilon)$; coefficients $a(\sigma), b(\sigma)$ depend on the forward process. |



Meanwhile, $F_\theta$ itself is not any of these targets. It is purely a **residual learner** operating under normalized conditions. No matter which target we adopt, $F_\theta$ always sees input and supervision with variance ‚âà 1, and learns a goal-agnostic residual. This design delivers stability in multiple ways:

1. **Normalized inputs** ($c_{\text{in}}$): consistent scale for all $\sigma$.
2. **Uniform outputs** ($c_{\text{out}}$): targets with equalized variance.
3. **Balanced skip path** ($c_{\text{skip}}$): statistically optimal linear baseline, reducing residual magnitude.
4. **Stable noise embedding** ($c_{\text{noise}}$): well-scaled conditioning variable.
5. **Target-agnostic residual learning**: by treating $D_\theta$ as a universal interface, EDM guarantees that switching objectives does not destabilize $F_\theta$‚Äôs training.


Preconditioning Coefficients for Different Targets

| ÁõÆÊ†á           | $c_{\text{skip}}(\sigma)$                                         | $c_{\text{out}}(\sigma)$                                                       | $c_{\text{in}}(\sigma)$                             | $c_{\text{noise}}(\sigma)$ |
| ------------ | ----------------------------------------------------------------- | ------------------------------------------------------------------------------ | --------------------------------------------------- | -------------------------- |
| $x_0$        | $\dfrac{\sigma_{\text{data}}^2}{\sigma_{\text{data}}^2+\sigma^2}$ | $\dfrac{\sigma\,\sigma_{\text{data}}}{\sqrt{\sigma_{\text{data}}^2+\sigma^2}}$ | $\dfrac{1}{\sqrt{\sigma_{\text{data}}^2+\sigma^2}}$ | $\tfrac14\ln\sigma$        | 
| $\epsilon$   | $\dfrac{\sigma}{\sigma_{\text{data}}^2+\sigma^2}$                 | $\dfrac{\sigma_{\text{data}}}{\sqrt{\sigma_{\text{data}}^2+\sigma^2}}$         | $\dfrac{1}{\sqrt{\sigma_{\text{data}}^2+\sigma^2}}$                                                  | $\tfrac14\ln\sigma$                          |
| $v$      | $0$                                                               | $1$                                                                            | $\dfrac{1}{\sqrt{\sigma_{\text{data}}^2+\sigma^2}}$                                                  | $\tfrac14\ln\sigma$                          |
| score  | $-\dfrac{1}{\sigma_{\text{data}}^2+\sigma^2}$                     | $\dfrac{\sigma_{\text{data}}}{\sigma\sqrt{\sigma_{\text{data}}^2+\sigma^2}}$   | $\dfrac{1}{\sqrt{\sigma_{\text{data}}^2+\sigma^2}}$                                                  | $\tfrac14\ln\sigma$                          | 


---



### <a id="section4.3.2">4.3.2 Sigma sampling distributions</a>

Along the new preconditioning network $F_{\theta}$, and given a chosen training target $T(\sigma)$ (which may be $x_0,\ \epsilon,\ v,$ or the score), we reconstruct the loss function.

$$
\boxed{\quad
\mathcal{L}(\theta)
=\mathbb{E}_{\sigma\sim p(\sigma)}\ \mathbb{E}_{x_0,\epsilon}\left[
\lambda(\sigma)\ \big\|\,F_\theta(x;\sigma)-T(\sigma)\,\big\|_2^2
\right].\quad}
$$

where $p(\sigma)$ is the **sigma sampling distribution**, and $\lambda(\sigma)$ is the **loss weight**. The primary difference from Equation \ref{eq:39} lies in the fact that we sample in the $\sigma$ space rather than the t space.


Once the EDM-style preconditioning framework is in place, the next critical design choice concerns **how to sample the noise scale $\sigma$** during training. Unlike classical diffusion models, which typically draw timesteps $t$ uniformly from $[0,1]$, EDM samples $\sigma$ values directly from a **log-normal distribution**. This subtle change dramatically improves training stability and efficiency.

In the following discussion, we examine three sampling strategies: Line Uniform, $\log$ Uniform, and $\log$ Normal.  All sampling strategies are performed within the interval from $\sigma_{\text min}=0.002$ to $\sigma_{\text max}=80$.

- **The Naive Approach: Linear Uniform Sampling on $\sigma$** 

  The simplest and most intuitive way to train a model across a range of noise levels from $\sigma_{\text min}$ to  is to sample $\sigma$ uniformly from this interval. While mathematically simple, this approach is disastrously inefficient from an information-theoretic perspective. **The core problem is that linear distance in $\sigma$-space does not correspond to a linear change in learning difficulty or perceptual quality**. Let's consider the range $[0.002, 80]$.
  - The sub-interval $[40, 80]$ has a length of 40.
  - The sub-interval $[0.002, 40.002]$ also has a length of 40.

  Under a linear uniform sampling scheme, these two intervals will receive an **equal number of training samples**‚Äîapproximately 50% of the total training budget each. However, this two sub-intervals are equal in terms of distance, but unbalanced in terms of information.

  - **In $\sigma \in [40, 80]$:** The noise term $\sigma \epsilon$ completely dominates the data term $x_0$. The SNR is exceptionally low. The model is essentially tasked with reconstructing a masterpiece from a blizzard of pure static. It can only learn the coarsest, most averaged-out features of the data distribution. A vast amount of computational resource is spent on a pure noise region with minimal learning value.
  - **In $\sigma \in [0.002, 40.002]$:** This single interval contains the entire meaningful learning journey. It's where the model learns to form global structures ($\sigma \approx 30$), define semantic content ($\sigma \approx 10$), add textures and medium-frequency details ($\sigma \approx 1$), and perform final, high-fidelity refinement ($\sigma \approx 0.1$).

  By sampling $\sigma$ linearly, we force the model to spend half its time learning from static, effectively wasting a massive portion of the training budget.


- **A Major Leap Forward: Log-Uniform Sampling**

  The fundamental flaw of the linear approach is its failure to recognize that noise levels are best understood in terms of **orders of magnitude**. The perceptual difference between $\sigma=0.1$ and $\sigma=1$ is far more significant than the difference between $\sigma=70.1$ and $\sigma=71$. This insight leads us directly to the logarithmic scale.

  Instead of sampling $\sigma$ uniformly, we sample $\log(\sigma)$ uniformly from the interval $[\log(\sigma_{\text min}), \log(\sigma_{\text max})]$. This is equivalent to $\sigma$ following a Log-Uniform distribution.

  This change in perspective is transformative. Let's revisit our comparison:
  - The interval $[0.1, 1]$ in log-space has a "length" of $\log(1) - \log(0.1) \approx 2.3$.
  - The interval $[1, 10]$ in log-space has a "length" of $\log(10) - \log(1) \approx 2.3$.
  - The interval $[10, 80]$ in log-space has a "length" of $\log(80) - \log(10) \approx 2.08$.

  Under Log-Uniform sampling, these intervals‚Äîrepresenting distinct orders of magnitude‚Äîreceive a **roughly equal allocation of training samples**. This aligns perfectly with the denoising task. The model now spends a balanced amount of effort learning to:
  - Transition from high noise to medium noise (e.g., $\sigma=10 \quad \to \quad \sigma=1$).
  - Transition from medium noise to low noise (e.g., $\sigma=1 \quad \to \quad \sigma=0.1$).

  On a log-scaled histogram, this distribution appears flat, confirming that each decade of `œÉ` is treated with equal importance. This strategy rectifies the catastrophic misallocation of the linear uniform approach and provides a robust, principled, and parameter-free (besides the boundaries) baseline.

- **The EDM Approach: Log-Normal Sampling**


  Log-Uniform sampling is a massive improvement, but it rests on a new assumption: that all orders of magnitude are equally important to learn from. The designers ($D_{\theta}$) of the EDM framework challenged this. Is learning to sculpt form from near-total noise ($\sigma=50 \quad \to \quad \sigma=5$) just as critical as adding the final, photorealistic details ($\sigma=0.5 \quad \to \quad \sigma=0.05$)?

  There is likely a "sweet spot"‚Äîa critical phase in the denoising process where the most complex and semantically meaningful features of the data are learned. Log-Normal Sampling strategy proposes that $\log(\sigma)$ should not be sampled uniformly, but from a **Normal (Gaussian) distribution**: 
  
  $$
  \log(\sigma) \sim \mathcal{N}(P_{\text mean}, P_{\text std}^2)
  $$
  
  Consequently, $\sigma$ itself follows a Log-Normal distribution. This approach abandons the idea of equal effort and instead adopts a strategy of **focused learning**. The Gaussian distribution in log-space creates a peak, concentrating the majority of training samples around a specific noise level, with density tapering off towards the extremes.

  - **Targeting the "Sweet Spot":** The key is that the denoising task is not uniformly difficult across scales.
    - **High-$\sigma$ Regime:** Learning coarse, global layouts. The task is relatively simple.
    - **Mid-$\sigma$ Regime:** This is often the most critical phase. The model transitions from abstract blobs to recognizable semantic content‚Äîforming faces, defining objects, creating complex textures. This is arguably the most difficult and information-rich part of the process.
    - **Low-$\sigma$ Regime:** Fine-tuning, removing minor artifacts, and adding high-frequency texture. This is a refinement task.

    The Log-Normal distribution, by choosing an appropriate $P_{\text mean}$ (e.g., -1.2 in the EDM paper, corresponding to $\sigma \approx 0.3$), focuses the model's training effort squarely on the crucial mid-to-low `œÉ` regime where core content is synthesized.

  - **Introducing Control via Hyperparameters:** The apparent downside of this approach is the introduction of two hyperparameters, $P_{\text mean}$ and $P_{\text std}$. However, these are not arbitrary constants but powerful **design knobs**:
    - $P_{\text mean}$: This parameter acts like a spotlight, setting the **center of gravity** for the training process. It allows researchers to target the most relevant noise scale for a given task.
    - $P_{\text std}$: This parameter controls the **focus** of the spotlight. A small $P_{\text std}$ creates a tight, focused distribution for tasks that operate in a narrow $\sigma$ range (e.g., super-resolution). A larger $P_{\text std}$ creates a broader distribution, closer to Log-Uniform, suitable for general-purpose image generation.

We apply these three sampling strategies respectively for sampling. Based on 500,000 samplings, a unified log-space histogram will be obtained

![Three sigma sampling strategies comparision](/images/posts/post_2/sigma_sampling.jpg)

- Linear Uniform: although flat in linear space, it allocates the majority of samples to the large-$\sigma$ end, since that region spans most of the interval.

- Log Uniform: distributes training budget approximately equally across each order of magnitude, giving all noise scales comparable attention.

- Log-normal: concentrates sampling density around the mid-range near $\sigma \approx 0.3$, where residuals are most informative, while still maintaining coverage of both low- and high-noise extremes.

---

### <a id="section4.3.3">4.3.3 Loss weighting in $\sigma$-space</a>

Even with a good sampling distribution, contributions across $\sigma$ can still be skewed unless losses are normalized. Returning to the preconditioned head. Define the normalized residual

$$
R(\sigma)=\frac{T(\sigma)-c_{\text{skip}}(\sigma)\,x}{c_{\text{out}}(\sigma)}.
$$

Then the loss can be written as

$$
\begin{align}
\mathcal{L}(\theta)
& =\mathbb{E}_{\sigma\sim p(\sigma)}\ \mathbb{E}_{x_0,\epsilon}\Big[
\lambda(\sigma)\ \big\|\,F_\theta(x;\sigma)-T(\sigma)\,\big\|_2^2
\Big] \\[10pt]
& = \mathbb{E}_{\sigma\sim p(\sigma)}\ \mathbb{E}_{x_0,\epsilon}\Big[
\lambda(\sigma)\ \big\|\,  c_{\text{out}}f_{\theta}(c_{\text{in}}x, c_{\text{noise}}) + c_{\text{skip}}\,x-T(\sigma)\,\big\|_2^2
\Big] \\[10pt]
& = \mathbb{E}_{\sigma\sim p(\sigma)}\ \mathbb{E}_{x_0,\epsilon}\Big[
\lambda(\sigma)\,c_{\text{out}}(\sigma)^2
\big\|f_\theta(c_{\text{in}}x;\ c_{\text{noise}})-{\frac{T(\sigma)-c_{\text{skip}}(\sigma)\,x}{c_{\text{out}}(\sigma)}}\big\|^2 \Big] \\[10pt]
& = \mathbb{E}_{\sigma\sim p(\sigma)}\ \mathbb{E}_{x_0,\epsilon}\Big[
\lambda(\sigma)\,c_{\text{out}}(\sigma)^2
\big\|f_\theta(c_{\text{in}}x;\ c_{\text{noise}})-R(\sigma)\big\|^2 
\Big]
\end{align}
$$

EDM set $$\lambda(\sigma)={1}/{c_{\text{out}}(\sigma)^2}$$, so that the factor cancels, leaving

$$
\mathcal{L}(\theta)
=\mathbb{E}_{\sigma\sim p(\sigma)}\ \mathbb{E}_{x_0,\epsilon}\Big[
\|f_\theta(c_{\text{in}}x;\ c_{\text{noise}})-R(\sigma)\|^2
\Big].
$$

Thus across all $\sigma$:

- $f_\theta$ always learns residuals of **unit variance**,
- gradient magnitudes are comparable,
- no single noise level dominates.

---

## <a id="section4.4">4.4 General Engineering Stabilization Techniques</a>

---

### <a id="section4.4.1">4.4.1 General numerical stability and Optimizer configurations</a>

Gradient clipping, mixed precision training, loss scaling, batch accumulation. AdamW, learning rate schedules (warmup, cosine decay, inverse sqrt).

---

### <a id="section4.4.2">4.4.2 EMA and Post-hoc EMA</a>

---


# <a id="section5">5. Stabilizing Diffusion Training via Network Architecture</a>


The preceding analysis has focused on algorithmic components crucial for the stable and efficient training of diffusion models. These include optimizer selection, the parameterization of the objective function (e.g., Œµ-prediction versus v-prediction), and various loss weighting strategies designed to balance the learning process across the noise schedule. While these methodologies are essential for refining the optimization landscape, an equally critical factor lies in the design of the network architecture itself.

The architectural backbone itself ‚Äî from the foundational U-Net to more recent Transformer-based designs ‚Äî fundamentally dictates the model's expressive capacity, inductive biases, and inherent numerical stability. The structural properties of the network exert a profound influence on its ability to capture complex data distributions and adhere to conditioning signals. As this foundation is of commensurate importance to the optimization protocols, a comprehensive examination of the architectural evolution of diffusion models is presented in our next [article](https://innovation-cat.github.io/posts/2025/03/diffusion-training-2/).


---


# <a id="section6">References</a>

[^iedm]: Karras T, Aittala M, Lehtinen J, et al. Analyzing and improving the training dynamics of diffusion models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 24174-24184.

[^edm]: Karras T, Aittala M, Aila T, et al. Elucidating the design space of diffusion-based generative models[J]. Advances in neural information processing systems, 2022, 35: 26565-26577.

[^Kingma]: Kingma D, Gao R. Understanding diffusion objectives as the elbo with simple data augmentation[J]. Advances in Neural Information Processing Systems, 2023, 36: 65484-65516.

[^Salimans]: Salimans T, Ho J. Progressive distillation for fast sampling of diffusion models[J]. arXiv preprint arXiv:2202.00512, 2022.

[^p2]: Choi J, Lee J, Shin C, et al. Perception prioritized training of diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 11472-11481.

[^min_snr]: Hang T, Gu S, Li C, et al. Efficient diffusion training via min-snr weighting strategy[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2023: 7441-7451.

[^max_snr]: Salimans T, Ho J. Progressive distillation for fast sampling of diffusion models[J]. arXiv preprint arXiv:2202.00512, 2022.

[^snr_based]: Kingma D, Gao R. Understanding diffusion objectives as the elbo with simple data augmentation[J]. Advances in Neural Information Processing Systems, 2023, 36: 65484-65516.

[^ddpm]: Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[J]. Advances in neural information processing systems, 2020, 33: 6840-6851.

[^iddpm]: Nichol A Q, Dhariwal P. Improved denoising diffusion probabilistic models[C]//International conference on machine learning. PMLR, 2021: 8162-8171.

[^ZTSNR]: Lin S, Liu B, Li J, et al. Common diffusion noise schedules and sample steps are flawed[C]//Proceedings of the IEEE/CVF winter conference on applications of computer vision. 2024: 5404-5411.