---
title: 'Analysis of the Stability of Diffusion Model Training'
date: 2025-01-23
permalink: /posts/2025/01/diffusion-model-2/
tags:
  - cool posts
  - category1
  - category2
---

<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>üìö Table of Contents</strong></span>
  </summary>
  <ul style="margin:0; padding-left:1.1rem;">
    <li><a href="#section1">Brief recap</a>
		<ul>
		  <li><a href="#section1.1">From Maximum likelihood to ELBO</a></li>
		  <li><a href="#section1.2">From KL collapses to a mean MSE</a></li>
		</ul>
	</li>
    <li><a href="#section2">Re-parameterising the mean with different target predictor</a>
		<ul>
		  <li><a href="#section2.1">x<sup>0</sup>-prediction</a></li>
		  <li><a href="#section2.2">&epsilon;-prediction</a></li>
		  <li><a href="#section2.3">v-prediction</a></li>
		</ul>
	</li>
  </ul>
</details>



Diffusion models have achieved unprecedented success in the field of generative modeling, producing incredibly high-fidelity images, audio, and other data. However, the training process of these models presents several unique challenges like divergence, vanishing gradients, or unstable training behavior during the learning process. A stable training process ensures that the model produces good quality samples and converges efficiently over time without suffering from numerical instabilities.

Training diffusion models can be challenging and unstable due to several factors:

  - **Noise scheduling**, which controls how noise is injected during the forward process and removed during the reverse process. The choice of noise schedule (linear, cosine, or others) directly impacts training stability. In particular, schedules that introduce large or small variations in noise at different time steps can lead to vanishing or exploding gradients. 
  
  - **Prediction Target**, 

---

# <a id="section1">Brief recap</a>

For generative models, we expect our model $p_{\theta}$ (parameterized by $\theta$) to be as close as possible to the true distribution $p_{data}$. Based on the KL divergence, we derive that

$$
\mathbb{KL}(p_{data}(x) \parallel p_{\theta}(x)) = \int p_{data}(x)\log (p_{data}(x))dx - \int p_{data}(x)\log(p_{\theta}(x))dx\label{eq:1}
$$

The first term, $\int p_{data}(x) \log (p_{data}(x))dx$, is the entropy of the true distribution
$p_{data}$, it is a constant with respect to the model parameters $\theta$. The second term, $\int p_{data}(x)\log(p_{\theta}(x))dx$, is the expected log-likelihood of the model under the true distribution. Thus, minimizing KL divergence is equal to maximize log-likelihood $p_{\theta}(x)$, where $x \sim p_{data}$.

## <a id="section1.1">From Maximum likelihood to ELBO</a>

Let $x_0$ be the original image, and $x_i (i=1,2,...,T)$ be the image with noise added to $x_0$. We wish to maximise 

$$
\log p_{\theta}(x_0)=\log \int p_{\theta}(x_{0:T}) dx_{1:T} \label{eq:2}
$$

Introduce the forward process $q(x_{1:T} \mid x_0)$ (a Markov chain with fixed noise‚Äëschedule). Using Jensen‚Äôs inequality gives the evidence lower bound:

$$
\begin{align}
\log p_\theta(x_0) \geq \mathcal{L}_\text{ELBO} & = \mathbb{E}_q \left[ \log p_\theta(x_0 \mid x_1) - \log \frac{q(x_{T} \mid x_0)}{p_\theta(x_{T})} - \sum_{t=2}^T \log \frac{q(x_{t-1} \mid x_t, x_0)}{p_\theta(x_{t-1} \mid x_t)} \right]\label{eq:3}
\end{align}
$$

The first term is reconstruction loss, the second term is prior matching, both of them are extremely small and can be ignored. Therefore, what we are truly concerned about is the third item, which also known as denoising term.


## <a id="section1.2">From KL collapses to a mean MSE</a>

For each denoising step, both forward posterior $q(x_{t-1} \mid x_t, x_0) \sim \mathcal{N}(\mu_{q}, \sigma_{q}^2I)$ and backward posterior
$p_{\theta}(x_{t-1} \mid x_t) \sim \mathcal{N}(\mu_{\theta}, \sigma_{\theta}^2I)$ are gaussian distributions. For two Gaussians with identical covariance, if we fix the two variances are equal to $\sigma_{q}^2$, then the KL divergence is equal to:

$$
\mathbb{KL}\left(q(x_{t-1} \mid x_t, x_0) \parallel p_{\theta}(x_{t-1} \mid x_t) \right) = \frac{1}{2\sigma_q^2} \|{\mu}_{q} - \mu_{\theta}(x_t, t)\|_2^2 + \text{const}
$$

Hence, for each denoising step, the loss function equals to

$$
\mathcal{L}_{\text{denoise}} =  \mathbb{E}_q \left[ \|\mu_q - \mu_{\theta}(x_t, t)\|^2 \right] 
$$

$\mu_q$ is the true target we want to predict, How do we calculate the value of 
$\mu_q$? Let's first decompose forward posterior $q(x_{t-1} \mid x_t, x_0)$ :

$$
q(x_{t-1} \mid x_t, x_0)=\frac{q(x_{t} \mid x_{t-1})q(x_{t-1} \mid x_{0})}{q(x_{t} \mid x_{0})} \propto q(x_{t} \mid x_{t-1})q(x_{t-1} \mid x_{0})
$$

where 

$$
q(x_{t} \mid x_{t-1}) \sim \mathcal{N}(x_{t-1};\mu_1, \sigma_1^2I),\ \ \mu_1=\frac{1}{\sqrt{\alpha_t}}x_{t},\ \ \sigma_1^2=\frac{1-\alpha_t}{\alpha_t}\label{eq:6} \\[10pt] q(x_{t-1} \mid x_{0})  \sim \mathcal{N}(x_{t-1};\mu_2, \sigma_2^2I), \ \ \mu_2=\sqrt{\bar \alpha_{t-1}}x_{0},\ \  \sigma_2^2=1-\bar \alpha_{t-1}
$$

The product of two Gaussian distributions is itself a Gaussian distribution, 

$$
\begin{align}
\mu_{q} = \frac{\mu_1\sigma_2^2+\mu_2\sigma_1^2}{\sigma_1^2+\sigma_2^2}  & =\frac{\sqrt{\alpha_t}(1-\bar {\alpha_{t-1}})x_t+\sqrt{\bar {\alpha_{t-1}}}(1-\alpha_t)x_0}{1-\bar \alpha_t} \label{eq:8}
\end{align}
$$

Combining equations 1 and 2, Our goal is to construct a neural network $\mu_{\theta}$, which takes $x_t$ and $t$ as inputs, such that the output of the network is as close as possible to $\mu_q$.


--- 

# <a id="section2">Re-parameterising the mean with different target predictor</a>

Following equation \ref{eq:8}, we can dirrectly build a network $\mu_{\theta}$ to output $\mu_{q}$. However, in practice, we usually do not directly fit the value of $\mu_{q}$, mainly due to the following reasons.

- $\mu_{q}$ is an affine function of $x_t$, which is known at training and test time, there is no need for the network to ‚Äúreproduce‚Äù it. If we regress $\mu_{q}$ directly, the network wastes capacity relearning a large known term and must also learn the residual that actually depends on the unknown clean content. 

- The mean target value is highly time-dependent scaling across $t$, which means that the output of the network is unstable, it is usually extremely difficult for a network to output results with a large variance range.


Instead of asking the network to output $\mu_{q}$ directly, the community typically uses four common prediction targets to train diffusion models: $\epsilon$-prediction, $x_0$-prediction, $v$-prediction, score-prediction. If we regard the original image $x_0$ and noise $\epsilon$ as two orthogonal dimensions, then All the common targets are linear in $(x_0, \epsilon)$


## <a id="section2.1">$x_0$-prediction (aka sample-prediction in Diffusers)</a>


## <a id="section2.2">$\epsilon$-prediction</a>

## <a id="section2.3">$v$-prediction</a>

## score-prediction 


---

# Training Objective


If we 

All prediction can be rewrited as: 

$$
\mathcal{L}=\mathbb{E}_{t, x_t}\parallel M_{\theta}(x_t, t) - target \parallel ^2
$$

The three targets are linearly related,

$$
\epsilon=\frac{x_t-\sqrt{\bar \alpha_t}x_0}{\sqrt{1-\bar \alpha_t}}, \ \ \ x_0 = \frac{x_t-\sqrt{1-\bar \alpha_t}\epsilon}{\sqrt{\bar \alpha_t}}, \ \ \ v=\sqrt{\alpha_t}\epsilon-\sqrt{1-\bar \alpha_t}x_0
$$

Because all three yield the same analytic mean, replacing one by another does not change the log‚Äëlikelihood bound‚Äîbut it does change the conditioning and the gradient statistics seen by the network.


All three neural networks are 

## $\epsilon$-prediction




## $x_0$-prediction


## $v$-prediction

## score-prediction 

## what is the different


 
---

# Noise Schedule and SNR 

---

# Conclusion

---

# References


