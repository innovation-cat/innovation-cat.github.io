---
title: 'Analysis of the Stability of Diffusion Model Training'
date: 2025-01-23
permalink: /posts/2025/01/diffusion-model-2/
tags:
  - cool posts
  - category1
  - category2
---

## Brief recap

For generative models, we expect our model $p_{\theta}$ (parameterized by $\theta$) to be as close as possible to the true distribution $p_{data}$. Based on the KL divergence, we derive that

$$
KL(p_{data}(x) \parallel p_{\theta}(x)) = \int p_{data}(x)\log (p_{data}(x))dx - \int p_{data}(x)\log(p_{\theta}(x))dx
$$

The first term, $\int p_{data}(x) \log (p_{data}(x))dx$, is the entropy of the true distribution
$p_{data}$, it is constant with respect to the model parameters $\theta$. The second term, $\int p_{data}(x)\log(p_{\theta}(x))dx$, is the expected log-likelihood of the model under the true distribution. Thus, minimizing KL divergence is equal to log-likelihood $p_{\theta}(x)$, where $x  \sim p_{data}$.

### From Maximum likelihood to ELBO

We wish to maximise 

$$
\log p_{\theta}(x_0)=\log \int p_{\theta}(x_{0:T}) dx_{1:T}
$$

Introduce the forward process $q_{\phi}(x_{1:T} \mid x_0)$(a Markov chain with fixed noise‑schedule). Using Jensen’s inequality gives the evidence lower bound:

$$
\begin{align}
\log p_\theta(x_0) \geq \mathcal{L}_\text{ELBO} & = \mathbb{E}_q \left[ \log p_\theta(x_0 \mid x_1) - \log \frac{q(x_{T} \mid x_0)}{p_\theta(x_{T})} - \sum_{t=2}^T \log \frac{q(x_{t-1} \mid x_t, x_0)}{p_\theta(x_{t-1} \mid x_t)} \right] \\
& = \mathbb{E}_q \log p_\theta(x_0 \mid x_1) - KL \left( q(x_{T} \mid x_0) \parallel p_\theta(x_{T}) \right) - \sum_{t=2}^{T} KL \left( q(x_{t-1} \mid x_t, x_0) \parallel p_\theta(x_{t-1} \mid x_t) \right) 
\end{align}
$$

The first term is reconstruction loss, the second term is prior matching, Therefore, what we are truly concerned about is the third item, which also known as denoising term.


### From KL collapses to a mean MSE

For each denoising step, both forward posterior $q_{\phi}(x_{t-1} \mid x_t, x_0)$ and backward posterior 
$p_{\theta}(x_{t-1} \mid x_t)$ are gaussian distributions, For two Gaussians with identical covariance:

$$
KL = \frac{1}{2\beta_t} \|\tilde{\mu}_t - \mu_\theta(x_t, t)\|_2^2 + \text{const}.
$$

Hence, for each denoising step.

$$
\mathcal{L}_{\text{denoise}} =  \mathbb{E}_q \left[ \frac{1}{2\beta_t} \|\tilde{\mu}_t - \mu_\theta(x_t, t)\|^2 \right].
$$


## Training Objective



### \epsilon-prediction


### $x_0$-prediction


### $v$-prediction

Headings are cool
======

You can have many headings
======

Aren't headings cool?
------