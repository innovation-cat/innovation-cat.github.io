---
title: 'Analysis of the Stability of Diffusion Model Training'
excerpt: "while diffusion models have revolutionized generative AI, their training challenges stem from a combination of resource intensity, optimization intricacies, and deployment hurdles. A stable training process ensures that the model produces good quality samples and converges efficiently over time without suffering from numerical instabilities."
date: 2025-01-23
permalink: /posts/2025/01/diffusion-model-2/
tags:
  - Diffusion Model
  - SNR
  - Model Training
---

<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>üìö Table of Contents</strong></span>
  </summary>
  <ul style="margin:0; padding-left:1.1rem;">
    <li><a href="#section1">1. Preliminary</a>
		<ul>
		  <li><a href="#section1.1">1.1 From Maximum likelihood to ELBO</a></li>
		  <li><a href="#section1.2">1.2 From KL collapses to a mean MSE</a></li>
		</ul>
	</li>
    <li><a href="#section2">2. Re-parameterising the mean with different target predictor</a>
		<ul>
		  <li><a href="#section2.1">2.1 x<sub>0</sub>-prediction</a></li>
		  <li><a href="#section2.2">2.2 &epsilon;-prediction</a></li>
		  <li><a href="#section2.3">2.3 v-prediction</a></li>
		  <li><a href="#section2.4">2.4 Score-prediction</a></li>
		</ul>
	</li>
	<li><a href="#section3">3. A Unified Analysis: Why Targets Differ in Practice</a>
		<ul>
			<li><a href="#section3.1">3.1 Correlation: Measuring Task Difficulty</a></li>
			<li><a href="#section3.2">3.2 Conditional Mean: Measuring Signal Strength</a></li>
			<li><a href="#section3.3">3.3 Conditional Variance: Measuring Gradient Stability</a></li>
			<li><a href="#section3.4">3.4 Summary and Comparative Table</a></li>
		</ul>
	</li>	
	<li><a href="#section4">4. SOTA Solutions: Balancing SNR in Diffusion Training</a></li>
		<ul>
			<li><a href="#section4.1">4.1 Strategic Loss Weighting</a></li>
			<li><a href="#section4.2">4.2 EDM-style Preconditioning</a></li>
			<li><a href="#section4.3">4.3 Strategic Loss Weighting</a></li>
		</ul>
	<li><a href="#section4">Amplification of Network Prediction Errors Over Timesteps</a></li>
	<li><a href="#section5">Conclusion</a></li>
	<li><a href="#section6">References</a></li>
  </ul>
</details>


Diffusion models have achieved remarkable success in generative modeling, producing state‚Äëof‚Äëthe‚Äëart results in image, audio, and multimodal generation. Yet training them remains notoriously difficult. Instabilities such as vanishing gradients, exploding loss values, and imbalanced learning across timesteps often hinder efficient convergence.

At the core of these issues lies the choice of **training objective**. Although the canonical derivation from maximum likelihood leads naturally to an evidence lower bound (ELBO) and finally to a mean‚Äësquared error (MSE) objective, how we reparameterize the regression target strongly affects optimization dynamics. Four parameterizations are commonly used in practice‚Äî$x_0$, $\epsilon$, $v$, and score prediction. They are mathematically equivalent but exhibit different levels of stability, gradient flow, and ease of optimization.

This post focus on diffusion models training - why diffusion training is unstable, how the four objectives differ, and how modern solutions re‚Äëbalance training dynamics.

---

# <a id="section1">1. Preliminary</a>

For generative models, we expect our model $p_{\theta}$ (parameterized by $\theta$) to be as close as possible to the true distribution $p_{data}$. Based on the KL divergence, we derive that

$$
\mathbb{KL}(p_{data}(x) \parallel p_{\theta}(x)) = \int p_{data}(x)\log (p_{data}(x))dx - \int p_{data}(x)\log(p_{\theta}(x))dx\label{eq:1}
$$

The first term, $\int p_{data}(x) \log (p_{data}(x))dx$, is the entropy of the true distribution
$p_{data}$, it is a constant with respect to the model parameters $\theta$. The second term, $\int p_{data}(x)\log(p_{\theta}(x))dx$, is the expected log-likelihood of the model under the true distribution. Thus, minimizing KL divergence is equal to maximize log-likelihood $p_{\theta}(x)$, where $x \sim p_{data}$.

---

## <a id="section1.1">1.1 From Maximum likelihood to ELBO</a>

Let $x_0$ be the original image, and $x_i (i=1,2,...,T)$ be the image with noise added to $x_0$. We wish to maximise 

$$
\log p_{\theta}(x_0)=\log \int p_{\theta}(x_{0:T}) dx_{1:T} \label{eq:2}
$$

Introduce the forward process $q(x_{1:T} \mid x_0)$ (a Markov chain with fixed noise‚Äëschedule). Using Jensen‚Äôs inequality gives the evidence lower bound:

$$
\begin{align}
\log p_\theta(x_0) \geq \mathcal{L}_\text{ELBO} & = \mathbb{E}_q \left[ \log p_\theta(x_0 \mid x_1) - \log \frac{q(x_{T} \mid x_0)}{p_\theta(x_{T})} - \sum_{t=2}^T \log \frac{q(x_{t-1} \mid x_t, x_0)}{p_\theta(x_{t-1} \mid x_t)} \right]\label{eq:3}
\end{align}
$$

The first term is reconstruction loss, the second term is prior matching, both of them are extremely small and can be ignored. Therefore, what we are truly concerned about is the third item, which also known as denoising term.

---

## <a id="section1.2">1.2 From KL collapses to a mean MSE</a>

For each denoising step, both forward posterior $q(x_{t-1} \mid x_t, x_0) \sim \mathcal{N}(\mu_{q}, \sigma_{q}^2I)$ and backward posterior
$p_{\theta}(x_{t-1} \mid x_t) \sim \mathcal{N}(\mu_{\theta}, \sigma_{\theta}^2I)$ are gaussian distributions. For two Gaussians with identical covariance, if we fix the two variances are equal to $\sigma_{q}^2$, then the KL divergence is equal to:

$$
\mathbb{KL}\left(q(x_{t-1} \mid x_t, x_0) \parallel p_{\theta}(x_{t-1} \mid x_t) \right) = \frac{1}{2\sigma_q^2} \|{\mu}_{q} - \mu_{\theta}(x_t, t)\|_2^2 + \text{const}\label{eq:4}
$$

Hence, for each denoising step, the loss function equals to

$$
\mathcal{L}_{\text{denoise}} =  \mathbb{E}_q \left[ \|\mu_q - \mu_{\theta}(x_t, t)\|^2 \right]\label{eq:5} 
$$

$\mu_q$ is the true target we want to predict, How do we calculate the value of 
$\mu_q$? Let's first decompose forward posterior $q(x_{t-1} \mid x_t, x_0)$ :

$$
q(x_{t-1} \mid x_t, x_0)=\frac{q(x_{t} \mid x_{t-1})q(x_{t-1} \mid x_{0})}{q(x_{t} \mid x_{0})} \propto q(x_{t} \mid x_{t-1})q(x_{t-1} \mid x_{0})\label{eq:6}
$$

where 

$$
q(x_{t} \mid x_{t-1}) \sim \mathcal{N}(x_{t-1};\mu_1, \sigma_1^2I),\ \ \mu_1=\frac{1}{\sqrt{\alpha_t}}x_{t},\ \ \sigma_1^2=\frac{1-\alpha_t}{\alpha_t} \\[10pt] q(x_{t-1} \mid x_{0})  \sim \mathcal{N}(x_{t-1};\mu_2, \sigma_2^2I), \ \ \mu_2=\sqrt{\bar \alpha_{t-1}}x_{0},\ \  \sigma_2^2=1-\bar \alpha_{t-1}\label{eq:7}
$$

The product of two Gaussian distributions is itself a Gaussian distribution, 




$$
\mu_{q} = \frac{\mu_1\sigma_2^2+\mu_2\sigma_1^2}{\sigma_1^2+\sigma_2^2}  = \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1}) x_t + \sqrt{\bar{\alpha}_{t-1}} (1 - \alpha_t) {x}_0}{1 - \bar{\alpha}_t}\label{eq:8}
$$

Combining equations \ref{eq:5} and \ref{eq:8}, Our goal is to construct a neural network $\mu_{\theta}$, which takes $x_t$ and $t$ as inputs, such that the output of the network is as close as possible to $\mu_q$.


--- 

# <a id="section2">2. Re-parameterising the mean with different target predictor</a>

Following equation \ref{eq:8}, we can dirrectly build a network $\mu_{\theta}$ to output $\mu_{q}$. However, in practice, we usually do not directly fit the value of $\mu_{q}$, mainly due to the following reasons.

- $\mu_{q}$ is an affine function of $x_t$, which is known at training and test time, there is no need for the network to ‚Äúreproduce‚Äù it. If we regress $\mu_{q}$ directly, the network wastes capacity relearning a large known term and must also learn the residual that actually depends on the unknown clean content. 

- The mean target value is highly time-dependent scaling across $t$, which means that the output of the network is unstable, it is usually extremely difficult for a network to output results with a large variance range.


Instead of asking the network to output $\mu_{q}$ directly, the community typically uses four common prediction targets to train diffusion models: $\epsilon$-prediction, $x_0$-prediction, $v$-prediction, score-prediction. If we regard the original image $x_0$ and noise $\epsilon$ as two orthogonal dimensions, then All the common targets are linear in $(x_0, \epsilon)$


## <a id="section2.1">2.1 $x_0$-prediction (aka sample-prediction in Diffusers)</a>

In $x_0$-prediction, the neural network is trained to directly estimate the clean original data $x_0$ from the noisy input $x_t$ at timestep $t$. Denoted the network as $ x_{\theta}(x_t, t)$, and the predicted output is $$\hat{x}_0$$, this approach reparameterizes the predicted mean $\mu_{\theta}$ using the estimated $$\hat{x}_0$$. Substituting into Equation \ref{eq:8}, the mean becomes:

$$
\mu_{\theta}(x_t, t) = \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1}) x_t + \sqrt{\bar{\alpha}_{t-1}} (1 - \alpha_t) \hat{x}_0}{1 - \bar{\alpha}_t}\label{eq:9}
$$

The loss function then simplifies to minimizing the MSE between the true $x_0$ and the predicted $\hat{x}_0$:

$$
\mathcal{L} = \mathbb{E}_{x_0, t, \epsilon} \left[ \| x_0 - x_{\theta}(x_t, t) \|^2 \right]\label{eq:10}
$$

**Pros**

This parameterization is advantageous because the target $x_0$ has a consistent scale across timesteps, reducing the variance in network outputs and improving training stability. 

**Cons**

The primary drawback of $x_0$-prediction lies in the uneven learning difficulty across signal-to-noise ratio (SNR) regimes, which induces heterogeneous gradient behaviors and ultimately hinders training convergence. 


## <a id="section2.2">2.2 $\epsilon$-prediction</a>

The $\epsilon$-prediction paradigm tasks the network with predicting the noise $\epsilon$ added during the forward process. This parameterization leverages the forward noising equation to express the clean data $x_0$ in terms of the noise $\epsilon$ via a simple linear transformation (equation \ref{eq:11}). 

$$
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon \  \Longrightarrow \  x_0=\frac{x_t-\sqrt{1 - \bar{\alpha}_t} \epsilon}{\sqrt{\bar{\alpha}_t}}\label{eq:11}
$$ 

Substituting this reparameterized form of $x_0$ (with $\hat{\epsilon}$ in place of $\epsilon$) into Equation \ref{eq:8} for the mean gives:

$$
\mu_{\theta}(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \hat{\epsilon} \right)\label{eq:12}
$$

The loss function then simplifies to minimizing the MSE between the true noise $\epsilon$ and the predicted $\hat{\epsilon}$:

$$
\mathcal{L} = \mathbb{E}_{x_0, t, \epsilon} \left[ \| \epsilon - \epsilon_{\theta}(x_t, t) \|^2 \right]\label{eq:13}
$$

**Pros**

This parameterization is advantageous because the target $\epsilon$ is timestep-independent target distribution since $\epsilon \sim \mathcal{N}(0, I)$.

**Cons**

Like $x_0$-prediction, $\epsilon$-prediction also suffers from the uneven learning difficulty problems, which needs re-weighting $ w(t) $ to promote balanced learning across the noise spectrum. We will conduct more in-depth analysis in the following sections.



## <a id="section2.3">2.3 $v$-prediction</a>

Velocity ($v$)-prediction combines elements of both $x_0$ and $\epsilon$ predictions by forecasting a velocity term $v$ that interpolates between them. Defined velocity as $$v = \sqrt{\bar{\alpha}_t} \epsilon - \sqrt{1 - \bar{\alpha}_t} x_0$$ (or its normalized variant), the network predicts $$\hat{v} = v_{\theta}(x_t, t)$$. The mean $\mu_{\theta}$ can be expressed in terms of $$\hat{v}$$:

$$
\mu_{\theta}(x_t, t) = \sqrt{\alpha_t}x_t- \frac{(1-\alpha_t)\sqrt{\bar \alpha_{t-1}}}{\sqrt{1-\bar \alpha_t}}{\hat v}\label{eq:14}
$$

The loss function then simplifies to minimizing the MSE between the true velocity $v$ and the predicted $$\hat{v}$$:

$$
\mathcal{L} = \mathbb{E}_{x_0, t, \epsilon} \left[ \| v - v_{\theta}(x_t, t) \|^2 \right]\label{eq:15}
$$

This hybrid approach adapts dynamically to timestep: in high SNR regimes, it behaves like $\epsilon$-prediction, while in low SNR regimes, it resembles $x_0$-prediction. This adaptability enhances stability by balancing gradient flows across SNR levels, reducing divergence risks, but requires precise calibration of the velocity formulation to avoid numerical instabilities during backpropagation.


## <a id="section2.4">2.4 Score-prediction</a>

This parameterization draws from the score-based generative modeling framework, where the neural network estimates the score function $s_{\theta}(x_t, t) = \nabla_{x_t} \log p_t(x_t)$, representing the gradient of the log-probability density at the noisy state $x_t$. In Gaussian diffusion models, the score is directly related to the noise via $$s(x_t, t) = -\frac{\epsilon}{\sqrt{1 - \bar \alpha_t}}$$, allowing a re-expression of the noise $\epsilon$ in terms of the score. Starting from the forward noising equation and substituting the equivalent form $$\epsilon = -\sqrt{1 - \bar{\alpha}_t} \, s(x_t, t)$$, the predicted mean $$\mu_{\theta}$$ is derived by inserting this into Equation \ref{eq:8}:

$$
\mu_{\theta}(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t + (1 - \alpha_t) \, s_{\theta}(x_t, t) \right)
$$

The loss function simplifies to minimizing the MSE between the true score $s(x_t, t)$ and the predicted $$\hat{s} = s_{\theta}(x_t, t)$$:

$$\mathcal{L} = \mathbb{E}_{x_0, t, \epsilon} \left[ \| s(x_t, t) - s_{\theta}(x_t, t) \|^2 \right]$$

**Pros**
Integrated in models like Score-Based Generative Models (SGMs) and aligned with continuous-time formulations, this approach facilitates theoretical connections to stochastic differential equations (SDEs) and enhances generalization across noise schedules. 

**Cons**
However, it may introduce scaling sensitivities in discrete timesteps, potentially leading to instabilities if the score magnitudes are not properly normalized, necessitating adaptive weighting or variance adjustments during training. In general, it has more theoretical value but is rarely used in practice.


---

# <a id="section3">3. A Unified Analysis: Why Targets Differ in Practice</a>

Although $x_0$, $\epsilon$, $v$, and score prediction are mathematically equivalent (all linear reparameterizations of the $(x_0,\epsilon)$ basis), their optimization behaviors differ dramatically. To explain this, we analyze them through **three quantitative lenses**:

- Correlation with $x_t$: Measuring Task Difficulty

- Conditional Mean: Measuring Signal Strength

- Conditional Variance: Measuring gradient stability

---

## <a id="section3.1">3.1 Correlation: Measuring Task Difficulty</a>

Correlation measures how predictable a target \$y\_t\$ is from the noisy input \$x\_t\$ via a *linear* relationship:

$$
\rho_t = \frac{\mathrm{Cov}(y_t, x_t)}{\sqrt{\mathrm{Var}(y_t)\,\mathrm{Var}(x_t)}}\label{eq:18}
$$

- **High correlation:** the target is strongly aligned with \$x\_t\$, making the regression problem easier.
- **Low correlation:** the target is weakly coupled to \$x\_t\$, making training harder.

We now analyze this quantity under two scenarios.

### <a id="section3.1.1">3.1.1 Case A: When $x_0$ is Gaussian (simplified setting)</a>

Assume $x_0 \sim \mathcal N(0,I)$, $\epsilon \sim \mathcal N(0,I)$, independent. Then $(x_0,\epsilon,x_t)$ is jointly Gaussian, so correlations are determined entirely by covariances. With

$$
x_t = a_t x_0 + b_t \epsilon, 
\quad \text{SNR} = \frac{a_t^2}{b_t^2},
$$

and $\mathrm{Var}(x_t)=(a_t^2+b_t^2)I$, we obtain closed-form correlations:

- **$\epsilon$-prediction**: Since $\mathrm{Cov}(\epsilon,x_t)=\mathrm{Cov}(\epsilon,a_t x_0 + b_t \epsilon)=b_t$, Substitute into Eq. \ref{eq:18}

  $$
  \rho_{\epsilon, x_t} = \frac{\mathrm{Cov}(\epsilon,x_t)}{\sqrt{\mathrm{Var}(\epsilon)\,\mathrm{Var}(x_t)}} = \frac{b_t}{\sqrt{a_t^2+b_t^2}} = \frac{1}{\sqrt{1+\text{SNR}}}.
  $$
  
  **strong at low SNR, weak at high SNR**, easiest to learn in low-SNR regions but harder in high-SNR regions.

- **$x_0$-prediction**: Since $\mathrm{Cov}(x_0,x_t)=\mathrm{Cov}(x_0,a_t x_0 + b_t \epsilon)=a_t$, Substitute into Eq. \ref{eq:18}
  
  $$
  \rho_{x_0, x_t} = \frac{\mathrm{Cov}(x_0,x_t)}{\sqrt{\mathrm{Var}(x_0)\,\mathrm{Var}(x_t)}} = \frac{a_t}{\sqrt{a_t^2+b_t^2}} = \frac{\sqrt{SNR}}{\sqrt{1+\text{SNR}}}.
  $$
  
  **strong at high SNR, weak at low SNR**, easiest to learn in high-SNR regions but harder in low-SNR regions.
  
- **$v$-prediction**: Since $\mathrm{Cov}(v,x_t)=\mathrm{Cov}(a_t \epsilon - b_t x_0,a_t x_0 + b_t \epsilon)=0$, Substitute into Eq. \ref{eq:18}

  $$
  \rho_{v, x_t} = \frac{\mathrm{Cov}(v,x_t)}{\sqrt{\mathrm{Var}(v)\,\mathrm{Var}(x_t)}} = 0.
  $$
  
  orthogonal to $x_t$ for all SNR, **tends to maintain a more balanced correlation across SNR, indicating more uniform difficulty.**
  
- **score-prediction**: Since $\mathrm{Cov}(s,x_t)=\mathrm{Cov}(-\dfrac{1}{b_t}\epsilon,a_t x_0 + b_t \epsilon)=-1$, and $\sqrt{\mathrm{Var}(s)}=\dfrac{1}{b_t}$, Substitute into Eq. \ref{eq:18}

  $$
  \rho_{s, x_t} = \frac{\mathrm{Cov}(s,x_t)}{\sqrt{\mathrm{Var}(s)\,\mathrm{Var}(x_t)}} = \frac{-b_t}{\sqrt{a_t^2+b_t^2}} = -\frac{1}{\sqrt{1+\text{SNR}}}.
  $$
  
  same magnitude as $\epsilon$ but opposite sign.

The following figure shows Pearson correlation between four target values and $x_t$, with the horizontal axis corresponding to SNR and T respectively.

![Correlation vs SNR and t](/images/posts/post_2/correlation.png)

---

### <a id="section3.1.2">3.1.2 When $x_0$ is non-Gaussian (realistic case)</a>

For real data, $x_0$ is non-Gaussian. Then: 
- The joint distribution $(x_0, x_t)$ is no longer Gaussian.
- The correlation coefficient $\rho(y_t, x_t)$ cannot be reduced to a simple function of SNR.
- Exact values depend on higher-order moments of the data distribution.

**Nevertheless, qualitative trends remain unchanged** because the corruption channel is Gaussian:

- **$\epsilon$-prediction:** correlation stronger in **low-SNR regime** (since $x_t \approx b_t\epsilon$ when SNR is low), weaker in high-SNR (since $x_t \approx a_tx_0$ when SNR is high).

- **$x_0$-prediction:** correlation stronger in **high-SNR regime**, weaker in low-SNR (since $x_t \approx a_t x_0$ when SNR is high).

- **$v$-prediction:** correlation remains small, though not exactly zero; still lacks strong SNR bias.

- **Score-prediction:** correlation trend mirrors Œµ (stronger in low-SNR, weaker in high-SNR).


---

## <a id="section3.2">3.2 Conditional Mean: Measuring Signal Strength</a>

A fundamental principle of statistics states that to minimize the loss function, the optimal model $f_{\theta}^{\star}$ must predict the conditional expectation of the target y given the input $x_t$, i,e., 

$$
f_{\theta}^{\star} = \mathbb{E}(y \mid x_t)
$$

- **If the conditional mean is large**: the network receives strong, coherent gradients ‚Üí fast convergence, efficient learning.

- **If the conditional mean is small**: the signal is weak, updates become noise-dominated, leading to slow progress or gradient starvation unless reweighted.

We now analyze this quantity under two scenarios.


---

### <a id="section3.2.1">3.2.1 Case A: When $x_0$ is Gaussian (simplified setting)</a>

If we assume both $x_0\sim \mathcal N(0,I)$ and $\epsilon\sim\mathcal N(0,I)$ (independent), then the joint distribution $(x_0,\epsilon,x_t)$ is Gaussian. In this case, the conditional expectations are **linear in $x_t$** and admit closed-form formulas:

$$
\mathbb{E}(y_t \mid x_t) = \mathbb{E}(y_t) + \frac{\mathrm{Cov}(y_t, x_t)}{\mathrm{Var}(x_t)}\left( x_t - \mathbb{E}(x_t) \right)\label{eq:20}
$$


-  **$ \epsilon $-prediction:** Substitute $y_t=\epsilon$ into Eq. \ref{eq:20}

   $$
   \begin{align}
   \mathbb{E}[\epsilon \mid x_t] & = \mathbb{E}(\epsilon) + \frac{\mathrm{Cov}(\epsilon, x_t)}{\mathrm{Var}(x_t)}(x_t - \mathbb{E}(x_t)) = b_tx_t
   \end{align}
   $$
   
   Strong in low-SNR ($b_t$ is large) regime, weak in high-SNR ($b_t$ is small) regime. Learns heavily noised states well, but struggles when data is clean.

-  **$ x_0 $-prediction:** Substitute $y_t=x_0$ into Eq. \ref{eq:20}

   $$
   \begin{align}
   \mathbb{E}[\epsilon \mid x_t] & = \mathbb{E}(x_0) + \frac{\mathrm{Cov}(x_0, x_t)}{\mathrm{Var}(x_t)}(x_t - \mathbb{E}(x_t)) = a_tx_t
   \end{align}
   $$
   
   Strong in high-SNR ($a_t$ is large) regime, weak in low-SNR ($a_t$ is small) regime. Captures coarse structure early, but struggles with heavily noised states.

-  **$ v $-prediction:**
   With $ v = a_t \epsilon - b_t x_0 $, substituting the above conditional means yields:
   
   $$
   \begin{align}
   \mathbb{E}[v \mid x_t] & = a_t \, \mathbb{E}[\epsilon \mid x_t] - b_t \, \mathbb{E}[x_0 \mid x_t] \\[10pt] 
   & =  a_t b_t x_t - b_t a_t x_t = 0
   \end{align}
   $$
   
   More uniform but weaker signal (zero when $x_0$ follows Gaussian distribution). Stable but slower unless boosted.
   

-  **Score-prediction:** From the relationship between the score and $ \epsilon $
   
   $$ 
   \mathbb{E}[s \mid x_t] = -\frac{\mathbb{E}[\epsilon \mid x_t]}{b_t} = -x_t
   $$  
   
   
   Roughly uniform mean signal (\$-x\_t\$ under Gaussian proxy). Even distribution, but scale imbalance remains (see ¬ß5.3).


The following figure shows conditional mean between four target values, with the horizontal axis corresponding to SNR and T respectively.

![Correlation vs SNR and t](/images/posts/post_2/condmean.png)

---

### <a id="section3.2.2">3.2.2 Case B: When $x_0$ is non-Gaussian (realistic case)</a>

Similarly, for real datasets, $x_0$ is far from Gaussian, the neat linear forms from Case A **do not hold exactly**. Instead, we can only describe **qualitative trends** based on information-theoretic arguments (I‚ÄìMMSE identity) and intuitive limits:

- **Œµ-prediction:**

  - Low SNR: $x_t \approx b_t\epsilon$, so $\epsilon$ is recoverable ‚Üí large conditional mean.
  - High SNR: $x_t \approx a_t x_0$, noise is hidden ‚Üí $\epsilon$ is unidentifiable ‚Üí conditional mean near zero.

- **$x_0$-prediction:**

  - High SNR: $x_t$ almost equals $x_0$, so posterior concentrates ‚Üí large conditional mean.
  - Low SNR: $x_t$ mostly noise, posterior diffuse ‚Üí conditional mean close to prior mean (‚âà0 if whitened).

- **$v$-prediction:**
  By construction, $v=a_t\epsilon - b_t x_0$. In non-Gaussian settings its conditional mean need not vanish, but it remains relatively small and without strong SNR bias.

- **Score-prediction:**
  Training target is $s=-\epsilon/b_t$. Its conditional mean is
  $\mathbb E[s \mid x_t] = -\mathbb E[\epsilon\mid x_t]/b_t$.
  Qualitative behavior mirrors Œµ-prediction: stronger at low-SNR, weaker at high-SNR.
  
---

## <a id="section3.3">3.3 Conditional Variance: Measuring Gradient Stability</a>

The **Law of Total Variance** provides the key identity linking signal and noise:

$$
\mathrm{Var}(y_t) \;=\; \underbrace{\mathbb E[\mathrm{Var}(y_t \mid x_t)]}_{\text{irreducible noise / conditional variance}}
+ \underbrace{\mathrm{Var}(\mathbb E[y_t \mid x_t])}_{\text{explainable signal}}\label{eq:31}
$$

Here: $y_t$ is the training target ($\epsilon, x_0, v, \tilde s$), $x_t$ is the noisy input.

- $\mathbb E[y_t \mid x_t]$ is the **signal** (learnable part).
- $\mathrm{Var}(y_t \mid x_t)$ is the **irreducible noise** (SGD variance floor).

This decomposition is why **conditional variance** is a central diagnostic of gradient stability. Near the optimum, the per-sample MSE gradient variance is proportional to: $$\mathrm{Var}(y_t \mid x_t)$$

This represents the irreducible noise of prediction.

- **Large conditional variance:** noisy gradients ‚Üí requires smaller LR, larger batch/EMA, or down-weighting; otherwise unstable.
- **Small conditional variance:** clean gradients ‚Üí stable, efficient learning; but if paired with small conditional mean, learning still stagnates.

---

### <a id="section3.3.1">3.3.1 Case A: When $x_0$ is Gaussian (simplified setting)</a>

Assume $x_0 \sim \mathcal N(0,I)$, $\epsilon\sim \mathcal N(0,I)$, independent, with

$$
x_t = a_t x_0 + b_t \epsilon, \qquad \text{SNR} = \frac{a_t^2}{b_t^2}.
$$

Then all conditional distributions are Gaussian, so exact formulas are available:

- **$\epsilon$-prediction:** Since $\mathrm{Var}(\mathbb E[\epsilon \mid x_t])=\mathrm{Var}(b_tx_t)=b_t^2\,I$, substitute into Eq. \ref{eq:31}

  $$
  \begin{align}
  \mathrm{Var}(\epsilon \mid x_t) & = \mathrm{Var}(\epsilon) - \mathrm{Var}(\mathbb E[\epsilon \mid x_t]) \\[10pt]
  & = I - \frac{b_t^2}{a_t^2+b_t^2}\,I = \frac{a_t^2}{a_t^2+b_t^2}\,I = \frac{\text{SNR}}{1+\text{SNR}}\,I
  \end{align}
  $$
  
  small variance in low-SNR regime, large variance in high-SNR regime. The range varies between 0 and 1.

- **$x_0$-prediction:** Since $\mathrm{Var}(\mathbb E[x_0 \mid x_t])=\mathrm{Var}(a_tx_t)=a_t^2\,I$, substitute into Eq. \ref{eq:31}

  $$
  \begin{align}
  \mathrm{Var}(x_0 \mid x_t) & = \mathrm{Var}(x_0) - \mathrm{Var}(\mathbb E[x_0 \mid x_t]) \\[10pt]
  & = I - \frac{a_t^2}{a_t^2+b_t^2}\,I = \frac{b_t^2}{a_t^2+b_t^2}\,I = \frac{1}{1+\text{SNR}}\,I
  \end{align}
  $$
  
  large variance in low-SNR regime, small variance in high-SNR regime. The range varies between 0 and 1.

- **$v$-prediction:** Since $\mathrm{Var}(\mathbb E[v\mid x_t])=0$, substitute into Eq. \ref{eq:31}

  $$
  \mathrm{Var}(v \mid x_t) = \mathrm{Var}(v) - \mathrm{Var}(\mathbb E[v \mid x_t]) = I, \quad \text{}.
  $$
  
  independent of SNR, the variance is stable in all SNR regime.

- **Score-prediction:** Since $s = -\epsilon/b_t$,  substitute into Eq. \ref{eq:31}

  $$
  \mathrm{Var}(s \mid x_t) = \frac{1}{b_t^2}\,\mathrm{Var}(\epsilon \mid x_t) = \text{SNR}\cdot I.
  $$
  
  Variance is directly proportional to SNR. In the high-SNR region, the variance is extremely large.


The following figure shows conditional mean between four target values, with the horizontal axis corresponding to SNR and T respectively.

![Correlation vs SNR and t](/images/posts/post_2/condvar.png)

---

### <a id="section3.3.2">3.3.2 Case B: When $x_0$ is non-Gaussian (realistic case)</a>

For real-world data, $x_0$ is highly non-Gaussian (e.g., images). However, the **qualitative SNR trends** remain the same, since the corruption channel is Gaussian.


- **$\epsilon$-prediction:** low-SNR ‚áí small variance (stable); high-SNR ‚áí large variance (unstable).
- **$x_0$-prediction:** high-SNR ‚áí small variance (stable); low-SNR ‚áí large variance (unstable).
- **$v$-prediction:** variance not exactly constant, but relatively flat; no strong SNR bias.
- **Score-prediction:** inherits Œµ‚Äôs trend but amplified by $1/b_t$; variance grows with SNR ‚áí high-SNR instability.


---

## <a id="section3.4">3.4 Summary and Comparative Table</a>

We had notice that no mather the data prior $p(x_0)$ is Gaussian or not, the qualitative trends is consistent with each other. That's because the channel is Gaussian ($x_t=a_tx_0+b_t\epsilon$ is a Gaussian-corrupted version of $x_0$), the **SNR regimes** have universal behavior, independent of the prior:

- **High-SNR ($a_t \gg b_t$)**: $x_t\approx a_t x_0$. The observation is dominated by the clean signal; the injected noise is nearly invisible.

- **Low-SNR ($b_t \gg a_t$)**: $x_t\approx b_t \epsilon$. The observation is dominated by noise.


These **qualitative trends** for the three metrics (correlation, conditional mean, conditional variance) therefore **do not depend** on assuming a Gaussian data prior, only the **closed-form expressions** require a Gaussian prior.



| Target               | Correlation (difficulty)         | Conditional Mean (signal) | Conditional Variance (stability)     | Net Effect                      |
| -------------------- | -------------------------------- | ------------------------- | ------------------------------------ | ------------------------------- |
| \$\epsilon\$         | High at low-SNR, low at high-SNR | Strong in low-SNR         | Stable in low-SNR, noisy in high-SNR | **Biased toward low-SNR**       |
| \$x\_0\$             | High at high-SNR, low at low-SNR | Strong in high-SNR        | Stable in high-SNR, noisy in low-SNR | **Biased toward high-SNR**      |
| \$v\$                | Zero (uniform)                   | Weak but uniform          | Constant (stable)                    | **Balanced but weaker**         |
| Score  | Mirrors \$\epsilon\$             | Roughly uniform           | Noisy in high-SNR, stable in low-SNR | **Unstable in high-SNR regime** |


---

# <a id="section4">4. SOTA Solutions: Balancing SNR in Diffusion Training</a>

The analysis in Section [3](#section3) makes it clear that a naive MSE loss, regardless of the chosen target, is suboptimal. The inherent bias of $\epsilon$ and $x_0$-prediction towards opposite ends of the SNR spectrum, and the weak signal of $v$-prediction, demand more sophisticated training strategies. This has led the community to develop several powerful techniques that are now standard in state-of-the-art models like Stable Diffusion, Imagen, and the EDM series. These solutions aim to normalize the learning task across all timesteps, ensuring balanced and stable convergence.

## <a id="section4.1">4.1 Strategic Loss Weighting</a>

The most direct way to counteract imbalanced learning is to apply a weighting term $w(t)$ to the loss function, making the objective:

$$
\mathcal{L}_{w} =  \mathbb{E}_{t, x_t} \left[ w(t)\|y_t - f_{\theta}(x_t, t)\|^2 \right] 
$$

Where **$w(t)=1$** in our previous discussion (vanilla diffusion training). The function $w(t)$ acts as a precision-engineered "equalizer" for our training process. It amplifies the learning signal where it's naturally weak and  suppress the loss when the gradient variance is large to prevent instability. The goal is to design $w(t)$ such that the effective learning task is normalized across all timesteps (or all SNR regions), we summarizes several mainstream weighting strategies as follows:


- **Perception Prioritized (P2) Weighting**: This innovative approach, introduced by Choi et al. [^p2], shifts the optimization goal from minimizing raw MSE to minimizing human-perceived error, often measured by metrics like LPIPS, where

  $$
  w(t) = \frac{1}{(\kappa + \text{SNR}(t))^\gamma}
  $$
  
  The paper recommends hyperparameters $\kappa=1$ and $\gamma=0.5$. This strategy is born from the observation that MSE is a poor proxy for perceptual quality. The authors found that most of the perceptual error occurs in the mid-to-high SNR range, even when the MSE is low. The P2 weighting scheme is therefore counter-intuitively designed to **down-weight the high-SNR regime** ($w(t) \to 0$ as $\text{SNR}(t) \to \infty$) and **up-weight the mid-to-low SNR regime** ($w(t) \approx 1$ as $\text{SNR}(t) \to 0$).
  
  The logic is to force the model to first learn the semantically crucial, large-scale structures of the image (governed by mid-to-low SNR steps) correctly. A well-formed global structure provides a better foundation for generating perceptually pleasing details, even if the MSE in the high-SNR details is not minimized as aggressively.


- **SNR-Based weighting**: This is a simple, elegant, and effective strategy that provides a balanced solution by weighting the loss directly by the SNR: $$ w(t) = \text{SNR}(t) $$ [^snr_based], Or, more generally, it can be formalized

  $$ 
  w(t) = (\text{SNR}(t))^{\gamma},\quad \gamma > 0 
  $$


  This approach cleverly addresses both primary difficulties simultaneously:
  
  1.  **At High SNR**: The weight is large, amplifying the weak signal of $\epsilon$-prediction and forcing the model to learn details.
  2.  **At Low SNR**: The weight is small, suppressing the large and noisy gradients common to $x_0$-prediction, thus promoting stability.
  
  It provides a balanced trade-off, ensuring that neither end of the SNR spectrum dominates the training process. However, due to the potentially large variation range of SNR, using logarithmic scaling ($w(t)=\log(\text{SNR}(t))$) can make the weight distribution across different SNR regions more balanced.
  
- **Min-SNR Weighting**: min-snr [^min_snr] sets an upper bound to suppress the high-SNR region.

  $$
  w(t) = \min(\text{SNR}(t), \tau)
  $$
  
  In both the official implementation and the Diffusers library, $\tau$ is set to 5 by default.
  1. **When $SNR(t) < \tau$ (Low-to-Mid SNR regime)**: The weight is $w(t) = \text{SNR}(t)$. The behavior is identical to standard SNR weighting.
  
  2. **When $SNR(t) \geq \tau$ (High SNR regime)**: The weight is "capped" at a constant upper bound, $w(t) = \tau$. It no longer increases as the SNR grows. 
  
  A massive weight in high-SNR area can cause the model to focus excessively on minuscule errors in high-frequency details. This can result in perceptually unpleasant artifacts, such as "over-sharpened" or "fried" textures, which harm the naturalness of the generated image. Min-SNR strategy acts as a "limiter" on the signal amplifier. It ensures that the model learns details sufficiently without the detrimental side effects of extreme weights.
  
  This balancing prioritizes **better perceptual quality and training stability** by preventing the model from becoming pathologically focused on details.

- **Max-SNR Weighting**: max-snr [^max_snr] sets an lower bound to ensure a minimum learning signal.
  
  $$
  w(t) = \max(\text{SNR}(t), \tau)
  $$
  
  1. **When $SNR(t) > \tau$ (Mid-to-High SNR regime)**: The weight is $w(t) = \text{SNR}(t)$. In this range, the behavior is identical to standard SNR weighting, leveraging a high weight to amplify the weak signal in the high-SNR region.
  
  2. **When $SNR(t) \leq \tau$ (Very Low SNR regime)**: The weight is "frozen" at a constant lower bound, $w(t) = \tau$. It no longer approaches zero as the SNR decreases. 
  
  
  When the weight is approach to 0 (in low-snr area), the model might learn that it receives no penalty for errors in these initial steps and therefore fails to learn how to meaningfully begin the denoising process. This can hinder the start of the generation or lead to artifacts in the final sample. Max-SNR strategy enforces a minimum level of supervision even in the noisiest regimes, and compels the model to learn a meaningful "first step" out of the noise distribution.
	
  
  Overall, This balancing prevents a "**blind spot**" in the learning process, ensuring the integrity and effectiveness of the entire denoising chain from pure noise to clean image.

- **SNR clipping**: This strategy is a robust combination of the previous two.
  
  $$
  w(t) = \min(\max(\text{SNR}(t), \tau_{min}), \tau_{max})
  $$
  
  1. **When $\text{SNR}(t) < \tau_{min}$ (Very Low SNR)**: $w(t) = \tau_{min}$ (the floor is active).
  2. **When $\tau_{min} \leq \text{SNR}(t) \leq \tau_{max}$ (Core SNR range)**: $w(t) = \text{SNR}(t)$ (standard behavior).
  3. **When $\text{SNR}(t) > \tau_{max}$ (Very High SNR)**: $w(t) = \tau_{max}$ (the ceiling is active).

  This strategy **boosts** the weights in the lowest SNR region and **suppresses** them in the highest SNR region, effectively creating a trapezoidal weighting curve. Theoretically speaking, it simultaneously optimize the learning signal, gradient stability, and perceptual quality. It is one of the most robust heuristic weighting schemes in practice.

---

## <a id="section4.2">4.2 EDM-Style Preconditioning</a>

In early diffusion models such as DDPM or DDIM, both training and sampling were defined in terms of a **discrete timestep $t \in \{1,\dots,T\}$**. A noise schedule $\{\beta_t\}$ was chosen, then transformed into $\alpha_t$ and $\bar{\alpha}_t$, and the network was trained with $t$ as its input.

The limitation of this viewpoint is that the **relationship between SNR and timestep $t$** depends entirely on the chosen schedule (linear, cosine, exponential, etc.). As a result, the effective learning task is **schedule-dependent**: the same $t$ can correspond to vastly different SNR levels and hence very different learning difficulties.

---

### <a id="section4.2.1">4.2.1 From $t$-space to $\sigma$-space</a>

EDM introduces a critical shift: **instead of modeling in the discrete timestep space, all training objectives and parameterizations are expressed directly in terms of the continuous noise scale $\sigma$.**

The corrupted sample is written as ncsn stryle with signal is constant

  $$
  x_t = x_0 + \sigma_t \,\epsilon, \quad \epsilon \sim \mathcal N(0,I).
  $$
  
Here $\sigma$ is the standard deviation of the added Gaussian noise. It is directly linked to the signal-to-noise ratio (SNR):

  $$
  \text{SNR} = \frac{1}{\sigma^2}.
  $$
  
> Important: All subsequent design choices‚Äîpreconditioning coefficients, loss weights, training distributions‚Äîare defined as functions of $\sigma$ rather than $t$.



1. **Decoupling objectives from schedules**: regardless of the noise schedule, the network always learns the same well-defined target.

2. **Simpler formulas**: all scaling coefficients are explicit analytic functions of $\sigma$, no longer entangled with $\alpha_t$ or $\bar{\alpha}_t$.

3. **Continuity and consistency**: since $\sigma$ is continuous, this formulation naturally aligns with SDE/ODE-based perspectives, unifying training and sampling under the same variable.

---


### 4.2.2 Preconditioning Formulation

The previous section on Strategic Loss Weighting aimed to correct the training objective, balancing the model's focus across different SNR regimes. However, the root of instability lies not just in the objective function but is also deeply embedded in how the network architecture handles inputs and outputs of dramatically varying scales.

From our "$\sigma$-space perspective", the instability of network training becomes exceptionally clear: as $\sigma$ varies from very small to very large values, the variance of the network input $x_t$, given by ${\text Var}(x_t) = {\text Var}(x_0) + \sigma_t^2$, and the scales of various potential prediction targets (like $x_0$ or $\epsilon$) change dramatically, often across several orders of magnitude.



For example, consider that across the diffusion timesteps $t$, the numerical magnitude of the network's input $x_t$ and its target $y_t$ (be it $x_0$, $\epsilon$, $v$, or score) can vary by several orders of magnitude. For instance:
- At high SNR (small $t$), $x_t \approx a_t x_0$, so its variance is close to that of the original data.
- At low SNR (large $t$), $x_t \approx b_t \epsilon$, and its variance approaches $b_t^2$.
- Similarly, the variance of the score ($-\epsilon/b_t$) is small at low SNR but explodes at high SNR.

Demanding a single, fixed neural network $f_Œ∏$ to effectively process inputs with a magnitude of $0.1$ in one forward pass and $100$ in another‚Äîwhile its output target undergoes similar wild fluctuations‚Äîposes a tremendous challenge for the optimizer and the network weights. This can lead to exploding or vanishing gradients at different timesteps, severely undermining training stability.

To remove scale inconsistencies across different noise levels, Karras et al. [^edm] introduced a pivotal technique: **Network Pre-conditioning**. The core idea is elegant: instead of forcing the raw network (U-Net) to adapt to these drastic scale changes, we wrap it with simple, analytically-defined scaling functions. These functions normalize the network's inputs and outputs, ensuring the core network `f_Œ∏` always operates in a **well-behaved regime** or "comfort zone"‚Äîfor instance, where its inputs and outputs have roughly unit variance. EDM expresses the pre-conditioned network (denoiser) as:

$$
F_{\theta}(x_t, t) = c_{\text{out}}(t) \cdot f_{\theta}(c_{\text{in}}(t) \cdot x_t, c_{\text{noise}}(t)) + c_{\text{skip}}(t) \cdot x_t
$$


Here, $f_Œ∏$ is our core U-Net architecture, and the $c_...$ terms are simple scalar functions that depend only on the noise level $\sigma$. Let's precisely break down this formula in $\sigma$-space:

- **$c_in(\sigma)\cdot x(\sigma)` (Input Scaling)**: This term aims to counteract the scale variation of the input $x(œÉ)$. To give $f_{\theta}$'s input a constant variance, $c_in(œÉ)$ must cancel out the variance of $x(œÉ)$. Letting the clean data variance be $\sigma_{\text data}^2 = Var(x_0)$, we have $Var(x(œÉ)) = \sigma_{\text data}^2 + \sigma^2$. EDM therefore chooses:
  
  $$
  c_in(œÉ) = \frac{1}{\sqrt{\sigma^2 + \sigma_{\text data}^2}}
  $$
  
  which ensures that the U-Net sees inputs with similar statistical properties regardless of $\sigma$.


-   **`c_noise(t)` (Noise/Timestep Scaling)**: It's not just the input data that needs scaling. The timestep information provided to the network (often an embedding of `log(SNR)` or `log(œÉ_t)`) should also be standardized to ensure consistent conditioning.

-   **`c_out(t) ¬∑ f_Œ∏(...)` (Output Scaling)**: `c_out(t)` performs the inverse operation. The core network `f_Œ∏`, operating in its stable internal environment, produces an output with approximately unit variance. `c_out(t)` is responsible for scaling this output back to the correct magnitude of our desired prediction target `y_t`. For example, if the target is the score, whose scale varies dramatically with `t`, `c_out(t)` will adjust the output of `f_Œ∏` accordingly.

-   **`c_skip(t) ¬∑ x_t` (Skip Connection Scaling)**: This is a brilliant design, drawing insight from modern architectures like ResNet. We know that many prediction targets (like `x_0` or `Œµ`) can be expressed as a linear combination of `x_t` and another unknown term (e.g., `x_0 = (x_t - b_t Œµ) / a_t`). This skip connection provides the `x_t` component of the solution "for free," allowing the core network `f_Œ∏` to focus exclusively on learning the more difficult **residual part** (i.e., estimating the `Œµ` or `x_0`-related term from `x_t`). This dramatically simplifies the network's learning task.

---


## <a id="section4.3">4.3 Noise Schedule Design</a>
---

# <a id="section5">Amplification of Network Prediction Errors Over Timesteps</a>



---

# <a id="section5">Conclusion</a>

---

# <a id="section6">References</a>

[^iedm]: Karras T, Aittala M, Lehtinen J, et al. Analyzing and improving the training dynamics of diffusion models[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 24174-24184.

[^edm]: Karras T, Aittala M, Aila T, et al. Elucidating the design space of diffusion-based generative models[J]. Advances in neural information processing systems, 2022, 35: 26565-26577.

[^Kingma]: Kingma D, Gao R. Understanding diffusion objectives as the elbo with simple data augmentation[J]. Advances in Neural Information Processing Systems, 2023, 36: 65484-65516.

[^Salimans]: Salimans T, Ho J. Progressive distillation for fast sampling of diffusion models[J]. arXiv preprint arXiv:2202.00512, 2022.

[^p2]: Choi J, Lee J, Shin C, et al. Perception prioritized training of diffusion models[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022: 11472-11481.

[^min_snr]: Hang T, Gu S, Li C, et al. Efficient diffusion training via min-snr weighting strategy[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2023: 7441-7451.

[^max_snr]: Salimans T, Ho J. Progressive distillation for fast sampling of diffusion models[J]. arXiv preprint arXiv:2202.00512, 2022.

[^snr_based]: Kingma D, Gao R. Understanding diffusion objectives as the elbo with simple data augmentation[J]. Advances in Neural Information Processing Systems, 2023, 36: 65484-65516.
