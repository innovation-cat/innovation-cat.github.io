---
title: 'Pseudo Numerical Solvers For Fast sampling'
date: 2025-03-08
permalink: /posts/2025/03/diffusion-model-3/
tags:
  - ODE
  - Diffusion Model
  - Numerical Computation
---


# Numerical ODE Solver

Numerical methods convert a continuous‐time initial-value problem into a sequence of discrete algebraic updates that march the solution forward in small time steps.

Numerical ODE solvers work by discretizing the continuous time domain into a sequence of time points: $t_0, t_1, ..., t_{n-1}, t_n$, the interval between any two adjacent time steps is $h$, i,e,. $t_i=t_{i-1}+h$. Given an initial-value problem:

$$
\frac{dx}{dt}=f(t,x),\ \ \ \ x(t_0)=x_0
$$

the Fundamental Theorem of Calculus rewrites the update over one step $h$:

$$
x_{t_{i+1}}=x_{t_i}+h \int_{t_i}^{t_{i+1}}f(t,x)dt
$$

Because the exact integrand $f(t,x)$ is unknown (it involves the unknown path $x$), numerical schemes replace that integral with a tractable quadrature formula  built from sample slopes. The essential difference between different numerical methods lies in the different strategies they use to approximate this integral. 

## One-step vs Multi-step 

This research dimension answers the question: **"How much historical information is needed to computer $x_{t_{n+1}}$?"**

### 1. One-step Methods

A one-step method uses only the information from the single previous point $(t_n, x_{t_n})$ to compute $x_{t_{n+1}}$, it does not care about earlier points like $x_{t_{n-1}}$, $x_{t_{n-2}}$, etc. That is to say, The information for $x_{t_{n+1}}$ is determined entirely by $x_{t_n}$ and some estimates of the slope within the interval $[t_n, t_{n+1}]$, we formalize as general form $x_{t_{n+1}}=\Phi(t_n, x_{t_n}, h)$   

Some classic numerical one-step methods are listed as follows:

|    Methods     | Order   |  NFE | Sampling Points| Update (explicit form) |
| :--------:  | :-----:  | :----:  | :--------:  | :-----:  | :----:  |
| Euler | 1 |1| $t_n $ | $$x_{t_{n+1}} = x_{t_n} + h*f(t_n, x_{t_n})$$ |
|  Heun (RK2)  | 2 |2| $$t_n \\ t_n+h$$ |  $$k_1=f(t_n, x_{t_n}) \\ k_2=f(t_n+{h}, x_{t_n}+{h}*k_1)  \\  x_{t_{n+1}}=x_{t_n}+\frac{h}{2}*(k_1+k_2)$$ |
| RK3 | 3 |3 | $$t_n \\ t_n+\frac{h}{2} \\ t_n+h$$ | $$k_1=f(t_n, x_{t_n}) \\ k_2=f(t_n+\frac{h}{2}, x_{t_n}+\frac{h}{2}k_1) \\  k_3=f(t_n+h, x_{t_n}-hk_1+2hk_2)  \\  x_{t_{n+1}}=x_{t_n}+\frac{h}{6}(k_1+4k_2+k_3)$$ |
| RK4|4 |4|  $$t_n \\ (t_n+\frac{h}{2})(2\times) \\ t_n+h$$ | $$k_1=f(t_n, x_{t_n}) \\ k_2=f(t_n+\frac{h}{2}, x_{t_n}+\frac{h}{2}k_1) \\ k_3=f(t_n+\frac{h}{2}, x_{t_n}+\frac{h}{2}k_2) \\  k_4=f(t_n+h, x_{t_n}+hk_3)  \\  x_{t_{n+1}}=x_{t_n}+\frac{h}{6}(k_1+2k_2+2k_3+k_4)$$ |

Geometrically, The integral on the right equals the signed area enclosed by the curve $f(t,x)$, the 
$t$-axis, and the vertical lines $t=t_i$ and $t=t_{i+1}$. Higher order numerical methods guarantee a better asymptotic error bound when all other factors (step size, stability, constants, arithmetic) are favourable.

![Area Approximations of 8 Common Explicit ODE Methods](/images/posts/post_3/1.png)

However, in real problems those factors often dominate, so a lower-order method can outperform a higher-order one.

### 2. Multi-step Methods

A multi-step method uses not only the information from the single previous point $(t_n, x_{t_n})$, but also from previous points, such as $(t_{n-1}, x_{t_{n-1}}), (t_{n-2}, x_{t_{n-2}})$, etc. we formalize as general form $x_{t_{n+1}}=\Phi(h, f_{n}, f_{n-1}, f_{n-2},...)$ , where $f_i=f(t_i, x_(t_{i}))$. 

Traditional multi-step methods including Adams family (Adams-Bashforth, Adams-Moulton), Backward Differentiation Formulas (BDF).

## Implicit vs Explicit

This research dimension aims to answer the question: "Is the formula for $x_{t_{n+1}}$ is a direct calculation or an equation to be solved?", that is to say, whether $x_{t_{n+1}}$ appears on both sides of the equation simultaneously.

### Explicit

Explicit methods refers to the formula for $x_{t_{n+1}}$ is an explicit expression, where the unknown  $x_{t_{n+1}}$ appears only on the left-hand side. We can directly plug in known values on the right-hand side to compute $x_{t_{n+1}}$.

Typical examples including forward euler method, RK families, Adams-Bashforth families.


### Implicit

Implicit methods refers to the formula for  $x_{t_{n+1}}$ is an equation, where the unknown $x_{t_{n+1}}$  appears on both sides of the equals sign. Typical examples including backward euler method, Adams-Moulton families.

--- 


# Local Truncation Error (LTE) and Global Truncation Error (GTE)

In numerical methods for solving ordinary differential equations (ODEs), errors arise because we approximate the continuous solution with discrete steps. Two key concepts are the Local Truncation Error (LTE) and the Global Truncation Error (GTE), which are the common indicators used to measure errors.

-  **Local Truncation Error (LTE):** This is the error introduced in a single step of the numerical method, assuming the solution at the previous step is exact. It measures how well the method approximates the true solution over one step, based on the Taylor series expansion of the exact solution. 

  Mathematically, if the exact solution at $t_n$ is $x(t_n)$, the LTE at step $n+1$ is:

  $$
  \tau_{n+1} = x(t_{n+1}) - x_{n+1}^{\text{approx}}
  $$
 
  where $x_{n+1}^{\text{approx}}$ is computed using the method with exact input $x(t_n)$. 

- **Global Truncation Error (GTE):** This is the total accumulated error at the end of the integration interval (e.g., from $t_0$ to $T$), considering that errors from previous steps propagate forward. It depends on the number of steps $N = (T - t_0)/h$, and for stable methods, GTE is typically $O(h^p)$ if LTE is $O(h^{p+1})$. The relationship is roughly GTE $\approx$ (number of steps) $\times$ LTE, but propagation can amplify or dampen it.

Below, we will use the Euler method and the Henu method as examples to demonstrate how to obtain its LTE and GTE.


---

## Example with Euler Method
The forward Euler method is a first-order method:

$$x_{n+1} = x_n + h f(t_n, x_n)$$

### Deriving LTE
- Assume exact input: Start with $x_n = x(t_n)$.

- The method approximates: $x_{n+1}^{\text{approx}} = x(t_n) + h f(t_n, x(t_n))$.

- From Taylor: $x(t_n + h) = x(t_n) + hf(t_n, x(t_n)) + \frac{h^2}{2} x''(t_n) + O(h^3)$.

- Subtract: LTE $\tau_{n+1} = x(t_n + h) - x_{n+1}^{\text{approx}} = \frac{h^2}{2} x''(t_n) + O(h^3)$.

- Thus, **LTE = $O(h^2)$**. (The leading term is quadratic in $h$.)

This shows how to arrive: The Euler update matches the first two Taylor terms (constant + linear), so the error starts from the quadratic term.

### Deriving GTE

- Let $e_n = x(t_n) - x_n$ be the global error at step $n$.

- The error recurrence: $e_{n+1} = e_n + h [f(t_n, x(t_n)) - f(t_n, x_n)] + \tau_{n+1}$.

- For Lipschitz-continuous $f$ (with constant $L$), $\|f(x(t_n)) - f(x_n)\| \leq L \|e_n\|$, so:
$$\|e_{n+1}\| \leq \|e_n\| (1 + h L) + \|\tau_{n+1}\|$$

- Since $\tau_{n+1} = O(h^2)$, over $N$ steps ($N \approx 1/h$), the accumulated error bounds to $|e_N| \leq C h$ for some constant $C$ (from summing the geometric series of propagated local errors).

- Thus, GTE = $O(h)$. (Errors accumulate linearly with $1/h$ steps, reducing the order by 1.)

Cause: Each local error adds up, and propagation (via $1 + hL$) amplifies like compound interest, but stability ensures it's bounded by $O(h)$.

## Example with Heun Method

The Heun method (a second-order Runge-Kutta) improves on Euler with a predictor-corrector:

$$k_1 = f(t_n, x_n), \quad k_2 = f(t_n + h, x_n + h k_1), \quad x_{n+1} = x_n + \frac{h}{2} (k_1 + k_2)$$

### Deriving LTE

- Assume exact input: $x_n = x(t_n)$.

- Expand $k_1 = f(t_n, x(t_n)) = x'(t_n)$.

- Predictor: $x_n + h k_1 = x(t_n) + h x'(t_n)$.

- $k_2 = f(t_n + h, x(t_n) + h x'(t_n))$. Taylor-expand $f(t_n + h, x(t_n) + h x'(t_n))$ in two variables:

 $$
 f(t_n, x(t_n)) + h \left(\frac{\partial f}{\partial t} + x'(t_n) \frac{\partial f}{\partial x} \right) + \frac{h^2}{2} \left( \frac{\partial^2 f}{\partial t^2} + 2 x' \frac{\partial^2 f}{\partial t \partial x} + (x')^2 \frac{\partial^2 f}{\partial x^2} \right) + O(h^3)
 $$ 
 But since $x'' = \frac{\partial f}{\partial t} + x' \frac{\partial f}{\partial x}$, $k_2 = x' + h x'' + \frac{h^2}{2} x''' + O(h^3)$.

- Then, 
 $$
 \begin{align}
 x_{n+1}^{\text{approx}} & = x(t_n) + \frac{h}{2} (x' + x' + h x'' + \frac{h^2}{2} x''' + O(h^3)) \\ & = x(t_n) + h x' + \frac{h^2}{2} x'' + \frac{h^3}{4} x''' + O(h^4)
 \end{align}
 $$

- True: $x(t_n + h) = x(t_n) + h x' + \frac{h^2}{2} x'' + \frac{h^3}{6} x''' + O(h^4)$.

- Subtract: LTE $\tau_{n+1} = \left( \frac{h^3}{6} - \frac{h^3}{4} \right) x''' + O(h^4) = -\frac{h^3}{12} x''' + O(h^4)$.
Thus, LTE = $O(h^3)$. (Matches up to quadratic term, error from cubic.)

### Deriving GTE

- Similar recurrence: 
$$e_{n+1} = e_n + \frac{h}{2} [ (f(t_n, y(t_n)) - f(t_n, y_n)) + (f(t_{n+1}, y(t_{n+1})) - f(t_{n+1}, y_n + h k_1)) ] + \tau_{n+1}$$.

- Using Lipschitz $L$, the perturbation terms are $O(e_n)$ and $O(h e_n + e_n)$, leading to $\|e_{n+1}\| \leq (1 + h L + O(h^2)) \|e_n\| + \|\tau_{n+1}\|$.

- With $\tau_{n+1} = O(h^3)$, over $N \approx 1/h$ steps, the bound sums to $\|e_N\| \leq C h^2$ (accumulated as $N \times O(h^3) = O(h^2)$).

- Thus, GTE = $O(h^2)$. (Higher local order leads to better global convergence.)

Cause: Fewer accumulations needed for the same accuracy, and the method's stability (A-stable for some cases) prevents excessive propagation.

In summary, LTE is per-step (higher order means smaller), while GTE is overall (reduced by 1 order due to accumulation). For Euler (order 1), halving $h$ halves GTE; for Heun (order 2), it quarters GTE—making higher-order methods more efficient for accuracy.


# What is the different between Numerical ODE and Diffusion Sampling 





# DDIM

# PNDM

# DEIS

# DPM-Solvers

# DPM-Solver++

# UniPC 

# K-Diffusion


------