---
title: 'A Panoramic View of Diffusion Model Sampling: From Classic Theory to Frontier Research'
date: 2025-03-08
excerpt: "This article takes a deep dive into the evolution of diffusion model sampling techniques, tracing the progression from early score-based models with Langevin Dynamics, through discrete and non-Markov diffusion processes, to continuous-time SDE/ODE formulations, specialized numerical solvers, and cutting-edge methods such as consistency models, distillation, and flow matching.  Our goal is to provide both a historical perspective and a unified theoretical framework to help readers understand not only *how* these methods work but *why* they were developed."
permalink: /posts/2025/03/diffusion-model-3/
tags:
  - ODE
  - Diffusion Model
  - Numerical Computation
  - Sampling
  - Langevin Dynamics
---

<details style="background:#f6f8fa; border:1px solid #e5e7eb; border-radius:10px; padding:.6rem .9rem; margin:1rem 0;">
  <summary style="margin:-.6rem -.9rem .4rem; padding:.6rem .9rem; border-bottom:1px solid #e5e7eb; cursor:pointer; font-weight:600;">
    <span style="font-size:1.25em;"><strong>📚 Table of Contents</strong></span>
  </summary>
  <ul>
	<li><a href="#section1">Introduction</a>
		<ul>
		  <li><a href="#section1.1">The Art of Generation: Sampling's Core Role in Diffusion Models</a></li>
		  <li><a href="#section1.2">The Eternal Trade-off: Why Sampling Methods Evolve So Rapidly</a></li>
		  <li><a href="#section1.3">A Guide to This Article: A Journey Through Time and Theory</a></li>
		</ul>
	</li>
	<li><a href="#section2">A Tale of Two Paradigms: The Dual Origins of Diffusion Sampling</a>
		<ul>
		  <li><a href="#section2.1">The Continuous-State Perspective: Score-Based Generative Models</a>
				<ul>
				  <li><a href="#section2.1.1">The Core Idea: The Wisdom of Score Matching</a></li>
				  <li><a href="#section2.1.2">Early Implementation: NCSN and Annealed Langevin Dynamics</a></li>
				  <li><a href="#section2.1.3">Historical Limitations and Enduring Legacy</a></li>
				</ul>
		  </li>
		  <li><a href="#section2.2">The Discrete-Time Perspective: Denoising Diffusion Probabilistic Models (DDPM)</a>
				<ul>
				  <li><a href="#section2.2.1">The Core Idea: An Elegant Markov Chain</a></li>
				  <li><a href="#section2.2.2">Forward Diffusion and Reverse Denoising</a></li>
				  <li><a href="#section2.2.3">Theoretical Elegance and Practical Bottleneck</a></li>
				</ul>
		  </li>
		  <li><a href="#section2.3">The Initial Convergence: Tying the Two Worlds Together</a></li>
		</ul>
	</li>
	<li><a href="#section1">Langevin Dynamics Sampling</a>
		<ul>
		  <li><a href="#section1.1">From score matching to a sampler</a></li>
		  <li><a href="#section1.2">Denoising Score Matching</a></li>
		  <li><a href="#section1.3">Annealed Langevin Dynamics</a></li>
		</ul>
	</li>
	<li><a href="#section2">Markov Chain-Based Sampling in DDPM</a></li>
	<li><a href="#section3">Non-Markovian Step-Skipping in DDIM</a>
		<ul>
		  <li><a href="#section3.1">Differences from DDPM</a></li>
		  <li><a href="#section3.2">Non-Markovian Forward Process</a></li>
		  <li><a href="#section3.3">Deterministic Sampling and Reverse Process</a></li>
		</ul>
	</li>
	<li><a href="#section4">PF-ODE Based Samplers: Pseudo Numerical Methods, DEIS, DPM-Solver, and Beyond</a>
		<ul>
			<li><a href="#section4.1">Numerical ODE Solver</a>
				<ul>
				  <li><a href="#section4.1.1">One-step vs Multi-step</a></li>
				  <li><a href="#section4.1.2">Implicit vs Explicit</a></li>
				</ul>
			</li>
			<li><a href="#section4.2">Local Truncation Error and Global Truncation Error</a>
				<ul>
				  <li><a href="#section4.2.1">Example with Euler Method</a></li>
				  <li><a href="#section4.2.2">Example with Henu Method</a></li>
				</ul>
			</li>
			<li><a href="#section4.3">Numerical ODE Solvers for Sampling</a></li>
			<li><a href="#section4.4">Fast Sampling for Diffusion Model</a>
				<ul>
				  <li><a href="#section4.4.1">DDIM</a></li>
				  <li><a href="#section4.4.2">PNDM</a></li>
				  <li><a href="#section4.4.3">DEIS</a></li>
				  <li><a href="#section4.4.4">DPM-Solver</a></li>
				  <li><a href="#section4.4.5">DPM-Solver++</a></li>
				  <li><a href="#section4.4.6">UniPC</a></li>
				</ul>
			</li>
		</ul>
	</li>
	<li><a href="#section5">Consistency Models</a></li>
	<li><a href="#section6">References</a></li>
  </ul>
</details>

This article systematically reviews the development history of sampling techniques in diffusion models. Starting from the two parallel technical routes of score-based models and DDPM, we explain how they achieve theoretical unification through the SDE/ODE framework. On this basis, we delve into various efficient samplers designed for solving the probability flow ODE (PF-ODE), analyzing the evolutionary motivations from the limitations of classical numerical methods to dedicated solvers such as DPM-Solver. Subsequently, the article shifts its perspective to innovations in the sampling paradigm itself, covering cutting-edge technologies such as consistency models and sampling distillation aimed at achieving single-step/few-step generation. Finally, we combine practical strategies such as hybrid sampling and guidance, conduct a comprehensive comparison of existing methods, and look forward to future research directions such as learnable samplers and hardware-aware optimizations.

# <a id="section1">1. Introduction</a>
In the landscape of modern artificial intelligence, diffusion models have emerged as the undisputed titans of generative tasks. From the breathtaking imagery of Stable Diffusion and Midjourney to the burgeoning field of AI-driven video and audio synthesis, these models have redefined the boundaries of digital creation. At the heart of their power lies a deceptively simple two-act structure: a training phase, where the model learns to systematically denoise corrupted data, and a generation phase, where this learned ability is inverted to create new data from pure noise.

## <a id="section1.1">1.1. The Art of Generation: Sampling's Core Role in Diffusion Models</a>

This article is dedicated to the second act: the process of sampling. If the trained model represents a repository of abstract knowledge about the data distribution—the "what"—then the sampling algorithm is the procedural engine that determines the "how." It is the chisel that methodically carves a masterpiece from a block of statistical marble. The sampler iteratively refines a random noise tensor, navigating a high-dimensional trajectory through a learned probability landscape, until it converges upon a coherent, high-fidelity artifact.

Therefore, sampling is not merely a passive, final step. It is an active, algorithmic process that profoundly influences the final output's quality, diversity, and, most critically, the efficiency with which it is produced. Understanding sampling is understanding the very mechanism of creation in diffusion models.


## <a id="section1.2">1.2. The Eternal Trade-off: Why Sampling Methods Evolve So Rapidly</a>

The history of diffusion model sampling is a story of a relentless quest for an ideal balance within a challenging trilemma: Speed vs. Quality vs. Diversity.

- **Speed (Computational Cost)**: The pioneering DDPM (Denoising Diffusion Probabilistic Models) demonstrated remarkable generation quality but required thousands of sequential function evaluations (NFEs) to produce a single sample. This computational burden was a significant barrier to practical, real-time applications and iterative creative workflows. Consequently, the primary driver for a vast body of research has been the aggressive reduction of these sampling steps.

- **Quality (Fidelity)**: A faster sampler is useless if it compromises the model's generative prowess. The goal is to reduce steps while preserving, or even enhancing, the fidelity of the output. Many methods grapple with issues like error accumulation, which can lead to blurry or artifact-laden results, especially at very low step counts. High-quality sampling means faithfully following the path dictated by the learned model.

- **Diversity & Stability**: Sampling can be either stochastic (introducing randomness at each step) or deterministic (following a fixed path for a given initial noise). Stochastic samplers can generate a wider variety of outputs from the same starting point, while deterministic ones offer reproducibility. The choice between them is application-dependent, and the stability of the numerical methods used, especially for high-order solvers, is a critical area of research.

This perpetual negotiation between speed, quality, and diversity has fueled a Cambrian explosion of innovative sampling algorithms, each attempting to push the Pareto frontier of what is possible.

## <a id="section1.3">1.3. A Guide to This Article: A Journey Through Time and Theory</a>

To navigate this complex and rapidly evolving landscape, this article adopts a structured, historical narrative. We will trace the evolution of sampling methodologies from their foundational concepts to the state-of-the-art, ensuring each new development is understood as a logical answer to the limitations of its predecessors. Our journey will proceed as follows:

1. We begin by exploring the dual origins of diffusion-based generation: the continuous-time, score-based models (NCSN) with Langevin dynamics, and the discrete-time, denoising-focused models (DDPM).
2. Next, we witness the first leap in efficiency with non-Markovian samplers like DDIM, which broke free from the step-by-step constraint of DDPM.
3. We then arrive at a pivotal moment of theoretical unification, where the elegant framework of Stochastic Differential Equations (SDEs) and their corresponding Probability Flow ODEs (PF-ODEs) reveals that all these earlier methods are merely different discretizations of the same underlying continuous process.
4. Armed with this unified ODE perspective, we will delve into solver engineering: the craft of designing specialized, high-order numerical solvers like DPM-Solver and UniPC that are tailor-made for the unique structure of diffusion ODEs.
5. We then elevate our perspective to framework design with EDM, understanding how optimizing the very definition of the diffusion process itself can lead to superior sampling performance.
6. Finally, we will survey the paradigm-shifting innovations aiming for the ultimate goal of single-step or few-step generation, including Consistency Models, Knowledge Distillation, and Flow Matching.

By charting this course, this article aims to provide a comprehensive and deeply interconnected understanding of diffusion model sampling, equipping the reader not only with knowledge of individual techniques but also with a robust mental model of the field's past, present, and future.

# <a id="section2">2. A Tale of Two Paradigms: The Dual Origins of Diffusion Sampling</a>

Before the unified theories of today, the world of diffusion-based generation was dominated by two distinct, yet parallel, schools of thought. One approached the problem from the continuous perspective of probability density gradients, while the other built a discrete, step-by-step bridge between data and noise. Understanding these two origins—Score-Based Models and Denoising Diffusion Probabilistic Models (DDPMs)—is essential to appreciating the elegant synthesis that followed.

## <a id="section2.1">2.1 The Continuous-State Perspective: Score-Based Generative Models</a>

The first paradigm was rooted in a fundamental question: if we had a function that could always point us toward regions of higher data probability, could we generate data by simply "climbing the probability hill"?

### <a id="section2.1.1">2.1.1 The Core Idea: The Wisdom of Score Matching</a>

At the heart of this approach lies a powerful mathematical object: the **score function**, defined as the gradient of the log-probability density of the data, $\nabla_x \log p_{\text data}(x)$. Intuitively, the score at any point $x$ is a vector that points in the direction of the steepest ascent in data density. To calculate the score function of any input, we train a neural network $s_{\theta}(x)$ (score model) to learn the score

$$
s_{\theta}(x) \approx \nabla_x \log p_{\text data}(x)\label{eq:1}
$$

by minimizing the Fisher divergence between the true data distribution and the model predicted output:

$$
\mathcal{L}(\theta) = \mathbb{E}_{x \sim p_{\text{data}}}\left[
\big\|\,s_{\theta}(x) - \nabla_x \log p_{\text{data}}(x)\,\big\|_2^2
\right]\label{eq:2}
$$


The challenge, however, is that we do not know the true data distribution $p_{\text data}(x)$. This is where **Score Matching** comes in [^Aapo]. Hyvärinen showed that via **integration by parts** (under suitable boundary conditions), the objective (equation \ref{eq:1}) can be rewritten in a form **only involving** the model’s parameters:

$$
\begin{align}
\mathcal{L}_{\text{SM}}(\theta) & = \mathbb{E}_{p_{\text{data}}} \left[ \frac{1}{2} \| s_\theta(x) \|^2 + \nabla_x \cdot s_\theta(x) \right] \\[10pt]
& \approx \frac{1}{N}\sum_{i=1}^{N} \left[ \frac{1}{2} \| s_\theta(x_i) \|^2 + \nabla_x \cdot s_\theta(x_i) \right] 
\end{align}
$$

where $\nabla_x \cdot s_\theta(x) = {\text{trace}(\nabla_x s_\theta(x))}$ is the divergence of the score field. However, SM is not scalable especially for high-dimension data points, because the second term is the jacobin of score model. 

For this purpose, Vincent introduced denoising score matching [^Vincent] , by adding Gaussian noise to the real data $x$, the score model becomes predicting the score field of noised data $\tilde{x} = x + \sigma$.

$$
s_{\theta}(\tilde{x}, \sigma) \approx \nabla_\tilde{x} \log p_{\sigma}(\tilde{x})\label{eq:1}
$$

where $p_{\sigma}$ is the data distribution convolved with Gaussian noise of scale $\sigma$, it is easy to verify that predicting $\nabla_{\tilde x} \log p_{\sigma}(\tilde x)$ is equivalent to predicting $\nabla_{\tilde{x}} \log p_{\sigma}(\tilde{x} \mid x)$.

$$
\mathbb{E}_{p_{\text{data}}} \left[ \frac{1}{2} \| s_\theta(x) \|^2 + \nabla_x \cdot s_\theta(x) \right] \\[10pt]
& \approx \frac{1}{N}\sum_{i=1}^{N} \left[ \frac{1}{2} \| s_\theta(x_i) \|^2 + \nabla_x \cdot s_\theta(x_i) \right] 
$$























### <a id="section2.1.2">2.1.2 Early Implementation: NCSN and Annealed Langevin Dynamics</a>

The first major practical realization of this idea was the **Noise-Conditional Score Network (NCSN)**. The authors recognized that real-world data often lies on a complex, low-dimensional manifold. In the vast empty spaces between data points, the score is ill-defined and difficult to estimate. Their solution was ingenious: perturb the data with Gaussian noise of varying magnitudes $\sigma$. This has two benefits:

1. It "fills in" the empty regions, making the score function well-behaved everywhere.

2. It allows the model to learn a single, robust score network $s_{\theta}(x, \sigma)$ that is conditional on the noise level.

To generate a sample, NCSN employed **Annealed Langevin Dynamics**. This is an iterative sampling process:

1. Start with a sample drawn from pure noise, corresponding to a very high noise level $\sigma_{\text large}$.

2. Iteratively update the sample using the Langevin dynamics equation:
   
   $$
   x_{i+1} \leftarrow x_i + \alpha  s_{\theta}(x_i, \sigma_i) + \sqrt{2\alpha}z_i
   $$
   
   where $\alpha$ is a step size and $z_i$ is fresh Gaussian noise. This update consists of a "climb" along the score-gradient and a small injection of noise to encourage exploration.
   
3. Gradually decrease (or "anneal") the noise level $\sigma_i$ from large to small. This process is analogous to starting with a blurry, high-level structure and progressively refining it with finer details until a clean sample emerges.

#### 2.1.3 Historical Limitations and Enduring Legacy

The NCSN approach was groundbreaking, but it had its limitations. The Langevin sampling process was inherently **stochastic** due to the injected noise $z$ and **slow**, requiring many small steps to ensure stability and quality. The annealing schedule itself was often heuristic.

However, its legacy is profound. NCSN established the **score function** as a fundamental quantity for generative modeling and introduced the critical technique of **conditioning on noise levels**. It provided the continuous-space intuition that would become indispensable for later theoretical breakthroughs.

### 2.2 The Discrete-Time Perspective: Denoising Diffusion Probabilistic Models (DDPM)

Running parallel to the score-based work, a second paradigm emerged, built upon the more structured and mathematically elegant framework of Markov chains.

#### 2.2.1 The Core Idea: An Elegant Markov Chain

**Denoising Diffusion Probabilistic Models (DDPMs)** proposed a fixed, two-part process.

1.  **Forward (Diffusion) Process:** Start with a clean data sample $x_0$. Gradually add a small amount of Gaussian noise at each discrete timestep $t$ over a large number of steps $T$ (typically 1000). This defines a Markov chain $x_0, x_1, \dots, x_T$ where $x_T$ is almost indistinguishable from pure Gaussian noise. This forward process, $q(x_t \mid x_{t-1})$, is fixed and requires no learning.

2.  **Reverse (Denoising) Process:** The goal of generation is to learn the reverse of this process: given a noisy sample $x_t$, how do we predict the slightly less noisy $x_{t-1}$? DDPM showed that if the noise added at each step is small enough, this reverse transition $p_{\theta}(x_{t-1} \mid x_t)$ can also be approximated as a Gaussian.

#### 2.2.2 Forward Diffusion and Reverse Denoising

The genius of the DDPM formulation lies in how it frames the learning problem. Instead of directly predicting the mean and variance of the reverse-step Gaussian, the model is trained on a much simpler proxy task: **predicting the noise $\epsilon$ that was added to $x_0$ to obtain $x_t$**.

A neural network, $\epsilon_{\theta}(x_t, t)$, is trained with a surprisingly simple objective: minimize the mean squared error between the true noise and the predicted noise. This "denoising" objective proved to be remarkably stable to train and capable of producing exceptionally high-quality samples.

Sampling is the direct inverse of the forward process. One starts with pure noise $x_T \sim \mathcal{N}(0, I)$  and iteratively applies the learned reverse step for $ t = T, T-1, \dots, 1$, using the noise prediction $\epsilon_{\theta}(x_t, t)$ at each step to denoise the sample until a clean $x_0$ is obtained.

#### 2.2.3 Theoretical Elegance and Practical Bottleneck

DDPMs offered a strong theoretical foundation, stable training, and state-of-the-art sample quality. However, this came at a steep price: **the curse of a thousand steps**. The model's theoretical underpinnings relied on the Markovian assumption and small step sizes, forcing the sampling process to be painstakingly slow and computationally expensive.

### 2.3 The Initial Convergence: Tying the Two Worlds Together

For a time, these two paradigms—one estimating a continuous gradient field, the other reversing a discrete noise schedule—seemed distinct. Yet, a profound and simple connection lay just beneath the surface. It was shown that the two seemingly different objectives were, in fact, two sides of the same coin.

The score function $s(x_t, t)$ at a given noise level is directly proportional to the optimal noise prediction $\epsilon(x_t, t)$:

$$
s_{\theta}(x_t, t) = \nabla_{x_t} \log p(x_t) = -\frac{\epsilon_{\theta}(x_t, t)} { \sigma_t}
$$

where $\sigma_t$ is the standard deviation of the noise at time $t$.

This equivalence is beautiful. **Predicting the noise is mathematically equivalent to estimating the score.** The score-based view provides the physical intuition of climbing a probability landscape, while the DDPM view provides a stable training objective and a concrete, discrete-time mechanism.

With this link established, the two parallel streams began to merge into a single, powerful river. This convergence set the stage for the next major leap: breaking free from the rigid, one-step-at-a-time sampling of DDPM, and ultimately, the development of a unified theory that could explain and improve upon both.

---

# <a id="section1">1. Langevin Dynamics Sampling</a>

**Langevin dynamics (LD)** is a classical result originating from statistical physics and stochastic differential equations [^Parisi] [^Grenander]. In the field of machine learning, this equation was first widely cited in the context of energy-based models (EBMs) and Markov chain Monte Carlo (MCMC) [^Aapo]. In modern generative models, Song et al. [^Song_2019] first combined Langevin dynamics with score matching for generative sampling from high-dimensional data distributions.

LD treats sampling as **stochastic gradient ascent on log-density** with Gaussian noise injected to preserve the target distribution.  In the diffusion family, LD first appeared as the core sampler in **Noise-Conditional Score Networks (NCSN)**, where a network learns the **score** $\nabla_{\tilde{x}} \log p_{\sigma}(\tilde{x})$ of noise-perturbed data $\tilde{x}$ at multiple noise scales and uses **Annealed Langevin Dynamics (ALD)** to progressively refine samples from high to low noise.

---

## <a id="section1.1">1.1. From score matching to a sampler</a>
 
Given a **true data distribution** with density $p_{\text data}(x) \propto \exp(−E(x))$, the continuous-time Langevin SDE is:

$$
\mathrm{d}x_t = \nabla_x \log p_{\text data}(x_t)\,\mathrm{d}t + \sqrt{2}\,\mathrm{d}W_t\label{eq:1}
$$

Euler–Maruyama discretization with stepsize $\eta > 0$ yields

$$
x_{k+1} = x_k + \eta\,\nabla_x \log p_{\text data}(x_k) + \sqrt{2\eta}\,z_k,\quad z_k \sim \mathcal{N}(0,I)\label{eq:2}
$$

Repeat this process for $T$ times, where $T \to \infty$, when $\nabla_x \log p_{\text data}(x_k)$ is **Lipschitz** and $\eta$ is small enough, Langevin dynamics sampling guarantees to converge to $p_{\text data}$, i,e., $x_{T} \sim p_{\text data}(x)$. To calculate the score function, we build a neural network $s_{\theta}(x)$ (score model) to learn the score. The core idea is to minimize the Fisher divergence between the true data distribution and the model predicted output:

$$
\mathcal{L}(\theta) = \mathbb{E}_{x \sim p_{\text{data}}}\left[
\big\|\,s_{\theta}(x) - \nabla_x \log p_{\text{data}}(x)\,\big\|_2^2
\right]\label{eq:3}
$$

Since $\nabla_x \log p_{\text{data}}(x)$ is unknown, Directly optimizing Formula \label{eq:3} is not feasible. Hyvärinen proposed **Score Matching (SM)** [^Aapo], which showed that via **integration by parts** (under suitable boundary conditions), the objective can be rewritten in a form **only involving** the model’s score function:

$$
\begin{align}
\mathcal{L}_{\text{SM}}(\theta) & = \mathbb{E}_{p_{\text{data}}} \left[ \frac{1}{2} \| s_\theta(x) \|^2 + \nabla_x \cdot s_\theta(x) \right] \\[10pt]
& \approx \frac{1}{N}\sum_{i=1}^{N} \left[ \frac{1}{2} \| s_\theta(x_i) \|^2 + \nabla_x \cdot s_\theta(x_i) \right] 
\end{align}
$$

where $\nabla_x \cdot s_\theta(x) = {\text{trace}(\nabla_x s_\theta(x))}$ is the divergence of the score field. However, SM is not scalable especially for high-dimension data points, because the second term is the jacobin of score model. 

## <a id="section1.2">1.2. Denoising Score Matching (DSM)</a>

NCSN avoids learning the exact score of $p_{\text data}(x)$. Instead it learns scores of **Gaussian-smoothed** marginals:

- Let $p_\sigma(x) = (p * \mathcal{N}(0,\sigma^2 I))(x)$.
- Train $s_\theta(x,\sigma) \approx \nabla_x \log p_\sigma(x)$ by minimizing

  $$
  \mathbb{E}_{\sigma\sim \pi} \;\mathbb{E}_{x_0\sim p}\;\mathbb{E}_{\varepsilon\sim\mathcal{N}(0,I)}
  \left[
  \lambda(\sigma)\,\big\|\,s_\theta(x_0+\sigma \epsilon,\,\sigma)\;+\;\tfrac{\epsilon}{\sigma^2}\big\|_2^2
  \right]\label{eq:6}
  $$
  
  where $x_0 \sim p_{\text data}(x)$, $\epsilon \sim \mathcal{N} (0, I)$.

Here the target score of the perturbed sample $x = x_0+\sigma \epsilon$ can be calculated, and equal to $-\frac{\epsilon}{\sigma^2}$. When $\sigma$ is small enough, $p_{\text data}(x) \approx p_\sigma(x)$. Optimizing Formula \ref{eq:6} does not require knowing the true data distribution, while also avoiding the computation of the complex Jacobian determinant.

---

## <a id="section1.3">1.3. Annealed Langevin Dynamics (ALD)</a>

However, In practice, the generated samples by solving \ref{eq:6} with a single small $ \sigma $ are of poor quality, the core issue is that training data primarily comes from high-density areas (real samples plus small noise), so low-density regions are underrepresented, leading to poor generalization by the network in those areas. The solution is to introduce multiple noise levels and Annealed Langevin Sampling [^Song_2019]:

- **Learning Phase:** Instead of a single $ \sigma $, use a sequence of noise levels $ \sigma_1 > \sigma_2 > \dots > \sigma_K > 0 $ (e.g., a geometric series from large to small). The network becomes noise-conditional, i.e., $ s_\theta(x, \sigma_i) \approx \nabla_x \log p_{\sigma_i}(x) $. For each $ \sigma_i $, train with perturbed samples at that noise level. This allows the network to learn scores at multiple scales:

  - **Large $ \sigma_i $**: The distribution is highly smoothed, covering the global space with small, stable scores in low-density areas (easier to learn).
  
  - **Small $ \sigma_i $**: Focuses on local refinement, closer to $ p_{\text{data}} $.
  
  Multi-scale training forces the network to model scores at varying "coarse-to-fine" levels, ensuring low-density regions (covered under large scales) are also captured.

- **Sampling Phase**: Rather than a single Langevin chain, use sequential annealing, 
  - Start with large $ \sigma_1 $: Initialize samples from a broad distribution (e.g., uniform or Gaussian), run a few Langevin iterations with $ s_\theta(x, \sigma_1) $ to generate coarse samples (good for global exploration).
  - Progressively decrease to $ \sigma_{i} $: Use the previous step's samples as initialization, and continue Langevin iterations with $ s_\theta(x, \sigma_{i}) $ for gradual refinement.
  
    $$
    x \leftarrow x + \alpha_i\, s_\theta(x, \sigma_i) + \sqrt{2\alpha_i}\,z,\quad
    \alpha_i = \epsilon \sigma_i^2,\; z\sim\mathcal{N}(0,I)
    $$
	
  - Continue until the smallest $ \sigma_K \approx 0 $, yielding high-quality samples close to $ p_{\text{data}} $.




---





# <a id="section2">2. Markov Chain-Based Sampling in DDPM</a>



As introduced in the [previous post](https://innovation-cat.github.io/posts/2025/01/diffusion-model-2/), DDPM [^ddpm] contains two independent processes: the forward and the reverse process. The reverse denoising process in diffusion models reconstructs the data distribution by iteratively reversing the forward noise addition. Each step in this reverse process is modeled as a Gaussian distribution, where the mean $\mu_\theta(x_t, t)$ and variance $\Sigma_\theta(x_t, t)$ are parameterized to approximate the true posterior transitions.

The reason $ p_\theta(x_{t-1} \mid x_t) $ exhibits Gaussian distribution characteristics is that it is designed to approximate the true posterior $ q(x_{t-1} \mid x_t, x_0) $, which itself is a Gaussian distribution derived from the forward process. Specifically, the forward diffusion steps involve Gaussian noise additions, making the conditional posteriors Gaussian. To elaborate: The true posterior $ q(x_{t-1} \mid x_t, x_0) $ can be computed using Bayes' rule and the Markov property of the forward chain:

$$
q(x_{t-1} \mid x_t, x_0) = \frac{q(x_t \mid x_{t-1}) q(x_{t-1} \mid x_0)}{q(x_t \mid x_0)}
$$

where all terms are Gaussian:

- $ q(x_t \mid x_{t-1}) = \mathcal{N}(x_t; \sqrt{\alpha_t} x_{t-1}, \beta_t I) $
- $ q(x_{t-1} \mid x_0) = \mathcal{N}(x_{t-1}; \sqrt{\bar{\alpha}_{t-1}} x_0, (1 - \bar{\alpha}_{t-1}) I) $
- $ q(x_t \mid x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I) $


Combining these, the posterior $ q(x_{t-1} \mid x_t, x_0) $ is also Gaussian:


$$
q(x_{t-1} \mid x_t, x_0) = \mathcal{N}\left( x_{t-1}; \tilde{\mu}_t(x_t, x_0), \tilde{\beta}_t I \right)
$$

with mean:

$$
\tilde{\mu}_t(x_t, x_0) = \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1 - \bar{\alpha}_t} x_0 + \frac{\sqrt{\alpha_t} (1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} x_t
$$

and variance:

$$
\tilde{\beta}_t = \frac{(1 - \bar{\alpha}_{t-1}) \beta_t}{1 - \bar{\alpha}_t}
$$

Since $ x_0 $ is unknown during sampling, the model can approximates this by predicting different targets ( $ x_0 $, noise, velocity or score), leading to the parameterized $ p_\theta(x_{t-1} \mid x_t) \approx q(x_{t-1} \mid x_t, x_0) $ (with $ x_0 $ replaced by a learned estimate).

Thus, the reverse transition $ p_\theta(x_{t-1} \mid x_t) $ is explicitly defined as:

$$
p_\theta(x_{t-1} \mid x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
$$

If we leverage $x_0$ as our learning target ($x_0$-prediction), substituting $ x_0 $ with the predicted $$\hat{x}_0=x_{0, \theta}(x_t, t)$$:

$$
\tilde{\mu}_t(x_t, x_0) = \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1 - \bar{\alpha}_t} x_{0, \theta}(x_t, t) + \frac{\sqrt{\alpha_t} (1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} x_t
$$

Or, If we leverage $\epsilon$ as our learning target ($\epsilon$-prediction), substituting $ x_0 $ with the noise predicted $\epsilon_{\theta}(x_t, t)$:

$$
\mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right)
$$


For the variance, DDPM uses a fixed schedule:

$$
\Sigma_\theta(x_t, t) = \sigma_t^2 I, \quad \sigma_t^2 = \tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \beta_t
$$

which directly matches the posterior variance $\tilde{\beta}_t$, interpolating between the forward variance $\beta_t$ and a value that aligns with the mode of the reverse posterior for better stability. Alternatives include setting $\sigma_t^2 = \beta_t$, but the mode-matched $\tilde{\beta}_t$ empirically yields higher sample quality by reducing variance mismatch in the early steps.

This Gaussian approximation ensures that the reverse chain closely approximates the true data-generating process, allowing the model to generate samples by starting from pure noise and progressively refining them through these Gaussian transitions. The integration of noise prediction ties directly into the training objective, as detailed next.

# <a id="section3">3. Non-Markovian Step-Skipping in DDIM</a>

Building on DDPM's foundation, Denoising Diffusion Implicit Models (DDIM) [^ddim], proposed by Song et al. in 2021, introduce non-Markovian processes to accelerate sampling by skipping steps, reducing iterations from thousands to tens while preserving quality. Unlike DDPM's stochastic Markov chains, DDIMs employ deterministic mappings, enabling implicit generative modeling where the forward process is non-Markovian, allowing flexible trajectories and latent interpolations. This shift not only boosts speed but also unlocks applications like semantic image editing, marking a pivotal evolution in diffusion sampling.


## <a id="section3.1">3.1. Differences from DDPM</a>
DDPM assumes a Markovian forward process where each step depends only on the previous, leading to stochastic reverse sampling. DDIM generalizes this by constructing non-Markovian diffusions that maintain the same marginals $q(x_t \mid x_0)$ but alter dependencies, making the reverse process implicit and deterministic when noise variance $\sigma_t = 0$. This results in consistent samples from the same initial noise across different step counts, unlike DDPM's variability, and allows encoding observations into latents with low reconstruction error.

## <a id="section3.2">3.2. Non-Markovian Forward Process</a>

The forward process is defined as:

$$
q_\sigma(x_{1:T} \mid x_0) := q_\sigma(x_T \mid x_0) \prod_{t=2}^T q_\sigma(x_{t-1} \mid x_t, x_0)
$$

with marginals matching DDPM:

$$
q_\sigma(x_t \mid x_0) = \mathcal{N}(\sqrt{\alpha_t} x_0, (1 - \alpha_t) I)
$$

The conditional transition is:

$$
q_\sigma(x_{t-1} \mid x_t, x_0) = \mathcal{N}\left( \sqrt{\alpha_{t-1}} x_0 + \sqrt{1 - \alpha_{t-1} - \sigma_t^2} \cdot \frac{x_t - \sqrt{\alpha_t} x_0}{\sqrt{1 - \alpha_t}}, \sigma_t^2 I \right)
$$

where $\sigma_t$ controls stochasticity (e.g., $\sigma_t = 0$ for determinism). This non-Markovian setup ensures the training objective remains identical to DDPM's noise prediction loss.

## <a id="section3.3">3.3. Deterministic Sampling and Reverse Process</a>

For deterministic sampling ($\sigma_t = 0$):

$$
x_{t-1} = \sqrt{\alpha_{t-1}} \left( \frac{x_t - \sqrt{1 - \alpha_t} \epsilon_\theta(x_t, t)}{\sqrt{\alpha_t}} \right) + \sqrt{1 - \alpha_{t-1}} \cdot \epsilon_\theta(x_t, t)
$$

The reverse transitions are:

$$
p_\theta(x_{t-1} \mid x_t) = q_\sigma(x_{t-1} \mid x_t, \hat{x}_0)
$$

where $\hat{x}_0 = \frac{x_t - \sqrt{1 - \alpha_t} \epsilon_\theta(x_t, t)}{\sqrt{\alpha_t}}$ is the predicted clean data.

DDIM's non-Markovian flexibility bridges probabilistic and deterministic generation, reducing NFEs while enabling novel capabilities, but still requires multi-step iterations, motivating ODE-based refinements.


# <a id="section4">4. PF-ODE Based Samplers: Pseudo Numerical Methods, DEIS, DPM-Solver, and Beyond</a>






Diffusion models involves a forward diffusion process that gradually adds Gaussian noise to data samples until they become pure noise, and a reverse process that learns to denoise this noise back to generate new samples. 

Mathematically, the forward process can be modeled as a stochastic differential equation (SDE): 

$$dx = f(x, t) \, dt + g(t) \, dw$$ 

where $x$ is the state variable (e.g., an image), $t$ is the time step, $f(x, t)$ is the drift term, $g(t)$ is the diffusion coefficient, and $dw$ represents Wiener process. The reverse process, used for sampling, is another SDE that approximates the time-reversal of the forward SDE, incorporating a score function $\nabla_x \log p_t(x)$ (the gradient of the log-probability density), which is estimated by a neural network trained via score matching.

A crucial insight is that this reverse SDE has an equivalent deterministic representation through the probability flow ordinary differential equation (PF ODE): 

$$\frac{dx}{dt} = f(x, t) - \frac{1}{2} g(t)^2 \nabla_x \log p_t(x)$$

This equivalence stems from the fact that the PF ODE is derived to match the Fokker-Planck equation (which describes the evolution of probability densities) of the original SDE, ensuring that trajectories generated by solving the ODE backward (from $t = T$ at pure noise to $t = 0$ at clean data) produce the same marginal probability distributions as the stochastic SDE paths, but without injecting additional randomness. Thus, sampling reduces to numerically integrating this ODE, making the process deterministic and potentially more efficient, as it avoids the variance introduced by stochastic sampling while preserving the generative quality.

In this post, we first review the numerical methods for solving ODEs. Then, we analyze why we do not directly use ODE numerical solvers for sampling in diffusion models. Finally, we explore how to construct an efficient sampler based on the properties of PF-ODE for sampling.


## <a id="section4.1">4.1. Numerical ODE Solver</a>

Numerical methods convert a continuous‐time initial-value problem into a sequence of discrete algebraic updates that march the solution forward in small time steps.

Numerical ODE solvers work by discretizing the continuous time domain into a sequence of time points: $t_0, t_1, ..., t_{n-1}, t_n$, the interval between any two adjacent time steps is $h$, i,e,. $t_i=t_{i-1}+h$. Given an initial-value problem:

$$
\frac{dx}{dt}=f(t,x),\ \ \ \ x(t_0)=x_0
$$

the Fundamental Theorem of Calculus rewrites the update over one step $h$:

$$
x_{t_{i+1}}=x_{t_i}+h \int_{t_i}^{t_{i+1}}f(t,x)dt
$$

Because the exact integrand $f(t,x)$ is unknown (it involves the unknown path $x$), numerical schemes replace that integral with a tractable quadrature formula  built from sample slopes. The essential difference between different numerical methods lies in the different strategies they use to approximate this integral. 

### <a id="section4.1.1">One-step vs Multi-step</a>

This research dimension answers the question: **"How much historical information is needed to computer $x_{t_{n+1}}$?"**

#### 1. One-step Methods

A one-step method uses only the information from the single previous point $(t_n, x_{t_n})$ to compute $x_{t_{n+1}}$, it does not care about earlier points like $x_{t_{n-1}}$, $x_{t_{n-2}}$, etc. That is to say, The information for $x_{t_{n+1}}$ is determined entirely by $x_{t_n}$ and some estimates of the slope within the interval $[t_n, t_{n+1}]$, we formalize as general form $x_{t_{n+1}}=\Phi(t_n, x_{t_n}, h)$   

Some classic numerical one-step methods are listed as follows:

|    Methods     | Order   |  NFE | Sampling Points| Update (explicit form) |
| :--------:  | :-----:  | :----:  | :--------:  | :-----:  | :----:  |
| Euler | 1 |1| $t_n $ | $$x_{t_{n+1}} = x_{t_n} + h*f(t_n, x_{t_n})$$ |
|  Heun (RK2)  | 2 |2| $$t_n \\ t_n+h$$ |  $$k_1=f(t_n, x_{t_n}) \\ k_2=f(t_n+{h}, x_{t_n}+{h}*k_1)  \\  x_{t_{n+1}}=x_{t_n}+\frac{h}{2}*(k_1+k_2)$$ |
| RK3 | 3 |3 | $$t_n \\ t_n+\frac{h}{2} \\ t_n+h$$ | $$k_1=f(t_n, x_{t_n}) \\ k_2=f(t_n+\frac{h}{2}, x_{t_n}+\frac{h}{2}k_1) \\  k_3=f(t_n+h, x_{t_n}-hk_1+2hk_2)  \\  x_{t_{n+1}}=x_{t_n}+\frac{h}{6}(k_1+4k_2+k_3)$$ |
| RK4|4 |4|  $$t_n \\ (t_n+\frac{h}{2})(2\times) \\ t_n+h$$ | $$k_1=f(t_n, x_{t_n}) \\ k_2=f(t_n+\frac{h}{2}, x_{t_n}+\frac{h}{2}k_1) \\ k_3=f(t_n+\frac{h}{2}, x_{t_n}+\frac{h}{2}k_2) \\  k_4=f(t_n+h, x_{t_n}+hk_3)  \\  x_{t_{n+1}}=x_{t_n}+\frac{h}{6}(k_1+2k_2+2k_3+k_4)$$ |

Geometrically, The integral on the right equals the signed area enclosed by the curve $f(t,x)$, the 
$t$-axis, and the vertical lines $t=t_i$ and $t=t_{i+1}$. Higher order numerical methods guarantee a better asymptotic error bound when all other factors (step size, stability, constants, arithmetic) are favourable.

![Area Approximations of 8 Common Explicit ODE Methods](/images/posts/post_3/1.png)

However, in real problems those factors often dominate, so a lower-order method can outperform a higher-order one.

#### 2. Multi-step Methods

A multi-step method uses not only the information from the single previous point $(t_n, x_{t_n})$, but also from previous points, such as $(t_{n-1}, x_{t_{n-1}}), (t_{n-2}, x_{t_{n-2}})$, etc. we formalize as general form $x_{t_{n+1}}=\Phi(h, f_{n}, f_{n-1}, f_{n-2},...)$ , where $f_i=f(t_i, x_(t_{i}))$. 

Traditional multi-step methods including Adams family (Adams-Bashforth, Adams-Moulton), Backward Differentiation Formulas (BDF).

### <a id="section4.1.2">Implicit vs Explicit</a>

This research dimension aims to answer the question: "Is the formula for $x_{t_{n+1}}$ is a direct calculation or an equation to be solved?", that is to say, whether $x_{t_{n+1}}$ appears on both sides of the equation simultaneously.

#### Explicit

Explicit methods refers to the formula for $x_{t_{n+1}}$ is an explicit expression, where the unknown  $x_{t_{n+1}}$ appears only on the left-hand side. We can directly plug in known values on the right-hand side to compute $x_{t_{n+1}}$.

Typical examples including forward euler method, RK families, Adams-Bashforth families.


#### Implicit

Implicit methods refers to the formula for  $x_{t_{n+1}}$ is an equation, where the unknown $x_{t_{n+1}}$  appears on both sides of the equals sign. Typical examples including backward euler method, Adams-Moulton families.

--- 


## <a id="section4.2">Local Truncation Error and Global Truncation Error</a>

In numerical methods for solving ordinary differential equations (ODEs), errors arise because we approximate the continuous solution with discrete steps. Two key concepts are the Local Truncation Error (LTE) and the Global Truncation Error (GTE), which are the common indicators used to measure errors.

-	**Local Truncation Error (LTE):** This is the error introduced in a single step of the numerical method, assuming the solution at the previous step is exact. It measures how well the method approximates the true solution over one step, based on the Taylor series expansion of the exact solution. 
	
	Mathematically, if the exact solution at $t_n$ is $x(t_n)$, the LTE at step $n+1$ is:
	
	$$
	\tau_{n+1} = x(t_{n+1}) - x_{n+1}^{\text{approx}}
	$$
	
	where $x_{n+1}^{\text{approx}}$ is computed using the method with exact input $x(t_n)$. 

- **Global Truncation Error (GTE):** This is the total accumulated error at the end of the integration interval (e.g., from $t_0$ to $T$), considering that errors from previous steps propagate forward. It depends on the number of steps $N = (T - t_0)/h$, and for stable methods, GTE is typically $O(h^p)$ if LTE is $O(h^{p+1})$. The relationship is roughly GTE $\approx$ (number of steps) $\times$ LTE, but propagation can amplify or dampen it.

Below, we will use the Euler method and the Henu method as examples to demonstrate how to obtain its LTE and GTE.


---

### <a id="section4.2.1">Example with Euler Method</a>
The forward Euler method is a first-order method:

$$x_{n+1} = x_n + h f(t_n, x_n)$$

#### Deriving LTE
- Assume exact input: Start with $x_n = x(t_n)$.

- The method approximates: $x_{n+1}^{\text{approx}} = x(t_n) + h f(t_n, x(t_n))$.

- From Taylor: $x(t_n + h) = x(t_n) + hf(t_n, x(t_n)) + \frac{h^2}{2} x''(t_n) + O(h^3)$.

- Subtract: LTE $\tau_{n+1} = x(t_n + h) - x_{n+1}^{\text{approx}} = \frac{h^2}{2} x''(t_n) + O(h^3)$.

- Thus, **LTE = $O(h^2)$**. (The leading term is quadratic in $h$.)

This shows how to arrive: The Euler update matches the first two Taylor terms (constant + linear), so the error starts from the quadratic term.

#### Deriving GTE

- Let $e_n = x(t_n) - x_n$ be the global error at step $n$.

- The error recurrence: $e_{n+1} = e_n + h [f(t_n, x(t_n)) - f(t_n, x_n)] + \tau_{n+1}$.

- For Lipschitz-continuous $f$ (with constant $L$), $\|f(x(t_n)) - f(x_n)\| \leq L \|e_n\|$, so:
$$\|e_{n+1}\| \leq \|e_n\| (1 + h L) + \|\tau_{n+1}\|$$

- Since $\tau_{n+1} = O(h^2)$, over $N$ steps ($N \approx 1/h$), the accumulated error bounds to $\|e_N\| \leq C h$ for some constant $C$ (from summing the geometric series of propagated local errors).

- Thus, GTE = $O(h)$. (Errors accumulate linearly with $1/h$ steps, reducing the order by 1.)

Cause: Each local error adds up, and propagation (via $1 + hL$) amplifies like compound interest, but stability ensures it's bounded by $O(h)$.

### <a id="section4.2.2">Example with Heun Method</a>

The Heun method (a second-order Runge-Kutta) improves on Euler with a predictor-corrector:

$$k_1 = f(t_n, x_n), \quad k_2 = f(t_n + h, x_n + h k_1), \quad x_{n+1} = x_n + \frac{h}{2} (k_1 + k_2)$$

#### Deriving LTE

- Assume exact input: $x_n = x(t_n)$.

- Expand $k_1 = f(t_n, x(t_n)) = x'(t_n)$.

- Predictor: $x_n + h k_1 = x(t_n) + h x'(t_n)$.

-  $k_2 = f(t_n + h, x(t_n) + h x'(t_n))$. Taylor-expand $f$ in two variables:

   $$
   \begin{align}
   f(t_n + h, x(t_n) + h x'(t_n)) = & f(t_n, x(t_n)) + h \left( f_t + x'(t_n) f_x \right) \\[10pt] & + \frac{h^2}{2} \left( f_{tt} + 2 x' f_{tx} + (x')^2 f_{xx} \right) + O(h^3)
   \end{align}
   $$ 
   
   But since $x'' = \frac{\partial f}{\partial t} + x' \frac{\partial f}{\partial x}$, $k_2 = x' + h x'' + \frac{h^2}{2} x''' + O(h^3)$.

- Then, 
 $$
 \begin{align}
 x_{n+1}^{\text{approx}} & = x(t_n) + \frac{h}{2} (x' + x' + h x'' + \frac{h^2}{2} x''' + O(h^3)) \\[10pt] & = x(t_n) + h x' + \frac{h^2}{2} x'' + \frac{h^3}{4} x''' + O(h^4)
 \end{align}
 $$

- True: $x(t_n + h) = x(t_n) + h x' + \frac{h^2}{2} x'' + \frac{h^3}{6} x''' + O(h^4)$.

- Subtract: LTE $\tau_{n+1} = \left( \frac{h^3}{6} - \frac{h^3}{4} \right) x''' + O(h^4) = -\frac{h^3}{12} x''' + O(h^4)$.
Thus, LTE = $O(h^3)$. (Matches up to quadratic term, error from cubic.)

#### Deriving GTE

-  Similar recurrence: 

   $$
   \begin{align}
   e_{n+1} =\  & e_n + \frac{h}{2} [ (f(t_n, y(t_n)) - f(t_n, y_n))] \\[10pt] &+ \frac{h}{2}[(f(t_{n+1}, y(t_{n+1})) - f(t_{n+1}, y_n + h k_1)) ] + \tau_{n+1}
   \end{align}
   $$

- Using Lipschitz $L$, the perturbation terms are $O(e_n)$ and $O(h e_n + e_n)$, leading to $\|e_{n+1}\| \leq (1 + h L + O(h^2)) \|e_n\| + \|\tau_{n+1}\|$.

- With $\tau_{n+1} = O(h^3)$, over $N \approx 1/h$ steps, the bound sums to $\|e_N\| \leq C h^2$ (accumulated as $N \times O(h^3) = O(h^2)$).

- Thus, GTE = $O(h^2)$. (Higher local order leads to better global convergence.)

Cause: Fewer accumulations needed for the same accuracy, and the method's stability (A-stable for some cases) prevents excessive propagation.

In summary, LTE is per-step (higher order means smaller), while GTE is overall (reduced by 1 order due to accumulation). For Euler (order 1), halving $h$ halves GTE; for Heun (order 2), it quarters GTE—making higher-order methods more efficient for accuracy.


## <a id="section4.3">Numerical ODE Solvers for Sampling</a>

Although the PF ODE formulation enables deterministic sampling, directly applying standard numerical ODE solvers (e.g., explicit Runge-Kutta methods like RK4, Euler methods, or adaptive solvers like Dormand-Prince) is often suboptimal or problematic for diffusion models. These general-purpose solvers do not fully exploit the specific structure of diffusion ODEs, leading to inefficiencies and quality issues, particularly in high-dimensional spaces like image generation. Below, I list the key reasons, drawn from analyses of diffusion sampling challenges:

- Numerical Instability and Stiffness Issues: Diffusion PF ODEs are semi-linear and can be stiff, especially in high dimensions, causing standard explicit solvers to become unstable with large step sizes. This results in exploding gradients or divergence, requiring tiny steps that increase computational cost and negate speed advantages.

- High Discretization Errors in Few-Step Regimes: With fewer integration steps (e.g., 10–20 instead of 1000), traditional solvers accumulate significant truncation errors, causing the approximated trajectory to deviate from the true ODE path. This leads to degraded sample quality, such as artifacts or lower fidelity, as the solver fails to accurately track the probability flow.

- Mismatch with Model Training Objectives: Diffusion models are trained to optimize score-matching losses, not to minimize ODE integration errors. Pursuing better ODE solving can paradoxically worsen perceptual quality, as seen in consistency models where tighter approximations to the PF ODE reduce sample fidelity due to inconsistencies between training and inference.

- Inefficiency for Guided or Conditional Sampling: Standard solvers do not inherently handle constraints like classifier guidance or conditional generation efficiently, often requiring additional modifications that increase function evaluations (NFEs) or fail to maintain distribution matching.

- Lack of Exploitation of Semi-Linear Structure: Diffusion ODEs have a specific semi-linear form (linear drift plus nonlinear score term), which general solvers ignore, leading to suboptimal performance. Without tailored approximations, they require more NFEs for convergence, making them slower than specialized methods.

These problems motivate the development of specialized solvers that incorporate higher-order approximations, exploit the ODE's structure, and provide convergence guarantees for fast, high-quality sampling in low-NFE settings.



## <a id="section4.4">Fast Sampling for Diffusion Models</a>

### <a id="section4.4.1">DDIM</a>

### <a id="section4.4.2">PNDM</a>

### <a id="section4.4.3">DEIS</a>

### <a id="section4.4.4">DPM-Solvers</a>

### <a id="section4.4.5">DPM-Solver++</a>

### <a id="section4.4.6">UniPC</a>


# <a id="section5">Consistency Models</a>
---

# <a id="section6">References</a>

[^Parisi]: Parisi G. Correlation functions and computer simulations[J]. Nuclear Physics B, 1981, 180(3): 378-384.

[^Grenander]: Grenander U, Miller M I. Representations of knowledge in complex systems[J]. Journal of the Royal Statistical Society: Series B (Methodological), 1994, 56(4): 549-581.

[^Aapo]: Aapo Hyvärinen, “Estimation of non-normalized statistical models by score matching”, JMLR, 2005.

[^Song_2019]: Yang Song and Stefano Ermon. "Generative Modeling by Estimating Gradients of the Data Distribution". NeurIPS 2019.

[^ddpm]: Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[J]. Advances in neural information processing systems, 2020, 33: 6840-6851.

[^ddim]: Song J, Meng C, Ermon S. Denoising diffusion implicit models[J]. arXiv preprint arXiv:2010.02502, 2020.

[^Vincent]: Vincent P. A connection between score matching and denoising autoencoders[J]. Neural computation, 2011, 23(7): 1661-1674.